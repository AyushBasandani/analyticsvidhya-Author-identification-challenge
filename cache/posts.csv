"title","author","date","week.day","content"
"Author identification data mining challenge on Analytics Vidhya data","Kunal Jain",2014-08-14 06:52:00,5,"
							
										
						What is the best form of analytics learning? Applying it to practical problems! This is exactly what led us to create this interesting problem, solving which would be a lot of fun! The challenge should be a good combo of basic text mining and predictive modeling. If you haven’t got your hands dirty learning these techniques, now is the time to do it!

Background
It’s family time for Kunal and Tavish! Both of them are on a break and have decided to stay away from any email / phone communications. Other Analytics Vidhya team members are not only filling in their shoes, but also have their own tasks to be completed!
Meet Navnit, our Tech Lead and web developer – who has decided that this is probably the best time to change the CMS (content management system) for our site. He creates a backup of data in our database, moves it over to a DVD and starts the migration.
Due to a mismatch in data models – a few fields got lost in the transition. Navnit, realized this only after he has deleted the entire data on previous CMS. When he realized this, he thought – nothing to worry, he has the backup, he can put up the lost fields through the DVD back. He checks that the DVD is on his desk and plans to restore the data first thing tomorrow morning.

Enter Jenika, Kunal’s lovely year old daughter – who feels that entire Analytics Vidhya office, is her playground. During her visit, she comes across this shiny blue disc, which she has not seen before! Nice toy, dad has in his office – she might have thought! In the hour, she had before Navnit is back in office, she tried eating her new toy, sliding it on the the ground and what not!
Poor Navnit, his only source of missing fields can not be used now! He checked and the last backup was taken on 6th July 2014,11:59 p.m. On comparing, one of the most critical fields lost is the author name. So, he can not identify the author for articles posted after 6th July 2014. He decides to finally learn and apply some predictive analytics for his work!
It’s your turn to help Navnit get back the data, before Kunal or Tavish are back in office (1st September 2014)!
Problem statement:
Classify all the articles written by Tavish or Kunal on analyticvidhya.com by the author’s name.
What Data you need to use for training your model?
All the articles written by Tavish and Kunal before 7th July 2014 can be used to train the model. You can use the date of article publish, day of article publish, tags of article publish and the content of the article. The data needs to be scrapped out of the website and used on local server.
What Data you need to score your model?
All the articles (excluding this article) written by Tavish and Kunal after 6th July 2014 need to be scored using your model.
Help : You can take reference from this video to start this analysis
What is the evaluation metric?
Average mis-classification rate of both training and scoring will be taken as the evaluation metric. For example 5 out of 10 in scoring and 50 out of 50 in training were found to be correct classes in training and scoring respectively. The average mis-classification rate will be 0.5 * (5/10 + 0/50) = 0.25. Hence, your score is 75%. You need to build model which has high predictive power and also stable over populations.

End Notes:

The aim of this challenge is to foster analytical thinking in our reader’s mind and have some fun with practical machine learning / analytics challenges!
We will give the winner of this challenge a chance to blog about his solution on Analytics Vidhya. Of course, he takes away all the visibility, which comes on the platform!
Last but not the least, the entire story presented before is hypothetical. It was created with the sole aim to create this challenge. All our data is secure and darling Jenika understands that she can’t play around with Dad’s stuff in office!

Happy learning!
Bonus:
If you want to foster discussion on any aspect of this problem, please feel free to do this through comments below. This is your chance to engage in community learning!

If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.

Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Performing exploratory analysis in Python (using Pandas)","Kunal Jain",2014-08-12 02:14:00,3,"
							
										
						In the last 2 posts of this series, we looked at how to install Python with iPython interface and several useful libraries and data structures, which are available in Python. If you have not gone through these posts and are new to Python, I would recommend that you go through the previous posts before going ahead.
In order to explore our data further, let me introduce you to another animal (as if Python was not enough!) – Pandas
Image Source: Wikipedia
Pandas are one of the most useful data analysis library in Python (I know these names sounds weird, but hang on!). They have been instrumental in increasing the use of Python in data science community. In this tutorial, we will use Pandas to read a data set from a Kaggle competition, perform exploratory analysis and build our first basic categorization algorithm for solving this problem.
 
Introduction to Series and Dataframes
Series can be understood as a 1 dimensional labelled / indexed array. You can access individual elements of this series through these labels.
A dataframe is similar to Excel workbook – you have column names referring to columns and you have rows, which can be accessed with use of row numbers. The essential difference being that column names and row numbers are known as column and row index, in case of dataframes.
Series and dataframes form the core data model for Pandas in Python. The data sets are first read into these dataframes and then various operations (e.g. group by, aggregation etc.) can be applied very easily to its columns.
 
Kaggle dataset - Titanic: Machine Learning from Disaster
You can download the dataset from Kaggle. Here is the description of variables as provided by Kaggle:

VARIABLE DESCRIPTIONS:
survival        Survival
                (0 = No; 1 = Yes)
pclass          Passenger Class
                (1 = 1st; 2 = 2nd; 3 = 3rd)
name            Name
sex             Sex
age             Age
sibsp           Number of Siblings/Spouses Aboard
parch           Number of Parents/Children Aboard
ticket          Ticket Number
fare            Passenger Fare
cabin           Cabin
embarked        Port of Embarkation
                (C = Cherbourg; Q = Queenstown; S = Southampton)

SPECIAL NOTES:
Pclass is a proxy for socio-economic status (SES)
 1st ~ Upper; 2nd ~ Middle; 3rd ~ Lower

Age is in Years; Fractional if Age less than One (1)
 If the Age is Estimated, it is in the form xx.5

With respect to the family relation variables (i.e. sibsp and parch)
some relations were ignored.  The following are the definitions used
for sibsp and parch.

Sibling:  Brother, Sister, Stepbrother, or Stepsister of Passenger Aboard Titanic
Spouse:   Husband or Wife of Passenger Aboard Titanic (Mistresses and Fiances Ignored)
Parent:   Mother or Father of Passenger Aboard Titanic
Child:    Son, Daughter, Stepson, or Stepdaughter of Passenger Aboard Titanic

Other family relatives excluded from this study include cousins,
nephews/nieces, aunts/uncles, and in-laws.  Some children travelled
only with a nanny, therefore parch=0 for them.  As well, some
travelled with very close friends or neighbors in a village, however,
the definitions do not support such relations.


Let the exploration begin
To begin, start iPython interface in Inline Pylab mode by typing following on your terminal / windows command prompt:
ipython notebook --pylab=inline
This opens up iPython notebook in pylab environment, which has a few useful libraries already imported. Also, you will be able to plot your data inline, which makes this a really good environment for interactive data analysis. You can check whether the environment has loaded correctly, by typing the following command (and getting the output as seen in the figure below):
plot(arange(5))

I am currently working in Linux, and have stored the dataset in the following location:
 /home/kunal/Downloads/kaggle/train.csv
 
Importing libraries and the data set:
Following are the libraries we will use during this tutorial:
numpy
matplotlib
pandas
You can read a brief description about each of these libraries here. Please note that you do not need to import matplotlib and numpy because of Pylab environment. I have still kept them in the code, in case you use the code in a different environment.
After importing the library, you read the dataset using function read_csv(). This is how the code looks like till this stage:

import pandas as pd
import numpy as np
import matplotlib as plt

df = pd.read_csv(""/home/kunal/Downloads/kaggle/train.csv"") #Reading the dataset in a dataframe using Pandas

 
Quick data exploration:
Once you have read the dataset, you can have a look at few top rows by using the function head()
df.head(10)
 

This should print 10 rows. Alternately, you can also look at more rows by printing the dataset.
Next, you can look at summary of numerical fields by using describe() function
df.describe()

describe() function would provide count, mean, standard deviation (std), min, quartiles and max in its output (Read this article to refresh basic statistics to understand population distribution)
Here are a few inferences, you can draw by looking at the output of describe() function:
Age has (891 – 714) 277 missing values.
We can also look that about 38% passangers survived the tragedy. How? The mean of survival field is 0.38 (Remember, survival has value 1 for those who survived and 0 otherwise)
By looking at percentiles of Pclass, you can see that more than 50% of passengers belong to class 3,
The age distribution seems to be in line with expectation. Same with SibSp and Parch
The fare seems to have values with 0 indicating possibility of some free tickets or data errors. On the other extreme, 512 looks like a possible outlier / error
In addition to these statistics, you can also look at the median of these variables and compare them with mean to see possible skew in the dataset. Median can be found out by:
df['Age'].median()
 
For the non-numerical values (e.g. Sex, Embarked etc.), we can look at unique values to understand whether they make sense or not. Since Name would be a free flowing field, we will exclude it from this analysis. Unique value can be printed by following command:
df['Sex'].unique()
Similarly, we can look at unique values of port of embarkment.
 
Distribution analysis:
Now that we are familiar with basic data characteristics, let us study distribution of various variables. Let us start with numeric variables – namely Age and Fare
We plot their histograms using the following commands:
fig = plt.pyplot.figure()
ax = fig.add_subplot(111)
ax.hist(df['Age'], bins = 10, range = (df['Age'].min(),df['Age'].max()))
plt.pyplot.title('Age distribution')
plt.pyplot.xlabel('Age')
plt.pyplot.ylabel('Count of Passengers')
plt.pyplot.show()

and
fig = plt.pyplot.figure()
ax = fig.add_subplot(111)
ax.hist(df['Fare'], bins = 10, range = (df['Fare'].min(),df['Fare'].max()))
plt.pyplot.title('Fare distribution')
plt.pyplot.xlabel('Fare')
plt.pyplot.ylabel('Count of Passengers')
plt.pyplot.show()


Next, we look at box plots to understand the distributions. Box plot for fare can be plotted by:
df.boxplot(column='Fare')

This shows a lot of Outliers. Part of this can be driven by the fact that we are looking at fare across the 3 passenger classes. Let us segregate them by Passenger class:
df.boxplot(column='Fare', by = 'Pclass')

Clearly, both Age and Fare require some amount of data munging. Age has about 31% missing values, while Fare has a few Outliers, which demand deeper understanding. We will take this up later (in the next tutorial).
 
Categorical variable analysis:
Now that we understand distributions for Age and Fare, let us understand categorical variables in more details. Following code plots the distribution of population by PClass and their probability of survival:
temp1 = df.groupby('Pclass').Survived.count()
temp2 = df.groupby('Pclass').Survived.sum()/df.groupby('Pclass').Survived.count()
fig = plt.pyplot.figure(figsize=(8,4))
ax1 = fig.add_subplot(121)
ax1.set_xlabel('Pclass')
ax1.set_ylabel('Count of Passengers')
ax1.set_title(""Passengers by Pclass"")
temp1.plot(kind='bar')

ax2 = fig.add_subplot(122)
temp2.plot(kind = 'bar')
ax2.set_xlabel('Pclass')
ax2.set_ylabel('Probability of Survival')
ax2.set_title(""Probability of survival by class"")


You can plot similar graphs by Sex and port of embarkment.
Alternately, these two plots can also be visualized by combining them in a stacked chart:
temp3 = pd.crosstab([df.Pclass, df.Sex], df.Survived.astype(bool))
temp3.plot(kind='bar', stacked=True, color=['red','blue'], grid=False)

You can also add port of embankment into the mix:

If you have not realized already, we have just created two basic classification algorithms here, one based on Pclass and Sex, while other on 3 categorical variables (including port of embankment). You can quickly code this to create your first submission on Kaggle.
End Notes:
In this post, we saw how we can do exploratory analysis in Python using Pandas. I hope your love for pandas (the animal) would have increased by now – given the amount of help, the library can provide you in analyzing datasets.
We will start the next tutorial from this stage, where we will explore Age and Fare variables further, perform data munging and create a dataset for applying various modeling techniques. If you are following this series, we have covered a lot of ground in this tutorial. I would strongly urge that you take another dataset and problem and go through an independent example before we publish the next post in this series.
If you come across any difficulty while doing so, or you have any thoughts / suggestions / feedback on the post, please feel free to post them through comments below.
If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"A Big Data Course with a ‘Big’ Difference","Kunal Jain",2014-08-08 19:50:00,6,"
							
										
						Big Data has emerged as one of the fastest growing fields in recent times and every business is looking to leverage Big Data to get ahead of the competition. It’s thus not surprising that the demand for skilled Big Data professionals is huge and far exceeds the current supply. So much so, that those with Big Data skills command high salaries (about 25- 50% higher than IT and other fields). It has become the field of choice for many IT professionals looking to fast track their career.
There were no courses in Big Data analytics, until even 18 months ago. However, today we are seeing a flood of courses on offer and many can’t decide, which is the right course for them.
I recently heard that Jigsaw Academy has launched its Big Data analytics course. What caught my attention was the fact that they claim to have India’s first globally recognized certification in Big Data. Now this is something I wanted to check out for real.
The first thing I did was check out the website – specifically the Big Data course page
 
Credibility of the certification
Jigsaw Academy has tied up with Wiley, a US based publication house, for the Big Data course. All of us, especially the engineers amongst us who swear by Resnick and Halliday know Wiley books. But what is less well known is the fact that Wiley also has a number of certifications, especially in IT.
What was interesting for me was that this is the first time Wiley has tied up with an Indian organization to come up with a joint certification – the Wiley-Jigsaw Big Data specialist certification.
The Wiley-Jigsaw certification certainly looks like the most credible certification available on Big data and should improve job and career prospects for the people, who undertake it.
Course Outline
Keeping the brands aside, let us look at the content of the course.
Comparing the Jigsaw course with the Edureka course, I found a huge overlap between the two. Both the courses cover hadoop, mapreduce, hive, pig and other popular Big Data technologies. However, Jigsaw’s course has an analytics orientation, while the Edureka course has a more technology orientation. It comes out clearly that Jigsaw has made a significant effort to make it more readable to a novice. Edureka, on the other hand, seems to be targeting IT professionals who are already familiar with technical terminologies.
Jigsaw Academy’s Big Data course has an edge when it comes to the analytics piece – they have modules on R and its integration with hadoop –  RHadoop and RMR packages. Also, their case studies cover both structured and unstructured data. If their past case studies are anything to go by, students are in for some great hands-on experience.
This difference stands out, when I compare this course from Jigsaw with other courses as well. Jigsaw’s Big Data course is the first one, I have seen in the market, that has clear focus on analytics.
Student Experience
After certification credibility and course outline, another important evaluation criterion is overall student experience.
I asked Jigsaw for access to one of their Big Data modules. I came away very impressed by the quality of their videos and assignments. Their videos are superior to anything I have seen, including the MIT and Coursera videos. You can have a look at a sample video here.
I also managed to catch hold of two students, who were part of the pilot batch run by Jigsaw in May. Both students were very appreciative of the course.
Umang Chugh, an MBA graduate from AIM Manila, said “I had previously done some analytics courses from Jigsaw and I had really liked them. So when they invited me to be a part of the pilot for Big Data I jumped at it. They have maintained the level of quality we all know them for. Even though Big Data is an intimidating area for a newbie like me, Jigsaw managed to make the course simple and interesting.”
Yatin Gupta is another IT professional who enrolled for the pilot batch.
“What I liked best about the course is the support from the mentors, who are knowledgeable and very helpful. I also liked the case studies and the way they have set up their Big Data lab. It is very easy for a beginner to get started.”
Image courtesy http://www.allanalytics.com/ (source Rurtch Works)
Having heard all the good things, I asked them specifically about the things they did not like. Umang mentioned that he was looking for case studies in healthcare but did not find any. Yatin said that while the support from the faculty is great, he would have liked some additional classes on certain advanced topics.
The Wiley Difference
Finally, I wanted to explore the Wiley component of the course. Jigsaw has several analytics courses that are offered independently. What was the reason for the tie up with Wiley? What does Wiley add to the offering?
Wiley has pulled in Big Data experts from around the world to create some of the content for the course. Students receive books (e-copies as well as physical copies) of the course material.

“Wiley has the ability to reach some of the best Big Data experts for building the content. We have the expertise to deliver a great online learning experience. I think together we have the ability to create a world class course in Big Data analytics.” Says Sarita Digumarti, the COO of Jigsaw Academy.

“This is the first time we will be offering physical study material in the form of books to our students. And we could think of no one better than Wiley to partner with for this” she said.
What I did not like
One thing I did not like about the course is that it assumes its students are familiar with analytics and the R tool. I would have liked the R module to be included in the course itself.

We feel that a combination of our Data Science course (where people learn analytics with R) and the Big Data course is the ideal combination for anyone looking to enter into this field. Combining both the courses into one would make the course very long. We deliberately ensure that all our courses are less than 6 months.

The price point of the course is surely something I am concerned about. At Rs. 42,000 this is among the most expensive courses in the market currently.
“Quality comes at a price” was the short answer from Sarita Digumarti when I quizzed her about the course fee. “Our courses have always been premium priced and we feel this kind of pricing is essential for us to be able to continue to deliver the quality of the student experience we aim for.”
Currently, Jigsaw academy is offering an introductory discount of over 15% on their course. This translates to a saving of Rs. 7000 and makes this course very attractive.
Is the course worth investing your time and money in?
Let’s look at the positives first – It offers a globally recognized certification, it offers Big Data with analytics perspective, it delivers a good student experience, the quality of the videos is top class and it leverages on Wiley’s 100+ years of experience in creating great content.
On the flip side, the course is pricey and it needs to be combined with their Data science course to get the full benefit.
Personally, if I can afford to spend Rs. 42,000 on a course, this is the course I would go for.
I hope this article will be useful to the hundreds of followers, who write to me for advise on the courses available in the market. And as always, I would love to hear your thoughts or comments. If you have further questions about this offering or other courses, let me know.
If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Review: Qlik Sense Desktop - Is this the next gen visualization tool you need?","Kunal Jain",2014-07-27 21:45:00,1,"
							
										
						QlikTech recently announced a free version of its next-generation data visualization application – Qlik Sense. According to QlikTech, the product delivers a simple drag-and-drop interface for business users to rapidly create interactive visualizations, reports, and dashboards.  
If you have been following QlikTech, Qlik Sense Desktop is the first commercially available release from the much talked about project QlikView.Next.

Initial excitement
I have been using Qlikview for about 2 years now. I have also used Tableau to a fair degree (though not as much as Qlikview). One of the benefits, Tableau had over Qlikview was a simple drag and drop interface, so that business users can create applications easily. So, when I heard that QlikTech has come out with a drag and drop visualization product, I got pretty excited! It could have been the best product available in the market. Did the excitement stay after I had tried the product? Let’s see.
 
Downloading and installing
I am on Windows 8.1, 64 bit architecture – downloading and Installing Qlik Sense was a breeze! ~120 MB download and installation happened smoothly. You can download the Desktop version here.
 
First impressions
Qlik Sense impresses in its first impressions. You come across neat, crisp layouts with simple user interface. A new app can be created and opened easily. You can load data using either option – Quick Load for Excel files and Data Load editor to load data from Databases and web. Everything till now looks easy and fairly intuitive. The data load happens instantaneously for the small data sets I tried.

Once the data is imported, you can open and edit sheets exactly as described – drag and drop! There is a range of charts you can include. Surprisingly, the product doesn’t provide an option to create all the charts already present in Qlikview. For example, you cannot create a cross-table or Pivot table.
 
Getting the hands dirty – creating my first application
I thought, I will use Qlik Sense to visualize baby names dataset, available freely on the internet. This is a fairly simple dataset – with names of top 200 male and female children in the last decade. Getting total number of babies named in a bar chart by Gender was easy – all well till now!
Next, I thought, let me look at top 5 names from the list. This is when I started facing problems, I didn’t expect – I couldn’t find a simple way to limit the data to top 5 names by Gender. I am sure Qlik Sense would have some way to deal with this – but I couldn’t find a way to do it through simple drag and drop way.
Update: This can be done through Properties -> Dimensions -> Limitation -> Fixed Number (Thanks Rob for telling the trick).
Next, I thought to create a cross-table with Names on Vertical axis and Gender on Horizontal axis. While the simple table was showing the sum by Gender, I could not drag it on the horizontal axis.
After searching on Qlik Community, I saw that you can create some of these visualizations in Qlikview and then import them in Qlik Sense – but doing so kills the philosophy of the product – giving new users the power to create powerful visualizations.
Another thing, which I did not like in the interface was that you cannot edit and play around with the sheet in a single window. If you are editing a sheet – you cannot play around with the sheet in same window. You have to open the preview in a separate window. This kills the natural data discovery for me. May be I have got used to Qlikview and Tableau, but editing and previewing a sheet in different windows looks in contradiction with natural data discovery. Hopefully, QlikTech is thinking of merging them before the final edition is out later this year.
 
A few awesome features:
While I am disappointed by the data discovery process, there are a few awesome features Qlik Sense has come with.
Data Storytelling
One of the best features was Data storytelling – you can create data based stories, while you are actually doing data discovery and play them as a slide show later on. Here are the 2 slides, I created with the small dataset I had:


Why is this feature a winner? Because it addresses my need. There have been instances, when I have created dashboards and then added stories in a sheet upfront through text boxes and use of bookmarks – but I have to admit, it was a mess. This storytelling feature makes life so much easier – you can now lead your discussions through the visualizations, without having to switch between PowerPoint and Qlikview.
 
Sharing visualizations:
This is a smart move from QlikTech. Sharing your work is a challenge in Qlikview, if you are working on a personal edition. Tableau, on the other hand, can help you share your work with other people – but you have to make it public. Qlik Sense just sits in the middle, I can share my work without any restriction to anyone who has Qlik Sense installed. How does it help?
I have faced this situation multiple times, I normally do data discovery in these visualization tools before starting on a new project. Also, now that I can do data based storytelling, I would like to share them at the end of projects as well. If the product takes off, Qlik Sense can become a standard way to tell data based stories – who would need Excel or PowerPoint any more!
 
Overall Verdict:
Sadly, it looks like, I ended up expecting more from the product that it actually delivers. However, this is a product, still in development (version 0.96), it would be interesting to see what comes out in September 2014. Also, the product is not too far from a very good product.
Here are the things I would want to see improvements upon:
The product should ideally support all the visualizations present in Qlikview
A more intuitive and connected data discovery – editing and exploring on the same tab.
 
Will I still use the tool in its current form? 
Definitely, I am highly impressed by data story telling capabilities and ability to share visualizations freely. Overall, I think QlikTech is on the right track to raise the bar against Tableau. However, it needs to make a few changes before it is out with a killer product in the market.
 
Disclaimers / A few notes:
All the views presented in the article are my independent views.
QlikTech emphasizes on a new holistic search, which I have not tested yet. So, I can’t comment on how good or bad it is.
There is a functionality for Maps based visualizations, which is in beta phase. It is not as good as Tableau Geo-intelligence. It requires you to input Latitude and Longitude in order to draw these visualizations.
 
What do you think about Qlik Sense? Have you downloaded and used it? If not, do you think, you will give it a try. Please share your review / feedback / experience with Qlik Sense through comments below.
 
If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Using statistics: How to understand population distributions?","Kunal Jain",2014-07-21 22:40:00,2,"
							
										
						One of the common queries, which I get on the blog is:

I am not a Mathematics / Statistics graduate. Can I still become a good business analyst?

or

I am not good at statistics. Can I still change my career to become a business analyst?

The simple answer to the question is - you can’t become a good analyst until you know statistics. However, you don’t need to be an expert in statistics to become a good business analyst.
So, you don’t need to understand the joke in the strip, in order to qualify as a business analyst:

 
Two types of Business Analytics:
Business anlaytics can be divided in two classes – applied business analytics and theoretical business analytics. Here are definitions of the two streams:
Applied Business Analytics – This is the work, where the emphasis is to solve a problem at hand. What matters is that you have a strategy / algorithm, which is better than what is happening currently. You deal with practical problems in this stream – messy data, missing values, bad data capture etc. More than 95% of times, you would typically use the algorithm and outputs straight from the tools. As long as you are aware of the assumptions in your model, can check whether they are holding good and interpret the output of the algorithms correctly, you are good for applied business analytics.
For example, if you know the assumptions in Linear regression and can interpret what is R-square and adjusted R-square, you would be good to apply Linear Regression.

Theoretical Business Analytics – This is the research area in business analytics. When you have a problem, where current set of algorithms are already optimized and applying standard techniques would not provide you any further uplift. This is when, you need to get into statistical details of various algorithms and then improve them.

 
Please note that this is not a standard categorization of Business Analytics and it might be difficult to identify some projects in exact buckets. However, it is good enough to communicate the point that you can deal with most of the business analytics problems with basic knowledge of statistics.
What does an analyst need to know?
Now, that you understand the two classes of business analytics, here is some good news! You don’t need to be a statistician to practice applied business analytics.
So, what exactly do you need to know to become an applied business analytics practitioner? I thought why not run a series of articles explaining the basic concepts of statistics, an applied BA practitioner needs to know.
Please note that this series is not intended to be a thesis on statistics. Instead, it takes a very practical outlook to apply statistics to solve business problems.
 
Concepts to understand population distributions:
One of the first things a business analyst needs to do is understand various distributions of parameters and population.
One of the most frequently used method to understand distributions is to plot them using histograms. A histogram represents frequencies of various values through a plot in uniform buckets (popularly known as bins). In case of continuous variables, a histogram represents the probability distribution function (we will cover this later). If you want an example of how histogram is plotted, you can look at this video from Khanacademy. Here is how a typical histogram might look like:
Example of histogram. Source: Wikipedia
There are 3 variety of measures, required to understand a distribution:
Measure of Central tendency
Measure of dispersion
Measure to describe shape of curve
Measures of Central tendency:
Measures of central tendencies are measures, which help you describe a population, through a single metric. For example, if you were to compare Saving habits of people across various nations, you will compare average Savings rate in each of these nations.
Following are the measures of central tendency:
Mean – or the average
Median – the value, which divides the population in two half
Mode – the most frequent value in a population
The following image illustrates how mean, median and mode would be placed in a couple of scenarios:

Among the three measures, mean is typically affected the most by Outliers (unusually high or low values), followed by the median and mode.
Measures of Dispersion:
Measures of dispersion reveal how is the population distributed around the measures of central tendency.
Range – Difference in the maximum and minimum value in the population
Quartiles – Values, which divide the populaiton in 4 equal subsets (typically referred to as first quartile, second quartile and third quartile)
Inter-quartile range - The difference in third quartile (Q3) and first quartile (Q1). By definition of quartiles, 50% of the population lies in the inter-quartile range.
Variance: The average of the squared differences from the Mean.
Standard Deviation: is square root of Variance
Difference in distribution of 2 populations with same mean, median and mode. Source: Wikipedia
 
Measures to describe shape of distribution:
Skewness - Skewness is a measure of the asymmetry. Negatively skewed curve has a long left tail and vice versa.
Kurtosis - Kurtosis is a measure of the “peaked ness”. Distributions with higher peaks have positive kurtosis and vice-versa

 
A few practical tips to understand distributions better:
Use of box plots: Box plots are one of the easiest and most intuitive way to understand distributions. They show mean, median, quartiles and Outliers on single plot.

You can use box plots next to each other for various categories / segments of population to understand overlap / differences in the population. Following is an example of one such comparison (with illustrative data):
In this post, we looked use of statistics to plot and understand distributions of populations – first steps for any business analyst to do in a project. In the articles to follow in this series, we will look at use of confidence intervals, hypothesis testing, probabilities and measures to judge various predictive models. If you would want me to cover more topics, please let me know through comments below.
In the article next week (from baby steps in Python series), we will see how to look at these measures and distributions using Python on a Kaggle dataset.
If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Director - Sales & Business Development - Analytics (10-16 yrs)","Kunal Jain",2014-07-17 09:59:00,5,"
							
										
						Are you a pro in selling analytics solutions to your clients. Are you a thought leader in Analytics Industry, with global exposure and network? If the answer is yes, check out the following opening:
Location: BangaloreAbout the Client: 
Leading research and analytics firm in Bangalore.
The client offers a blend of applied research and business strategy that includes: quantitative and qualitative research, professional services and consulting, technology solutions, and industry thought leadership with a strong presence in US, Europe & Asia. 
Responsibilities: 
Identify new sales opportunities and articulate Analytics value proposition to the Clients.
Build qualified pipeline to drive sales growth for analytics in the Retail sector. Refine go to market for the sector
Attend industry events to represent the organization brand and network with prospects
Develop & successfully close the opportunities with support from the established pre-sales & delivery team
Looking forward to ambitious & growth oriented professionals be a part of this team. 
 
Skills / experience required: 
10+ years of experience in a customer-facing, Sales-related role
Proven track record of carrying and attaining annual quota; meet set quarterly bookings and revenue goals as an individual contributor and/or with a Sales & Account Management team.
Passion for Sales, long-term relationship development and management.
Hunter profile; Self-starter; Creative
Experience in selling to business leaders across functional areas and to CxO-levels
Deep knowledge of selling into the Retail and/or Manufacturing (CPG, Consumer Electronics, Apparel, Footwear, Food & Beverages) sector
 
Interested people can send a mail to jobs@analyticsvidhya.com with Director – Sales & Business Development as subject
If you want to stay updated on latest analytics jobs, follow our job postings on twitter or like our Careers in Analytics page on Facebook
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Baby steps in Python - Libraries and data structures","Kunal Jain",2014-07-15 23:46:00,3,"
							
										
						In one of the posts last month, we started taking baby steps in learning Python for data analysis. This post will take you one step ahead in your journey to learn Python. By end of this post, you will understand the role of several python libraries and various kinds of data structures used in Python.
We will take simple examples for each kind of dataset to illustrate their purpose.

 
Important libraries in Python:
Python provides basic set of commands and functionality in its base version. If you need more functions, there are several libraries, which should be imported into your environment. There are several ways of importing libraries in Python:

import pandas as pd
from pandas import *

In the first manner, we have defined an alias pd to library pandas. We can now use various functions from pandas library (e.g. read_csv()) by referencing it using the alias pd.read_csv().
In the second manner, you have imported the entire name space in pandas i.e. you can directly use read_csv() without referring to pandas.
 Tip: Google recommends that you use first style of importing libraries, as you will know where the functions have come from. 
Following are a list of libraries, you will need for any scientific computations and data analysis:
NumPy stands for Numerical Python. The most powerful feature of NumPy is n-dimensional array. This library also contains basic linear algebra functions, Fourier transforms,  advanced random number capabilities and tools for integration with other low level languages like Fortran, C and C++
SciPy stands for Scientific Python. SciPy is built on NumPy. It is one of the most useful library for variety of high level science and engineering modules like discrete Fourier transform, Linear Algebra, Optimization and Sparse matrices.
Matplotlib for plotting vast variety of graphs, starting from histograms to line plots to heat plots.. You can use Pylab feature in ipython notebook (ipython notebook –pylab = inline) to use these plotting features inline. If you ignore the inline option, then pylab converts ipython environment to an environment, very similar to Matlab. You can also use Latex commands to add math to your plot.
Pandas for structured data operations and manipulations. It is extensively used for data munging and preparation. Pandas were added relatively recently to Python and have been instrumental in boosting Python’s usage in data scientist community.
Scikit Learn for machine learning. Built on NumPy, SciPy and matplotlib, this library contains a lot of effiecient tools for machine learning and statistical modeling including classification, regression, clustering and dimensionality reduction.
Statsmodels for statistical modeling. Statsmodels is a Python module that allows users to explore data, estimate statistical models, and perform statistical tests. An extensive list of descriptive statistics, statistical tests, plotting functions, and result statistics are available for different types of data and each estimator.
 
Additional libraries, you might need:
urllib for web based operations like opening URLs and performing operations
os for Operating system and file operations
networkx and igraph for graph based data manipulations
regular expressions for finding patterns in text data
BeautifulSoup for scrapping web
 
Data Structures:
Following are some data structures, which are used in Python. You should be familiar with them in order to use them as appropriate.
Lists – Lists are one of the most versatile data structure in Python. A list can simply be defined by writing a list of comma separated values in square brackets. Lists might contain items of different types, but usually the items all have the same type. Python lists are mutable and individual elements of a list can be changed.
Here is a quick example to define a list and then access it:

Strings – Strings can simply be defined by use of single ( ‘ ), double ( ” ) or triple ( ”’ ) inverted commas. Strings enclosed in tripe quotes ( ”’ ) can span over multiple lines and are used frequently in docstrings (Python’s way of documenting functions). \ is used as an escape character. Please note that Python strings are immutable, so you can not change part of strings.

Tuples - A tuple is represented by a number of values separated by commas. Tuples are immutable and the output is surrounded by parentheses so that nested tuples are processed correctly. Additionally, even though tuples are immutable, they can hold mutable data if needed.
Since Tuples are immutable and can not change, they are faster in processing as compared to lists. Hence, if your list is unlikely to change, you should use tuples, instead of lists.

Sets - A set is an unordered collection with no duplicate elements. Basic uses include membership testing and eliminating duplicate entries. Set objects also support mathematical operations like union, intersection, difference, and symmetric difference. Set can be defined by using set() function

Dictionary - Dictionary is an unordered set of key: value pairs, with the requirement that the keys are unique (within one dictionary). A pair of braces creates an empty dictionary: {}. 

Now that you are familiar with ipython environment, various important libraries and key data structures in Python, we will discuss Arrays, Pandas and dataframes – most commonly used tools to handle structured data in Python.
In our next article in this series, we will read the dataset from Kaggle Titanic competition, import it into a dataframe and then perform exploratory analysis on the data.
In the meanwhile, if you have any tips to share for handy usage of these data structures, please feel free to share them through comments below.
If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"The definitive guide to prepare for analytics interview","Kunal Jain",2014-07-07 23:12:00,2,"
							
										
						Let’s face it! Facing an analytics interview can be daunting at times! 
I have met a lot of analysts, who are good analysts when you interact with them informally. But something happens to them, as soon as they enter into an interview!

Have you seen one of these analysts and wondered what happens to them in the room? Or have you faced this situation yourself? This guide is meant to help you / your friend to ace the next analytics interview!
The first thing to keep in mind before appearing analytics interview is:

As long as you know your subject, are a logical person and can stay calm – you can ace these interviews easily!
 
What is the employer trying to judge you on?
The actual skills, which the employer might be judging on, would vary from employer to employer, but it is likely a mix of the following skills:
Technical skills – comfort and knowledge about various analytical tools
Knowledge of statistics – whether you apply algorithms blindly or actually understand what they do?
Structured thinking – Can you take ambiguous problems and put a framework around them?
Business understanding – How well can you put on your business thinking hat?
Problem solving - Can you provide (out of the box) solutions to problems?
Communication skills – Can you communicate your thoughts clearly and crisply? Can you influence people?
Comfort with numbers – How good are you at crunching them?
Attention to details – Do you pay attention to small details and at them up to see the bigger picture
This article can help you understand the perspective of an employer in some more details.
 
Types of analytics interviews:
Analytics interviews can be divided in broadly three categories:

The preparation for technical analytics interviews happens over time. These interviews test how much time and efforts have you put, in learning your subject and tools.
If you are really good at what you do, these rounds should be a cake walk. If you are not, the best strategy is to be honest about what you know and what you don’t and let your potential employer know. Here are a few articles to help you with technical interviews:
Tricky questions on SAS – part I and part II
Tricky questions in R
There is a lot of material available on the internet to prepare for behavioural interview, hence I would skip those details.
 
Skill assessment interviews:
These are the deciding factor in most of the analytical hiring, and for a good reason – if a person has sound logical skills and can demonstrate good business thinking and logical skills – he can pick up technical skills easily! Since these interviews are aimed to assess various skills, what matters more, is that you demonstrate those skills. The actual answer and solution is irrelevant in most cases. Any hiring manager would prefer a wrong answer with a better approach rather than an accurate answer with bad approach.
Skill interviews, again can be categorized in  2 categories:
Guess estimates
Case studies and role plays
 
Guess-estimates:
Guess estimates are puzzle like questions, where you are expected to estimate a figure by putting a framework to a question, creating segments, making assumptions and adding up the numbers to arrive at a number.
You can read details on how to ace a guess-estimate along with a few examples here. Here are a few tips I would recommend:
It is the approach that matters – not the exact numbers. However, you should cross-validate numbers once you have them.
Always go top down to solve a problem. Draw neat segmentation & diagrams to illustrate your approach.
Keep a few common starting points / proxies on your finger tips. Population of your country, population across the globe, the GDP of your country are a few good starting points you should definitely remember.
Analyze all possible uses of the subject. E.g. You should consider B2B & B2C markets, if you are asked to estimate market of tablets or smartphones.
Call out assumptions and possible blind spots.
 
Case studies / Role plays:
Here is what Capital One says about case studies on its website:

Case interviews are broad, two-way discussions rather than one-way tests. You will be assessed more on how you go about dealing with the problem rather than on the specific answers you come up with.

A case typically starts with a broad question providing a business scenario and then narrows down in a particular direction. Cases might also evolve and grow in complexity as the interview progresses. Here is how a typical interview evolves over time:

Here is an example of a typical case study interview. Here is another one.
Following are some best practices to follow in a case study round:
Case study is all about illustrating 3 things – Structure, structure and structure! Focus on putting framework to the problem provided, and you will be safe. Try deviating from it and you’ll find yourself in trouble.
For example, when asked how can you increase Profits for a product company, you should not jump to conclusions like “I’ll improve marketing or I’ll cut costs”. You should say Profits = Revenues – Costs. In order to increase profits, we can either increase Revenues or reduce costs. Revenues can be increased by increasing Sales or increasing the price. Costs can be reduced by doing ….
Keeping a structure will not only help the interviewer understand you better, it will also help you make sure that you have not missed out any thing.
Call out assumptions, whenever you are making them. These could be assumptions about business or the sector in discussion.
Lay out things neatly on paper, such that, they can be re-used later. Most of the times, case studies evolve over time. You will be asked to do similar questions, multiple times under multiple scenario. Keeping them handy can reduce calculation time!
Think out loud – it is the thinking process, which matters. If you are not sure – ask the interviewer rather than staying quite!
Communicate crisply and clearly – if you are not clear about your thoughts, take 2 minutes from the interviewer to arrange your thoughts and then communicate them nicely.
 
Finally, here is a list of activities / behaviour, you should avoid during the interview. These, along with the best practices mentioned above, should give you enough ammunition to handle any analytics interview.
 
What do you think about this guide? Do you have handy tips and tricks, which helped you in your interviews? Please share your thoughts and practices through comments below. I am looking forward to hear them.
If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.
image credit: Oregon State University
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Baby steps in learning Python for data analysis","Kunal Jain",2014-07-01 05:25:00,3,"
							
										
						Last weekend turned out to be a very special one! 
My 10 month old daughter took her first baby steps and watching her take those steps was one of the most beautiful moment of my life. A baby, brimming with excitement to reach out to her father, trying to balance, while exploring her newly acquired skill and trying to speak simultaneously in her own way – was a moment to cherish! I hope I could have recorded that instance!
What followed the first little walk was equally exciting – Jenika (my daughter) was enjoying practicing her new skill in spite of all the limitations, she faced. She kept faltering every 3-4 steps, but it didn’t seem to affect her. There were a few things, which stood out in the manner, she was practicing her new skill:
She was enjoying the process.
The outcome of previous attempt didn’t matter. Every attempt was a fresh start.
Most of the time her focus was on the immediate next step rather than the end of the journey.

After spending the weekend with Jenika, both of us are still not over with the fun! She is still enjoying her day in the same innocent manner and I can’t stop thinking about it.
This is when this thought came to my mind – why not create a series of articles like baby steps – articles which enable users to take small steps towards a new skill, keep the excitement up and provide a refreshing start. I knew I was doing it, the second this thought crossed my mind!
The next question was, which area should these articles be written on? There were 2 areas I considered – Python for data analysis and Big data. Why them? For a simple reason, we have not spent a lot of time learning these skills till now. I will start with Python and we will take Big data at a later date.
 
Baby steps with Python for data analysis
In this series of articles, we will take bite sized information about how to use Python for data analysis, chew it till we are comfortable and practice it at our own end.
In today’s article, we will cover:
Why learn Python for data analysis?
How to install Python?
Running a few simple programs in Python.
Quick and dirty build of first model – just to illustrate, how easy it is to build a model in Python.
 
Why learn Python for data analysis?
Python has gathered a lot of interest recently as a choice of language for data analysis. I had compared it against SAS & R some time back. Here are some reasons which go in favour of learning Python:
Open Source – free to install
Awesome online community
Very easy to learn
Can become a common language for data science and production of web based analytics products.
Needless to say, it still has a few drawbacks:
It is an interpreted language rather than compiled language – hence might take up more CPU time. However, given the savings in programmer time (due to ease of learning), it might still be a good choice.
 
How to install Python?
There are 2 approaches to install Python:
You can download Python directly from its project site and install individual components and libraries you want
Alternately, you can download and install a package, which comes with pre-installed libraries. I would recommend Enthought Canopy Express to go ahead. Another option could be Anaconda.
Second method provides a hassle free installation and hence I’ll recommend that to beginners. The imitation of this approach is you have to wait for the entire package to be upgraded, even if you are interested in the latest version of a single library. It should not matter until and unless, until and unless, you are doing cutting edge statistical research.
 
Choosing a development environment:
Once you have installed Python, there are various options for choosing an environment. Here are the 3 most common options:
Terminal / Shell based
IDLE (default environment)
iPython notebook – similar to markdown in R
IDLE editor for Python
While the right environment depends on your need, I personally prefer iPython Notebooks a lot. It provides a lot of good features for documenting while writing the code itself and you can choose to run the code in blocks (rather than the line by line execution)
We will use iPython environment for our future articles.
 
Warming up: Running a few programs in Python:
You can use Python as a simple calculator to start with:

A few things to note:
You can start iPython notebook by writing “ipython notebook” on your terminal / cmd, depending on the OS you are working on
You can name a iPython notebook by simply clicking on the name – Untitled0 in the above screenshot
The interface shows In [*] for inputs and Out[*] for output.
You can execute a code by pressing “Shift + Enter” or “ALT + Enter”, if you want to insert an additional row after.
 
A few additional things you can try are:
Printing a message
Creating and running loops
If you feel, it is overwhelming, don’t worry, we will do these step by step in coming days.
 
Building a simple Logistic Regression model in Python:
Following are the steps required to create the first Logistic Regression model:
Import the required libraries
Read the data
Explore and clean the data
Build Logistic Regression model
Interpret the results
Since the purpose of this model is to just illustrate how to build a model in Python, I will take a clean dataset (remember dataset iris, which Tavish used in his previous article?) and just go ahead and build a model on the entire dataset. We will look at ways to split the data into test and train at a later point.
P.S. If you don’t understand all the steps today, don’t worry. I would suggest that you download and install iPython and just run this program as is to get used to the environment.
Here are the libraries and the dataset you will need:

Once you have read the dataset, you can print the dataset to explore it and build a Logistic Regression model on it:
And finally, you can compare the expected and predicted values:

That’s it. Your first model in Python is ready. You can now use model.predict(‘input data’) to make classifications basis the data provided. In the next article, we will show a way to share these notebooks with other people, accept contributions from other people and ways to version control it through github.
What do you think about using Python for data analysis? Did you find this tutorial and idea about this series useful? Do let me know, through comments below.
If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.
 
You might be interested in: Baby steps in Python: Part 2 (Libraries and data structure)
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"What is deep learning and why is it getting so much attention?","Kunal Jain",2014-06-24 05:19:00,3,"
							
										
						A few days back, the content feed reader, which I use, showed 2 out of top 10 articles on deep learning.
This is when I thought I need a better understanding of what is deep learning. I probably noticed the term – deep learning sometime late last year. And it has grown in its presence around me since then. I wanted to make sure this is a global phenomena and not just me getting served specific content based on my searches or past history. So, I pulled up Google trends for deep learning. This is what, it showed:

Clearly, I was catching up a new trend. So, let me summarize my findings and views on the topic, basis what I have read in the last few days.
 
P.S. You might have figured out by now, that I am not an expert in Deep Learning (phew!). But, I am more motivated than most of the people to learn about them. I hope to provide a meaningful summary to starters and a few thought provoking questions to the experts. If you have any questions / opinion on the topic, please add them in the comments below.

What is deep learning?
Deep learning is probably one of the hottest topics in Machine learning today, and it has shown significant improvement over some of its counterparts. It falls under a class of unsupervised learning algorithms and uses multi-layered neural networks to achieve these remarkable outcomes. Here is a simple illustration from Analytic Store’s blog:

A large number of pixels are fed to the network as input, after which the network learns and evolves to recognize higher level features like faces and cats.
Here are a few achievements driving the attention to this area:
Amazing accuracy on quite a few Kaggle competitions – Dogs vs. Cats image recognition (98.9% accuracy), Saving the whales problem (98% accuracy)
Ability to learn and identify cats by using YouTube videos without any supervision
Following are a few events, which suggest the lookout for people with knowledge of deep learning:
Google’s acquisition of Deepmind Technologies
Google hiring Jeff Hinton, one of the thought leaders in this space (you can check out his course on neural networks on Coursera)
Facebook hiring Yann LeCun, a student of Jeff Hinton to lead its AI lab
Baidu hires Andrew Ng, another pioneer in the field (and  co-founder of Coursera).

Applications of deep learning:
If you have not figured it out already, deep learning finds its applications in following areas:
Image recognition (e.g. Tagging faces in photos)
Voice recognition (e.g. Voice based search, Siri)
Pattern detection (e.g. Handwriting recognition)
But, neural networks have been there for decades, what is re-kindling this interest now?
Yes, neural networks have existed since ages. Interest in neural networks peaked in the 1980s and 90s and then died off because of the inherent problems with them and black box like approach.
There are a few reasons why this is happening now. The biggest one among them, being the drop in computational costs. Classification of cats through unsupervised learning of YouTube videos was achieved by deploying 16,000 computers in Google lab! The cost of deploying these algorithms is not small, even by today’s standards.
 
Resources in deep learning:
Here is a list of some good resources to start reading / following, if you are interested in this area:
www.deeplearning.net along with its tutorials
Jeff Hinton’s Neural network course on Coursera
Google+ community
Yann LeCun overview of Deep Learning with Marc’Aurelio Ranzato
 
Questions in my mind:
Some of the questions, which remain in my mind are:
This looks like a huge black box to deal with (something like a scaled up version of random forests), which might do well in data science competitions, but fails to deliver any business understanding.  Will it be useful and impactful for larger community or stay with in labs of data giants working on huge data sets? For example, the model might say classify a particular person as a likely defaulter, but would not provide, why it is doing so.
The methods have shown some flaws, which are difficult to explain. According to a recent study, algorithms were able to classify the images on the left in the picture below, but were not able to classify the images on the right – which may seem very similar to human eyes. (Source: KDNuggets, original case study)

Overfitting / Choosing the right algorithm - Given the nature of these algorithms, you are building multiple middle (hidden) layers in your architecture. These would work well on problems with infinite (or very large) degrees of freedom. However, if you have limited degrees of freedom, we might end up with an overfitted model – probably time to go back to traditional methods.
 
End notes:
I have to admit, I started my research from a place where these algorithms looked more like a buzz. But given the attention from data giants, success in some of the Kaggle competitions and the reducing costs of computations, I am starting to believe that the hotness of the field is justified.
Whether it is actually justified or not, only time will tell. In the meanwhile, I’ll continue with my research and keep you posted on how are things panning out at my end.
What do you think about Deep learning? Do you think it will change the way people look at machine learning today? Or do you think this might just be another hype? How would people solve for some of the challenges, I have mentioned in the post? Do let me know your thoughts through comments below.
If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"SAS Analytics U launched by SAS as a free version - Our review","Kunal Jain",2014-06-16 04:32:00,2,"
							
										
						I have spent the entire 7 years of my corporate work experience working on SAS. So, when I heard that SAS launched a free version (late May) – I was all excited! My initial reaction was that if SAS becomes available for free, it would become the preferred choice of analysis tool for people entering the industry.
It is easier to learn (compared to R), has the highest market share (in terms of number of jobs) with awesome customer support (read more details here). Usually, high cost is the biggest hindrance to its wide usage among freelancers, consultants, trainers and data scientists. If SAS offers a version for free – it would be like having the best of both worlds. Did it actually turn out to be so? Well, we will figure out in a few minutes!
 

The free offering – SAS Analytics U (or SAS University edition)
Let us look at what SAS has to offer in its free version and then try and understand what they were aiming to achieve. The free offering, which is called SAS Analytics U, is free for download for teaching, learning and research.
This is what SAS says on its overview page:
Our new software for teaching, learning and research in higher education is now available to download for free. You’ll gain access to the latest SAS software and programming environments for data analysis and reporting.
Sounds good! However, I am not sure what classifies as learning and research here. I participate in Kaggle competitions for the learning – so can I use it there? I am not sure. I think it is better to check. You can download the software here.
 
Components of SAS Analytics U:
I thought, I’ll download it anyway and use the software for teaching it to a few people, I used to coach. I was in for the next surprise as soon as I got into details. These are the components which are available for download in SAS Analytics U:
Base SAS
SAS / STAT
SAS / IML
SAS / ACCESS Interface to PC Files
SAS Studio
My eyes stayed wide open when I saw this list for the first time. There is no SAS / GRAPH in this package! How do you complete an analysis without the use of PROC GCHART? Who uses SAS without accessing SAS / GRAPH? This looked like a serious limitation to me initially. Thankfully, SAS has included ODS graphics into the package. So, you need to use PROC SGCHART instead of PROC GCHART. The output of SGPLOT looked better compared to GPLOT and SAS has provided easy GUI access in SAS Studio, so that even a beginner can learn SGPLOT.
There are a few more components which SAS could have added, e.g. SAS / ETS, SAS / OR or SAS / QC. SAS Enterprise Guide, Enterprise Miner or Visual Analytics would have been icing on the cake – but for now, we will have to live with what SAS has provided! I am told there is more coming in July. Till then my decision trees have to wait!
 
Installation:
You need to set up a virtual environment (through Oracle Virtualbox or VMware Player), both of which are free to download for non-commercial uses. So, I went ahead and did so. Virtualbox seemed to slow down even my top end monster - Quad core i7, 16 GB RAM and did not work for some reason. On the other hand, VMware ran fine and was up as soon as I finished downloading 1.5+ GB of installation.
I loved to see the SAS graphic back on my screen. You run SAS Studio through your browser. The interface is neat and should be familiar to anyone who has used base SAS in the past.
I tried installing the same on Ubuntu 12.04, but the SAS Studio failed to load – something to research at a later point.


Performance:
SAS has put on limitations to how much memory / processing power you can use. You can only use a maximum of 1 or 2 GB RAM with 2 processors. So, clearly, SAS is not offering Analytics U for advanced learning. This was the second deal breaker for me (first one being the missing components)
 
Community:
One new initiative worth mentioning is the community SAS is trying to create with Analytics U. It has Facebook, Linkedin and YouTube channels. However, the community is very small right now and lacks the quality of discussions you might see on the R or Python in their communities. Let us see how it grows and whether it converts into a meaningful community in future.
 
Overall Verdict:
It is heartening to see SAS reacting to open source softwares and their rise in recent years. It has definitely made a good attempt in pushing out first product. However, the current product is aimed towards new learners. I think they have put too many limitations to appeal to serious learners.
Having said that, it is not too far from a product which can provide a SAS career path for people entering the industry. If they can add the (few) key missing components, it can become a stepping stone for the future analysts / data scientists. Till that time, R and Python continue to reign this place. I am not moving away from any of them in near future.
It would be interesting to see what SAS comes out with in a few days. I am following this space closely.
If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Analytics Vidhya Apprentice - programme to graduate with recognition for your knowledge!","Kunal Jain",2014-06-10 19:05:00,3,"
							
										
						It has been more than a year since we started our journey to change how Analytics knowledge flows in communities. The experience has been rewarding, fulfilling, gratifying and filled with a lot of learning at the same time. In this short span, we have become one of the leading analytics blogs (in India) and have started transforming Analytics Vidhya into a knowledge portal.
Today, we unveil, a very special programme, Analytics Vidhya Apprenticeship (AV Apprenticeship) or the Data Science Apprenticeship, to take a leap forward in this direction.
The genesis of this programme lies in the numerous interactions, we have had with people across the globe in last one year. What stands out in these discussions, is the talent and recognition imbalance in the industry. Whether it is an experienced analyst (read lone warrior), trying to influence his customers or management about the benefits of investing in cutting edge infrastructure, or some one who has just graduated from one of the various certifications across the globe, recognition of your knowledge does not come easy.
Through this programme, we are set to change this forever (at least for a few people).

The proposition:
The Analytics Vidhya team will extend it’s learnings to a small batch of professionals, and mentor them to achieve great heights in their analytics career. For 6 months, these people will work along with the team, and gain practical knowledge about the subject. Some potential areas of work are:
Blogging – We train you on what it takes to become a successful storyteller / writer. Your high quality output gets published on our platform under your name and stays there even after the programmel (However, the content will come under the Analytics Vidhya Copyright)
Interactions / advice people on social platforms – You solve peoples’ queries and give them specific advice on various forums – be it Quora / Stackoverflow / Github / Kaggle
Active participation on Analytics Vidhya forums – It could be from starting discussions to answering people on specific subject matter.
Participate in data science competitions – You participate in various data science competitions and get mentored by the Analytics Vidhya team
Interact with people in industry – Conduct interviews with Industry leaders
Consultancy projects – Work on actual live problems to gain knowledge
Conducting guest lectures on behalf of Analytics Vidhya
Solve case studies
 
What do you gain from this programme?
Loads…starting from interactions with several like minded people, to interviewing experts from the Industry, to solving real life problems – you learn from every exercise. Your knowledge is showcased to all the readers on our platform, and hence you see the recognition coming accordingly.
The idea is, to create a unique programme, which focuses on implementing & showcasing your knowledge practically.

What do I need to pay for this awesome opportunity?
Well…nothing financially. You need to put in a lot of effort to go through this rigorous program. We will get the content and any financial rewards that come in from the work we do together.
However, in order to make sure that we only get serious candidates for the programme, we have introduced a very nominal deposit, which is INR 6000/- (~$100). The entire amount would be refunded to you, the day you pass out of the programme. In case you don’t finish the programme, this deposit would be treated as a compensation for our efforts (and hence will not be refunded).
You will need to clear a tough, multi-stage selection process to enter the programme. Following that, there will be a 6 month long rigorous apprenticeship.
P.S. – You only get the certificate post completion of programme. There are no marks for participating.

 
Selection process:
Stage 1: Complete the application on Analytics Vidhya website by 25th June 2014. This application includes several essays about your experience and the your reasons to join the programme. Next stages of selection would start as soon as you fill in the application and seats will be provided on first come, first serve basis. So don’t wait - go and fill in the application.
Your application will be selected / rejected basis your knowledge, passion and your essays. So, please fill them accordingly.
You will get more information about upcoming rounds after you register for stage 1.
P.S. At this stage, you don’t need to make any payment. We will collect this at a later stage.
 
Few other details:
This is not a traditional programme, but a practical hands on programme in Analytics. Do not expect class room sessions. Do expect a lot of analytics related discussions.
The onus to learn lies with you. If you need to learn any technical stuff, your mentor will direct you to the right place, but it is you, who needs to learn
Expect a lot of feedback and guidance on your work, be it on a blog post or a data science project.
It will be expected of you, to spend at least 4 hours every week as part of the programme.
We are very excited to launch this programme, and can’t wait to interact with you participants. Do let us know your thoughts / feedback / suggestions on this programme. If you need more details, feel free to write to me about it.
If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Must have books for data scientists (or aspiring ones)","Kunal Jain",2014-06-05 00:50:00,4,"
							
										
						I am back to one of my favourite topics – books! To double up the excitement, this time the list is for data scientists (or aspiring ones). Unlike the previous lists, these books are not for the light readers. These books are meant for people who enjoy programming and statistics – just the kind a data scientist should be.
As can be expected, there are 2 languages which deliver the subject matter in these books (no points for guessing which ones. If you can’t, this article is not for you) – R & Python. If you are a data scientist (or aspiring to be one), you should consider these books as must have in your library. Due to for some strange reason – I personally prefer these books in hard copy and not in Kindle format – but that is a personal choice. I probably like my walk up to my book rack thinking which book would be the best to refer to, for the problem, I might be working upon.
Here is the list of books (first the ones on R and then on Python):

1. R Cookbook by Paul Teetor
This is simply the best book to start your journey with R. It contains tons of examples and practical advice on a wide range of topics like file input / output, data manipulations, merging and sorting to building a regression model. For a starter in R, this book becomes your best pal during the initial testing time.
While the book is aimed towards starters, it still remains a prominent feature of the library of any data scientist.
 
2. Machine Learning for Hackers by Drew Conway & John Myles White
I think this book actually has a wrong title. I dropped purchasing it twice before giving it a shot (which happened only because of a recommendation from a close friend). This book is meant for data scientists and not hackers. I don’t know why the title says so. A very practical manual for learning machine learning, it comes with good visuals and you can get a copy of codes in Python (original book is based on R).
 
3. R graphics cookbook by Winston Chang
You can’t be a good data scientist unless you master the graphics in R! There is no better way for visualization, but to learn ggplot2. Sadly, learning ggplot2 might seem like learning a completely new language in itself. This is where this “cookbook” comes to rescue. The recipes from Winston are short, sweet and to the point. Buy this and it is bound to end up as one of the most referred book in your library.
 
4. Programming Collective Intelligence by Toby Segaran (popularly referred as PCI)
If there is one book you want to choose, out of this selection (for learning machine learning) – it is this one. I haven’t met a data scientist yet who has read this book and does not recommend to keep it on your bookshelf. A lot of them have re-read this book multiple times. The book was written long before data science and machine learning acquired the cult status they have today – but the topics and chapters are entirely relevant even today! Some of the topics covered in the book are collaborative filtering techniques, search engine features, Bayesian filtering and Support vector machines. If you don’t have a copy of this book – order it as soon as you finish reading this article! The book uses Python to deliver machine learning in a fascinating manner.
 
5. Python for Data Analysis by Wes McKinney
Written by Wes McKinney, this book teaches you everything you need about Pandas. For the starters (not sure why you are still reading this article), pandas are Python’s way to handle data structures. Except for the title of the book (which I find misleading), I like everything else about this book. It contains ample codes and examples to leave you capable of performing any operation / transformation on a dataframe in Python (using pandas).
For the advanced users, if you already know pandas, you should look at this presentation from Wes on what are the shortcomings of pandas.
 
6. Agile data science by Russell Jurney
A recent addition by O’Reilly, this book looks like a must read for data scientists. The focus is on using “light” tools, which are easy to use and still get the work done. This is currently on my reading list and I’ll update more details once I have read it.
 
These are the 6 must have books, if you are serious about being a data scientist. There are a couple of additional Python books, which you can consider - Natural Language processing with Python by Steven Bird et al and Mining the social web by Matthew A. Russell. The reason I have not kept them in the list is because you can find a lot of the information in these books easily on the web.
 
If you would have noticed, all the books I have mentioned are from O’Reilly – I think it is a tribute to the fascinating collection of books they have provided! What do you think about the list? Any other recommendations you would want to add to this list? Have you read any of these books mentioned above? Do let me know through the comments below.
If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"The lack of analytics work experience and how to overcome it?","Kunal Jain",2014-05-26 00:11:00,1,"
							
										
						
Let me present the two sides of a debate going on in my mind:
One of the most common reasons quoted for rejection of Freshers (or people with non-analytics experience) for analytics jobs is lack of experience! What an irony for someone young wanting to enter the field! They can’t get experience without a job. They can’t get a job without experience!
A quick survey of about 20 freshers (trying to enter the world of analytics) I did before writing this article, revealed that on an average, they had been rejected 25+ times due to lack of work experience – even for junior roles.
It is dis-heartening to see the difficulty faced by these people!
Having said that, I have been on the other side of the table for more than last 4 years. Let’s look at what one of my colleagues has to say:
We were expanding our in-house analytics team from 2 members to 5 members. Instead of hiring an experienced professional, we thought of bringing really smart people (straight out of college) on-board. Post their joining, we took them through a very structured induction and training programme and even got them certified from the SAS training institute (all of this, at the cost of delaying important projects for business owners). We were hoping to get loyal and smart analysts, who would pay for these investments through high quality work.
3 months after joining, the first person left to start his own venture. 6 more months later, the remaining 2 people left the Organization to join other companies offering them better salaries!
Imagine the kind of costs, the team leader had to bear. Loss of business from key stakeholders, loss of time, knowledge and efforts. To make it worse, they were back to where they started – a 2 member team! I think, he was lucky that he did not get fired!
P.S. While I am saying Freshers, what I refer to is people with no analytics experience.
 Let’s understand the problem:
How do we solve for this problem? Lets look at the reasons why employers prefer people with work experience over Freshers:
Attitude towards work and Maturity in taking decisions - Most of the employers feel that people out of college take immature decisions and need to be mentored on attitude towards work. This is less of a concern with people shifting careers, until and unless there is a specific reason from the past.
Time to bring a new person up to speed - An analyst typically takes anywhere between 2 – 6 months to come to speed with the subject. It can easily be a year before you see anything significant coming out of an analyst.
Short life span - Because of the demand for trained professionals in the industry, Freshers tend to leave jobs either in search of higher pay or expectation of better role / work. Leaving for better work / role as such should not be a problem, as long as it is actually the case.
A few things which can work as strengths for freshers:
Ability to learn - People fresh into their roles, typically have higher ability to learn. It might be because of higher willingness or because they have been just out of an environment conducive to learning.
No need to un-learn old habits - This is a big advantage with freshers. You can mould them the way you want. You want them to put a framework before touching data – train them in that manner. Try doing it with someone who is in the practice of doing it the other way.
Ability to look at everything with a fresh perspective - How good can an analyst be, if he does not ask questions? Experienced people find it difficult to do so, as they may already have a perspective on the subject.
So, here is the strategy, which a fresher needs to follow:
Address the concerns of the employers (listed above) and emphasize on your strengths.
How do you implement this?
Well, everyone needs to carve out his or her own way. But here are a few tips, which might get you started:
Start learning analytics early - there is no substitute for hard work and real knowledge. Even if you spend 3 – 4 hours every week reading about the subject, or undergoing trainings at MOOCs (e.g. Coursera, eDX), doing certifications in the last 2 years of your college, you would know more than a lot of people in the industry (at least the theoretical part of the journey).
Become an expert on at least one programming language - You can choose the language you want. I would choose Python or R, if I was passing out in near future.
Participate in contests - Now that you have the knowledge, apply it! You can participate in coding contests, hackathons or analytics competitions on platforms like Kaggle. It gives you immense learning while you compete alongside the best data scientists in the world.
Contribute to these communities - Whether it is your contributions on Github / stackoverflow or Kaggle, all of them will count immensely when scoring a point with the recruiter.
Attend events and network with people in the industry - See if your college is arranging some talks / conferences on the subject. Are there seniors / batchmates you have, who are already in the industry? Reach out to them.
Get an internship - Internships are a good way to gain work-experience. We have 2 summer interns in our office this summer and none of them had analytics work experience before. By the way, they are doing a fabulous job (to all those recruiters who think work experience is mandatory)
Try getting a job in companies which are open to hiring freshers / non-experienced people. For example, companies like Mu-Sigma, Fractal, WNS, Citi etc. are open to hire people without prior work experience.
If you work on even some of these pointers, you can address the concerns mentioned in the article before. Next, emphasize on your strengths and make a mature decision – no short term decision please!
Are you a recruiter / a fresher / or experienced analytics professional? What is your take on this debate? Who do you prefer to recruit? Why? Any tips to freshers wanting to join the industry? Please share them through comments below.
P.S. I normally don’t do this, but since the topic is very relevant, I thought I should mention. We are starting our first training on Analytics from 1st of June. This training is aimed towards freshers wanting to enter into analytics industry and is the first training you should attend on the subject. You can check the details by visiting this page. I have decided just now to run a 2 day discount for people who have read this article and are interested in the training. Please use coupon AV10 to get 10% off while making the payment. Your payment should reach us by 12:00 midnight (India time) on 27-05-2014.
If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our Facebook page.
photo credit: Caro Wallis via photopin cc
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"9 best practices for analytics talent management","Kunal Jain",2014-05-19 05:40:00,2,"
							
										
						Interviewing for Analytics positions is fun! I have conducted hundreds of interviews and I still grab most of the opportunities which come my way. However, right recruitment is only the start of the journey. The next most important thing is to manage this talent effectively. This article aims to lay out these practices for the benefit of new managers and leaders. These are practices, I have learned over the years through a lot of practice.

I will keep these best practices specific to anlaytics talent management and refrain from putting up any generic advice (e.g. Keep a healthy work life balance! ). You can search them elsewhere.
Understand what they (analysts) want to do? Make sure they are not in the wrong place! – This is the most critical takeaway for any manager. Honest understanding and evaluation of the ambitions / motivation of the analyst can itself avoid most of the big management disasters. The evaluation, which, I try and do in the initial few days, typically results in one of the following
Person is not capable of a technical role. Go back and check why was he hired in the first place. Also, do a favour by putting him into a more relevant role
Try and see which profile does he / she fits in – Programmer, Statistician, Data Scientist, Consultant, BI programme manager, Developer, Engagement Manager

Set aside time for regular mentoring and work catch up – probably daily in initial days; Feel free to do so in an informal environment, once in a while. The frequency can go down once both of you are comfortable with each other’s way of working.
Ask them to surprise you, especially in the initial days! Typically a good performer will surprise you with their work, multiple times within first 90 days! If this does not happen, you are dealing with mediocre talent or low motivation. Try and get to the bottom and address quickly.
Make them knowledge junkies: The best of analysts love learning new tools and techniques. Also, given the fast pace of changes in this industry, it is best to put aside time for structured learning and development.
Give them more and more ownership: Over time, this is the best gift for your best analysts. Give this and they will love it. Tell them the business problem and leave them on their own. Help them with brainstorming! Mentor them! But, at the end of the day, give more and more ownership to them. Or, as one of my mentors says, “Ask them to get into bigger shoes!“
Challenge them: Ask them to think more! Ask them to compete against the best of analysts across the globe through various data science competitions across the globe.
Nothing beats an honest, clear, objective and unambiguous constructive feedback: Don’t mix up the words here. Communicate your feedback honestly and objectively in a constructive manner.
Set up opportunities for getting mentored by the best analysts (with in / outside team): Learning from someone, who has been through similar situation some time back and has come out flying, can give completely fresh and independent perspective.
Finally, see career progression by getting in the shoes of the analyst: You can only do justice to an analyst’s career, if you understand what he wants (remember practice # 1?). Whether your best talent is moving on from his current job or wants to take a sabbatical to try out something new, do the right thing in his interest (rather than yours). This might be difficult in the short term, but is the right thing to do!
What do you think about these best practices? Do you have more to add on? Do you find it difficult to implement some of these? What challenges did you face? I’ll be interested to hear your perspective.
If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.
photo credit: ffaalumni via photopin cc
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Planning a late career shift to Analytics / Big data? Better be prepared!","Kunal Jain",2014-05-12 00:51:00,1,"
							
										
						I feel lucky to be part of the data revolution happening around us. Because of the attention and the focus on Analytics / Big Data, there are quite a few people considering a late career switch.

I get quite a few queries asking my advice on this matter regularly. Here are a few samples (after removing names):

I am an equity analyst (finance background) and have 10+ years experience. I don’t have any background in IT. I have a background in chemical technology (BE) and MBA finance.
After all these years and entirely different background; how easy/difficult is to pursue fresh career in these areas – Analytics and Big data. Will there be opportunities for such diverse background on immediate and Long term basis.

Here is another one:

I have 6+ years of experience in IT consulting, none of them is related to Analytics. But I want to move into analytics. Is there a way to do that? Please advise. 

Yet another one:
I came across this site as a part of my quest to change my career track. I have 10 years of experience , out of which last 5 years is in institutional relationship management cum acquisition in banking sector. Prior to that I have 1.5 years sales experience in FMCG sales. I have done MBA(mktg) and B.E. Please guide me,1. Would my existing profile be adaptable to career in business analytics?2. How easy (or difficult) it would be to get job in business analytics?3. Which course and from which institute would relatively assure me of a job immediately in analytics?Thanks
Hence, I thought of expressing my thoughts for the benefit of a bigger audience. Also, through discussion with other people in this domain, I am hoping to pull in broader views on the subject.
Please note that rest of this article assumes that you are looking for a hands on core analytics role (where you analyze the data yourself) and not a supporting role (e.g. Pre-Sales for Analytics products, Business development for a consultancy)
A few facts you should consider:
The rosy picture (recruiters running behind you) you thought, would be at least 3 – 5 years away. The first few years would be very arduous. More so, if you don’t need to be an Individual Contributor in your current role.
Analytics / Big data is a knowledge intensive domain. You can do well only if you gain knowledge and work hard regularly.
A lot of good companies believe in hiring smart people straight out of college and training them internally. These Organization would not think about hiring you.
In an ideal world, there should be no bias against people with experience. They should be judged only on the knowledge and skills they possess and their willingness to learn / work. Sadly, this is not true. A career shift (especially late in career) is often looked suspiciously, until you have received outstanding recognition in your previous roles.
Would your work experience be considered?
The answer depends on the domain of your experience:
Non technical experience will not count in your analytics jobs – the only benefit you might get is that the interviewer can expect you to be more mature with your thought process / decision.
Technical experience (programming / SQL / server) gets counted, but will only be considered as equivalent of 1 – 2 years experience.
Consulting / research would count more. May not be counted 100%, but healthy fraction would be considered.
How soon can you get a job?
Well, bake in any where between 3 months to a couple of years depending on your learning agility, problem solving skills, communication and presentation skills. If you are outstanding on these attributes, you should get a job as soon as you get some technical skills under your belt. An average person typically takes 6 – 12 months after gaining necessary technical skills. If you take any thing more than 12 months, you should try and re-assess what is going wrong.
P.S. This would vary from geography to geography. The above answer is applicable in markets where there is big un-fulfilled need of analysts.
OK, you have thought enough. You are absolutely sure that your life would not be complete without a shot at analytics. What next?
In case you are absolutely sure, here are the next steps:
Join a course on basics of data science on Coursera / eDX to get first hand flavor of being an analyst. Here are some good courses:
The Analytics Edge – Intensive 12 week course which should give a goof headstart
Data Science speicialization from John Hopkins University – Relatively more relaxed, but longer duration. A collection on 9 courses.

Make sure you do all the assignments. This is your chance to get the experience first hand.
Subscribe to some of these blogs / communities to regularly read about the subject:
Analytics Vidhya (what else did you expect first!)
Occam’s Razor (For people interested in Web analytics)
Smartdata Collective
KDNuggets

Be a part of Linkedin Groups related to analytics -
Advanced Business Analytics, Data Mining and Predictive Modeling
Big Data / Analytics / Strategy / Predictive and Business Analytics

Once you have followed these blogs / communities for a while (say at least a month after your course at Coursera), you can look out for certification courses to begin your journey.
My final advice:
If you are not in deep love with data and can not spend hours slicing and dicing data in front of a computer, analytics may not be your cup of tea.
Take this up only if you tick all the boxes below:
You are absolutely crazy about this industry. You can’t help but analyze any numbers you come across – I play with numbers on the number plate of any vehicle which passes me.
You have undergone a few courses on Coursera / eDX and have excelled at them. You have submitted all the assignments and have scored extremely well.
You have the perseverance and motivation to undergo 2 – 3 years of arduous work learning about a new knowledge intensive domain.
You are willing to spend a lot of time as Individual Contributor
Lastly, if you have family support in this move / decision, it will make the move less painful (or more enjoyable depending on the way you look at).
P.S. The aim of this article is not to dissuade people wanting to join Analytics Industry. I know a lot of people, who made this switch in middle of their career and they have not been more happier in their career ever before! Their passion for analytics and numbers comes across even if you spend 5 minutes talking to them.
If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Lead / Assistant Manager, Web Analytics, Policybazaar, Gurgaon","Kunal Jain",2014-05-07 15:42:00,4,"
							
										
						We are looking to hire a seasoned Web Analyst with the ability to measure and analyze the nature of the customer’s interaction with our website PolicyBazaar.com
 
Designation: Lead / Assistant Manager
Location: Gurgaon
About Policy Bazaar:
PolicyBazaar.com is India’s largest online comparison platform for insurance and other financial products and one of the fastest growing e-commerce companies in India. We specialize in helping the customer to make easy purchase decisions related to their personal finance.
Our strength lies in our ability to fuse technology, operations and people to provide the best platform for customers to compare and buy insurance. We are the pioneers in our industry and were voted the “Best Financial Website of the Year” at the IAMAI Digital Awards 2014. PolicyBazaar.com is backed by leading investors like InfoEdge (Naukri.com), Intel Capital and Inventus Capital.
It is this strength and customer centric approach that has earned us credibility across the nation and this is why more than 1 million unique users visit PolicyBazaar.com every month to compare and buy insurance plans.
Our team comprises of people who have experience across e-commerce, insurance, financial services and technology. We are a company that believes in empowering our teams to take decisions that allow us to have a fast turnaround time and get things done faster. We are thought leaders when it comes to the insurance sector and we look forward to having future leaders join us in our mission.
 
About the RoleWe are looking to hire a seasoned Web Analyst with the ability to measure and analyze the nature of the customer’s interaction with our website PolicyBazaar.com. The Web Analytics Manager will be responsible for collection, analysis, and presentation of various web metrics and test results. Working with Google Analytics in particular and other web analytics platforms, you will develop regular and ad-hoc reports, analyze trends, summarize and present findings, and articulate key insights to internal stakeholders. He/she must be able to understand relevant technology applications, complex web ecosystems and best practices and apply this knowledge to work on a daily basis.
 
Responsibility:
Expert knowledge of web analytics tools; particularly Google Analytics
Expertise with attribution methodologies, A/B and multivariate testing
Internalize customer insights and champion the customer
Measure and analyze the larger picture of business performance; including Click stream data – Traffic, pathing, abandonment, purchase behavior, etc
Testing and Behavioral Targeting of Customers, Optimizing various Purchase Paths
Work with testing and analytics team to identify & track online KPI, provide actionable insights and recommendation to improve online business performance
Use business knowledge, data knowledge and domain knowledge to provide ongoing business insights
Ideal profile:
Bachelor’s degree in a quantitative, business, or marketing discipline OR PG – MBA from a Premier Institute with an Engineering degree
Atleast 2+ years of experience with analytics tools in particular Google Analytics
2+ years of business analysis experience in leading eCommerce portals
Must Haves – Google Analytics, Webtrends Analytics experience
Experience with tag management tools is a plus
Additional experience with mobile APP analytic tools would be preferred
 
Interested candidates can apply by sending their CV to rohits@policybazaar.com
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Manager / Sr. Manager, Policybazaar, Gurgaon","Kunal Jain",2014-05-07 15:30:00,4,"
							
										
						Exciting role with India’s largest Online comparison platform for Insurance. Get involved at a strategic level and make an impact by defining and implementing analytics strategy leading to business growh!
 
Designation: Manager / Sr. Manager
Location: Gurgaon
About Policy Bazaar:
PolicyBazaar.com is India’s largest online comparison platform for insurance and other financial products and one of the fastest growing e-commerce companies in India. We specialize in helping the customer to make easy purchase decisions related to their personal finance.
Our strength lies in our ability to fuse technology, operations and people to provide the best platform for customers to compare and buy insurance. We are the pioneers in our industry and were voted the “Best Financial Website of the Year” at the IAMAI Digital Awards 2014. PolicyBazaar.com is backed by leading investors like InfoEdge (Naukri.com), Intel Capital and Inventus Capital.
It is this strength and customer centric approach that has earned us credibility across the nation and this is why more than 1 million unique users visit PolicyBazaar.com every month to compare and buy insurance plans.
Our team comprises of people who have experience across e-commerce, insurance, financial services and technology. We are a company that believes in empowering our teams to take decisions that allow us to have a fast turnaround time and get things done faster. We are thought leaders when it comes to the insurance sector and we look forward to having future leaders join us in our mission.
 
Responsibility:
Work closely with product and business teams to answer critical questions
Be comfortable handling high volume, complex data
Be involved in Machine Learning, Data Mining, High Volume Analytics, Algorithms, Pattern Mining
Be able to make sense out of ambiguity
Ideal profile:
MBA / Engineer / Statistician From Premier institutes.
Hands-on work for at least 2-6 years (sample projects: Recommendation engines, conversion propensity, customer segmentation and experience, risk Scorecards
Skills required:
Wonderful sense of data and statistics
Ability to combine Mathematics, Tools/ Technology and Business.
Build and fine tune advanced statistical models
Experience / expertise on tools like SAS / SPSS / R
Excellent communication skills
 
Interested candidates can apply by sending their CV and cover letter to jobs@analyticsvidhya.com with subject as Manager / Sr. Manager, Policybazaar
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Analytics events in 2014 - India and abroad","Kunal Jain",2014-05-05 03:54:00,2,"
							
										
						One of the queries I frequently get on my blog is:

Which events / conferences are happening in India and are they worth attending? 

So, I thought I will collate the resources about these articles and give my opinion about them.
I have collated the events happening in India and pointed to places which already have a collated list of events outside India (with my recommendations).

Disclaimer: Before we go any further, I have to admit, I am not a big fan of events and conferences! I go to these events only if I am convinced that they are sure to add value. Also, I do not have any first hand experience of events outside India. I am giving the recommendation basis what I have read on the Internet and assimilated during my regular reading.
Analytics events in India:
1. Gartner Business Intelligence & Information Management Summit, June 9 – 10, Mumbai – A must attend event for all BI leaders. Expect who’s who of the BI industry in this event. The sessions are divided into 3 tracks and workshops. The topics vary from the assessment of your BI strategy to the integration of Hadoop in your BI landscape. Also, there will be opportunities to ask questions specific to your problems, which is a big plus.
You can see the latest agenda here. I would recommend this every to BI leader who has the resources to attend this (it is a bit expensive, but worth every rupee).

2. NASSCOM Big Data and Analytics Summit, June 27, Hyderabad – The agenda looks to have a mix of entire analytics spectrum. Probably aimed to have mid-management / Senior management in the participants. The speakers also come from diverse background. If you want to attend this event, the early bird discount ends today (5th May 2014). Register and save on cost, if you are keen to attend this.

3. PyCon 2014, 26 – 28 September, Bangalore – I had mentioned this event while recommending tutorials from PyCon 2014, USA. The event would not be as big as the one in the US, but would still be a good opportunity to meet the Python community. Also, session details are not available currently, so it is difficult to comment how many sessions would be there on data analysis.

4. The fifth elephant, 23 – 26 July, Bangalore – OK, this is not an analytics event strictly! But, this would probably leave you with more knowledge / leads about Big data and Big data analytics, compared to any other events mentioned above. A word of caution – Go to this event, only if you want to get into details of big data and can cope up with IT jargon and technical details.

Analytics events outside India:
As mentioned, there are a few websites listing these events already. I usually refer KDNuggets list of meetings. The list differentiates business oriented and research oriented events and lists most of the events across the world.
Which are the events I would recommend? Here is the list:
1. Chief Data Officer Summit, May 22 – 23, San Francisco – One of the various events from Innovation enterprise.
2. Dataedge, May 8 – 9, UC Berkeley School of Information – for meeting the mathematicians, computer scientists and data scientists
3. Predictive Analytics World – Various chapters across the globe. PAW, founded by Eric Siegel, hosts one of the most recognized event in the industry. One not to miss
4. Gigaom Structure 2014, June 18 – 19, San Francisco – For looking at what Thought leaders across the globe are doing.
5  2014 INFORMS Conference on the business of big data, June 22 – 24, San Jose – Another event hosted by veterans
6. Discovery Summit, JMP, Sept 15 – 19, North Carolina – How could a SAS summit not be in this list?
Each of these conferences has an impressive list of speakers and should leave you with a lot of information, if you happen to attend them.
Which events are you attending this year? For what reasons? Do add your recommendations through the comments below.
If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page
 
 
 
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Leader - Data Science, Bangalore (15+ years experience)","Kunal Jain",2014-04-26 13:40:00,7,"
							
										
						Do you have a innate ability to turn data into action? You have a learning agility, have applied Machine Learning, Text mining, Predictive model concepts before? This job might be for you.
 
Designation - Chief Data Scientist
Location - Bangalore / Mumbai / Pune
About employer - Confidential
 
Skills / Experience required:
Min 15 years of experience on advanced analytics e.g. ML,Text Mining Concepts
Carry a strong education pedigree from a Tier-1 school with graduation in Statistics/Mathematics/Computer science/Quant discipline
 
Compensation: Competitive
 
Interested candidates, please contact:
Manav Das,
Rinalytics Advisors
manav@rinalytics.com 
Telephone: +919986460308
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Analytics Training recommendation - Tutorials from PyCon 2014 - USA","Kunal Jain",2014-04-24 22:54:00,5,"
							
										
						I am usually very selective about attending conferences!
Its not because I don’t like networking or talking to people. Its because I have a very high bar on time used in networking. I feel self-learning and discussing actual problems (when working on them) with people results in far more value for the time spent. However, there is one conference I want to attend at some point in future – PyCon (USA chapter).

For those who don’t know, PyCon is the largest annual gathering for the community using and developing the open-source Python programming language. While the conference is aimed to entire Python community (and not just data scientists), there is still a lot of learning and networking opportunities for data scientists. In India, the event is still not as big, but its recognition in industry is only a matter of time! This year, it is scheduled to be in September.

PyCon 2014 – USA happened recently (9th – 17th April) and I am sure it would have been a great platform for like minded people to interact and learn. As part of this conference, leading industry experts took a few tutorials. These tutorials are awesome source of knowledge, if you want to become awesome in using Python!
Here are my recommendations from these tutorials and the reasons why I like them:
Exploring Machine learning with Scikit-learn and Diving deeper into Machine Learning with Scikit-learn - Just the right place to start learning about machine learning in Python. The 2 tutorials combined provide you with all the knowledge you need on the subject. The second tutorial covers some of the advanced topics like automated parameter tuning and how to scale up using iPython.parallel.
Mining Social Web APIs with IPython Notebook - Coming from the author of popular book, mining the Social Web (O’Reilly, 2013), the tutorial and the accompanying iPython notebook provide you ways to extract data using social media APIs. Aimed towards beginners, the tutorial shows step by step extraction and mining of data. Once you know how to extract this data, you can get as creative as you want – How different are you from likes of your Facebook Friends? How much influence do you carry on Twitter? are just some of the questions to start your journey.
Data Wrangling for Kaggle Data Science Competitions — An etude - A must read if you have participated in Kaggle competitions or want to at some point in near future. Krishna’s insights into how the competitions work and what data providers might do before providing data can save you months of scrolling through Kaggle Forums!
Hands-on with Pydata: how to build a minimal recommendation engine - If you have not built a recommendation engine till now and are fascinated by the idea of building one, this is just what you needed. A tutorial which covers all the basics and quickly moves towards practical applications and the idea of iterational improvement.
I am left with another one Bayesian statistics made simple to watch – I am hoping that it is a good one as well. All these tutorials are awesome resources for learning Python for data analysis from some of the best minds across the globe.

Do you have any other tutorials which you would want to add to the recommendations? Please feel free to do so through comments below.

A quick update on our blogoversary contest:
We ran a 3 day contest asking 15 questions across our social media platforms. We saw awesome participation with really tough competition during these three days and are pleased to announce Datta Dharanikota as our winner. He gets a 3000 Rs. voucher from Amazon.
 A few special mentions for their superb participation:  Nitesh Singh, Dhanman Gupta, Nimit Gupta, Raghavendra Reddy, Avinash Asuri, Jenny Srinivasan, Sateesh Kumar, Aditi Vij - well done guys and thanks for your participation! I hope that the contest gave you a small glimpse of the work we have done in last year! 
If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Manager-Big Data Analytics, Bangalore (3 - 6 yrs experience)","Kunal Jain",2014-04-22 20:45:00,3,"
							
										
						Passionate for continuous learning, experimenting, applying and contributing towards cutting edge open source technologies and software paradigms? Can you build and manage a high performance team? This job requires exactly this.
Designation - Manager – Big data analytics
Location - Bangalore
Education - Bachelor’s/Master’s degree/PhD in Computer Science/ ECE, Information Systems, or other related field
Skills required:
Experience on Hadoop and other large scale distributed computing platforms is desirable
Experience and background in one or more of the following areas: Machine learning, information retrieval, data mining, large scale analytics, distributed computing, algorithms, large scale data visualization and pattern mining
Experience with C++, SQL, Unix/Linux Shell scripting is a plus
Passionate for continuous learning, experimenting, applying and contributing towards cutting edge open source technologies and software paradigms
Build and manage a high performing and highly engaged team.
Passion for detail and accuracy with strong verbal and written communication skills
Interested prospects, can write to ambika.cs@rinalytics.com for a confidential conversation.
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Data Mining Scientist - Videology, Baltimore, MD","Kunal Jain",2014-04-22 14:03:00,3,"
							
										
						If you have a passion for data and want to work with a talented, motivated, fast-growing, and fun team, this might be the right place for you.
 
Designation - Data Scientist
 
Location - Videology
 
About employer -  
Videology is a leading technology company in the digital advertising industry. Our mission is to provide our clients with the ability to target their advertisements to exactly the right person. We achieve this objective by collecting massive amounts of data from diverse sources, and solving a large-scale dynamic, online optimization problem. Our Research and Development (R&D) group – which is responsible for creating the machine learning and optimization algorithms that power all aspects of our business – is currently looking to hire a Data Mining Scientist to help design, implement, and curate the next generation of our predictive technologies. www.videologygroup.com
 
Responsibility -
 
Research, design, and develop new predictive technologies to help Videology leverage its data to support Videology’s business.
Work independently and take ownership of existing and new projects through their life cycle.
Support Videology’s ongoing research into ad network optimization
Support existing predictive modeling and optimization technologies.
Work closely with Software Development and Product Management teams to implement and deploy research systems.
Help with ad-hoc data analysis needs from other business units
 
Recommended Work Experience -
 
B.S/M.S/Ph.D. in Computer Science, Applied Mathematics, Statistics, Operations Research or related fields.
0-3 years of experience in machine learning in an academic or industrial setting.
 
 
Skills required:
 
Strong knowledge of state of the art machine learning techniques, including but not limited to maximum entropy modeling, graphical models, association rules, and techniques for online learning and clustering.
Proficiency in at least one programming language such as Scala/Python.
Proficiency in at least one statistical modeling environment, such as R.
Demonstrated ability to perform comfortably in a fast-paced work environment
Demonstrated ability to successfully work independently and collaboratively.
Experience with or ability to get up to speed quickly on SQL/Hadoop/Map-reduce a plus.
Practical knowledge of statistics, e.g. hypothesis testing, experimental design a bonus.
 
Contact:Please submit a resume and a cover letter to careers@videologygroup.com including the position you are applying for (“Data Mining Scientist”) in the subject line.
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"We just turned 1! - Analytics Vidhya Anniversary","Kunal Jain",2014-04-20 02:33:00,1,"
							
										
						Yes, that’s right! The first article on Analytics Vidhya went live exactly an year ago (20th April 2013). In less than a year, Analytics Vidhya has transformed from being just a personal blog to one of the leading analytics communities in India & has earned a lot of recognition globally. We today have a collection of 80+ top quality articles on various topics related to analytics and more than 15 comments on each article on average.

I want to take this moment and thank all our readers & followers, who have been a constant source of encouragement, knowledge and learning. Your engagement keeps us ticking and gives us the motivation to come back to you with high quality content. We strive to make a difference in analytical learning for each and every one of you.
As a special mention, I have listed a few articles which are my favorite. I share a brief reason why they make into the list along with the mention:
 How to start a career in Analytics? and Advanced analytics certifications in India: Why are they my favorites? Simply because of the engagement and the value add we have done through these articles. Both of the articles have 200+ comments from readers individually. Combined every one (including me) has learnt a lot through these articles.
SAS vs. R (vs. Python) – which tool should I learn?: This article is right up here because it reminds me the importance of listening to our audience. This question came up from readers every now and then, we compiled our research, simplified the outcome and wrote this article. Result – Tremendous response from not only our readers but analytics communities across the globe.
Tricky SAS interview questions Part 1 & Part 2: Outstanding quality of questions, their answers and insights on SAS. Must read for any one who claims to know SAS and is looking out for roles with in industry
Interview with industry experts – Top Kaggler and CEO, Fractal Analytics: Because of the knowledge and perspective these articles provide.
The others in this list includes – Regression trick, Qlikview set analysis, tips to solve guess estimate. Do go through these articles if you have missed any of them.
 
Road ahead:
How could this article be complete with out talking about what is happening at our end and keeping us on our toes:
 
A new logo: 
Why do we like this design? Well, for its simplicity and its reflection of what we stand for – Analytics learning and growth for our audience. We are in love with this design. What do you think about it? Do share your thoughts / views with us.
Training: Our first training has received tremendous response. It is a completely online, self-paced training with access to instructors for Q & A. If you are a beginner in Analytics, this would be exactly what you need. The early bird discount is ending soon.
Job alerts platform: We have started putting analytics jobs here. Over the next few months, we aim to list all jobs in analytics on this portal. We hope that this will add tremendous value to our readers and gives us a way to complete the loop (job after training)
New website: With all these changes, our IT team is just managing to keep up with our demands. They are working on a fabulous website, which will be unveiled in coming month. Watch out this space for more!
 
A contest for our lovers & followers:
This is just a way to say thank you for your tremendous support and making us what we are today. Over next 3 days, we will ask 15 questions through our social media channels (Facebook, twitter and Google+). One person who answers all the questions correctly, will receive a gift voucher worth INR 3,000 (~ $45) from Amazon.in. So, over the next few days, watch out for these questions and answer them to show us what you have learnt.
 
We are highly excited about each of these things and what we are doing. Do you have any wishes / thoughts / questions / feedback for us? Do let us know through comments below. As usual, we are looking forward to them.
If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.
 
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"The importance of context for an analyst!","Kunal Jain",2014-04-17 05:16:00,5,"
							
										
						We analysts enjoy crisp, objective and to the point conversations. An ideal conversation for us is when we come straight to the point, discuss and finish the conversation.
Don’t understand what I mean? Here is an example: This is a mail I received in my inbox (name of sender removed). The mail might look crisp, but it does not connect with me. Can you spot the problem?

The problem: It has absolutely no context! No Subject and a 3 word qualification of the person…that’s it. How do I help this person, even if I want to? Imagine what would I do if I am not some one inclined to help? Just delete it.
Don’t get me wrong here. I am as much an advocate of brevity as any of you are. I am not asking for a 2 page long mail here. All I am saying is providing right amount of context is critical to influence people. For example, if this person would have mentioned details like: Where has he done B.Tech. form? Which stream? What are his interests? Why does he want a job in analytics? and what all has he already tried? I could have been of more help to him.
Same principle applies to your projects / analytical work as well. 
Until you provide the right details and context, it would be difficult to influence your stakeholders and business owners. In rest of this article, I explain how this principle applies to various situations in life of an analyst and then provide simple tips to overcome these situations.

Case 1: A business Intelligence professional:
Lets take a scenario, where you are responsible for creating dashboard and monitoring business for a e-commerce website. You have tallied your numbers across sources and put in all the hard work to make sure they are accurate. You present these absolute numbers to business users in form of a neat dashboard, but they don’t use it.

Why? You forgot to include the right context for them.
If you just provide number of sales in last month as a metric, it does not help. You need to compare this number against last year / last month / plan / benchmark. Whether is has improved / degraded? If the Sales were higher, were they because of more visitors or better conversion?
Providing these details makes your easier and more insightful to use. So, next time when you create a dashboard, ask yourself, whether you have provided enough context?

Case 2: A predictive modeler:
Lets take another scenario. You were asked to build next generation fraud detection model for your employer (a credit card provider). You have put in a lot of hard work and are excited about your findings. Just one final step, before you can implement the model – you need to get it approved from credit risk approval committee. You prepare your presentation making sure you explain every minute detail. But guess what? While presenting this model to the committee, you feel like you are talking to stones. No one is responding to the details you provided. Reason – they are not at same page with you.

While making any presentation, you need to start with what is in it for business users? Even if they are aware of the background, quickly recap it before you get into details. This will align everyone back to the business problem. Then, you can explain how your solution can benefit and then present the solution!

2 simple tips to make more impact from your work:
We come across these situations on day to day basis, where we fall short of creating the right impact because we undermine the importance of providing context. So next time when you are preparing / presenting something to your users, keep these simple tips in mind. They might look simple on the surface, but are very empowering:
Start with the user in mind. If you follow this practice alone, you will be good in most of the cases.What are your users interested in? How aware are they? Do they have specific pain points you are aware of? Which product are they focusing on? What was the marketing campaign last month? What is the opportunity size for them? Each of these question impact what you present and how you present. So think carefully about these aspects, identify and address the needs of the customer
Present as if you are narrating a story to a child – Visualize as if you are narrating a story to a child. What details will you include? How will you make it exciting? Include only the relevant details and pack it into a story / structure. Emphasize on aspects you want them to take away. If they are analysts, technical details will excite them more. If they are business users, details about implementation of your solution will matter more.
That’s it. Keep these simple tips in your mind and practice them every time you are talking to your business users.
What do you think about the importance of context? Do you have tips which can help to make more impact from our work? If yes, please share them through the comments below.
If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Technical Consultant - Web Analytics","Kunal Jain",2014-04-13 19:13:00,1,"
							
										
						 
Designation - Technical Consultant – Web Analytics
Location - Bangalore
About employer - Top IT MNC
Responsibility -
Technical Consultants work to gather and understand the client”s unique business requirements and develop and build customized solutions to meet their client’s analytic and reporting needs & help each client implement web analytic tracking code.Developing statistical, Financial and judgment based models to enhance the business decision making.
 
Skills / Experience required:
Min 3-8 years of exp working in web analytics
Must have good understanding of HTML , JavaScript & web protocols
Solid understanding of online marketing
Strong communication skills
Exp with SiteCatalyst, Omniture, coremet or related tools
 
Compensation: INR 18,00,000 – 25,00,000 P.A. Not a constraint for a good candidate
 
Interested candidates, please contact:
Vipul Tandon,
Prasha Solutions Private Limited
vipul.tandon@prashasolutions.com
Telephone: 0-8860140009
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"8 rules for new age analytics learning!","Kunal Jain",2014-04-11 04:45:00,6,"
							
										
						Data science has become one of the most dynamic field. Every alternate month I hear about a start up coming up with next gen tools and products. Cloudera, Neo4j, MongoDB, Ayasdi are some of the companies, which are showing us this exciting future.
With this pace of development, your learning strategy needs to change accordingly. You can no longer learn a tool and be good for 2 – 3 years. You need to learn continuously.

I lay out 8 rules of new age analytics learning in this post. If you are serious about your future in analytics, you should keep them in your heart:
 
Rule 1: Open Source tools are in
Open source tools are growing in their presence by the day, for the right reasons. They are cost effective, have big communities to support and have faster pace of development. They are also the tools available to a scientist (or a freelancer) working on his own.
You can read a more detailed comparison of these tools against SAS here. Except for people just entering analytics industry (with primary concern as jobs), learning R (or Python) is almost mandatory to future-proof yourself.
So, if you want a long term career in analytics, learn one of these now!
 
Rule 2: Democratization and free trials of tools will become the norm
Free trials of tools in individual capacity will become the norm (if it is not already). What I mean is, that more and more companies will offer a basic version of their tools for free. For example, Qlikview offers personal edition as a free download, but you need to buy licenses if you want to share you dashboards. Big query (from Google) offers querying till a particular data size for free.
How does this matter?
Well, you get access to any tool you want as a starter. You can test out tools before purchasing / deploying them. Also, you can accelerate your learning through downloading and experimenting through these tools.
 
Rule 3: Deep learning in at least one subject will propel your career 
Especially so, in early part of your career. In order to distinguish yourself, you will need specialization in at least one area. If you are a Business Intelligence professional, you need to understand the entire spectrum of tools available, their pros and cons. Same with Big data experts and data scientists. The pace of change doesn’t allow you to specialize in all these subjects.
You are free to pick what you prefer, as each of the specialization offer good career aspects.
 
Rule 4: However, for leadership positions, you will need broader perspective
Leaders will be expected to know what is happening across spectrum and how can that benefit the organizations. So, at some point, when you have deep knowledge in one of the subject areas, you need to pay equal attention to other areas (at a higher level though).
Or as Srikanth Velamakkani, CEO, Fractal Analytics puts it:

 
Rule 5: Outstanding visualizations and storytelling will differentiate best analysts from the rest
With data size increasing every second, you can no longer rely on bar-charts and pie-charts to tell your stories. New creative visualizations help deliver stories effectively and efficiently. Be it infographics, graph representations of networks or geo-spatial heat maps – all of these are far better to make an impact compared to a bar graph / table narrating the same.
 
Rule 6: Man machine co-ordination will gain in importance
Machine learning is gaining importance due to strong fundamental reasons. Be it Google driverless cars, or your smartphones trying to understand your needs or a sensor on your wrist to monitor your health regularly, all of these require man machine co-ordination. I think the career opportunities here can almost be divided in 2 categories:
Data collection from various sensors and machines
Analyzing stream of data to come up with insights and personalized experiences
 
Rule 7: Data Science competitions are opportunities to learn and showcase your talent
I love these competitions and hope that you do too. Sadly, I don’t get as much time for these as I would want. But these are ideal platforms to learn along side your peer. Look at the kind of discussions happening on various competitions on Kaggle and you will understand what I am talking about, You learn immensely through these competitions.
As a side benefit, they can become your hiring platforms.
 
Rule 8: Last but not the least, learn continuously
This goes without saying! The more you learn, the better it is. What is important is to learn regularly and continuously. Be it courses on Coursera, Youtube tutorials, blogs or iPython notebooks on github, just spend time to learn on a regular basis.
Each of these rules is something I believe in. Internalizing them will help you become clearer on your learning journey. What are your thoughts on these? Do you have any other rules / perspective to add? Kindly do so, through the comments below.
If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.
photo credit: giulia.forsythe via photopin cc
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Freelancing consultant - SAS, India's leading travel portal","Kunal Jain",2014-04-04 06:38:00,6,"
							
										
						We are looking at a consultant who would help in setting up & integrating SAS for us. This would be a 2 week role and would encompass the following -
1.       How to administer the SAS environment? (Concept, architecture, installation and basic management)
2.     back up the SAS environment and move metadata
3.     administer users and access to data
4.     understand how client applications interact with metadata
5.     administer SAS content
6.     monitor, log, and troubleshoot SAS servers.
7.     create and use internal accounts
8.       understanding of SAS authentication  mechanism and configuration
9.     secure access to RDBMS data
10.   secure access to information maps
11.   create and apply ACTs
12.   secure access to OLAP data
13.   secure reports
14.   secure users and groups, servers, and ACTs.
 
Interested candidates can send their CVs to jobs@analyticsvidhya.com
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Consultant / Sr. Associate Consultant, Indus Insights","Kunal Jain",2014-04-04 06:07:00,6,"
							
										
						Indus Insights offer tremendous career growth, exposure to cutting edge business thinking, and the experience of working in a high-energy environment.
About Indus Insights
Indus Insights is a specialized consulting firm, focused on using analytical methods to drive business performance. We are a premier consulting company in the analytics domain, catering to both international and domestic clients. We take pride in our relentless focus on creating actionable results, and this focus has helped us earn the trust of some of the leading companies in the world. Our clients include 5 out of top 10 ecommerce companies in India, one of the largest financial services company in US, Indian subsidiary of a large international bank, and one of the top 3 media agencies in the world.
Indus Insights is led by seasoned and energetic professionals with experience at top notch firms in the US, including Bain & Company, Capital One, HSBC, and Citibank. Our Advisory Board consists of global leaders in the financial services sector, including Mr. TS Anil – the worldwide head of Credit Cards and Personal Loans at Standard Chartered. More information about the company can be found at our website www.IndusInsights.comAbout the Roles
Indus Insights is looking to fill positions of Consultant / Sr. Associate Consultant. These positions will be based out of Gurgaon (Delhi NCR). The individuals will drive delivery on management consulting projects for the clients of Indus Insights. You will be using advanced analytics, data mining techniques, and management consulting concepts to develop actionable insights for our clients and to present these to client contacts. Clients span across different verticals including financial services, ecommerce, digital marketing etc. Associate consultants interact directly with clients to drive the projects forward. The individuals will also assist the leadership team in business development activities.
These roles offer tremendous career advancement opportunities, exposure to senior management, experience of working in a high-energy environment.
About the Candidate
You are our ideal candidate if the following describes you:
You are a star! You have excelled at whatever you have done – at academics, at your workplace, at your extra-curricular interests
You are great at problem solving. You enjoy taking a complex business problem, breaking it down into smaller, manageable questions, and then answering those systematically
You are passionate about creating results for the client. We have an intense focus on “wowing” our client in every single engagement, and you need to drive this
You are ready to “do what it takes”, and understand the needs of a client-facing role.
You are comfortable with changing priorities, somewhat unpredictable schedules, and traveling to clients’ sites when needed
You enjoy discussions and debates, and love to speak your mind, go to the white board, and listen & understand others’ arguments
You are self-driven and take pride in your work
You are fanatical about meeting timelines. You do not believe in the concept of Indian Stretchable Time
You take your work very seriously, but don’t take yourself too seriously
Remuneration
We hire star performers only, and our compensation is also in-line with the caliber of people joining us. We offer Rs. 13-20L depending on the candidate’s prior work experience.
 
Interested candidates can respond with their CVs to wannabe@indusinsights.com
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Excitement going up at Analytics Vidhya (and I can't stop smiling)!","Kunal Jain",2014-04-04 05:57:00,6,"
							
										
						Guess what…we will celebrate our first year anniversary shortly. Last year has just flown by! I still remember the excitement and the anxiety I had while writing my first article on Analytics Vidhya. I had no clue, whether people will read what I’ll write, whether they will like and appreciate it. I didn’t know how things will pan out (I still don’t). What I did know was that I wanted to write and build a community of analysts, where they could learn from each other.
And the year has been nothing short of amazing! Our readers love what we write, they come back to read our new articles time and again. We relish writing articles, doing our research before writing them, listening back from our audience through comments, mails and one on one interactions. We love sleeping with our mind full of new and exciting articles / case studies / tips.
In our efforts to make Analytics Vidhya more meaningful for our audience, we are taking new (baby) steps. We are very excited about these new initiatives and can’t hide is any more. Following are the new initiatives which have doubled tripled up excitement at Analytics Vidhya:
Starting analytics training – Professional training is heart of analytics learning. And the supply for good quality training is limited. So, we couldn’t stop from getting our hands dirty! We will keep a high bar on quality of trainings (similar to all our content) and hope that through these trainings, we will have lot more active role to play in analytics learning of our audience.
The first course which we are launching is a Summer training aimed towards beginners. This course aims to provide deep understanding of basics of analytics. By end of this course, you will be able to build a simple predictive model (in excel). The idea behind this course is not to make you technically proficient in a tool (like most of the trainings out there), but to really drive home the basic fundamentals of statistics and analytics.
So, if you are a fresher looking to enter this industry, a college student passing out in an year or MIS professional looking for a shift to analytics, this could be the course you might be looking for. In order to deliver this training, we have partnered with Internshala. They have tremendous reach among college students and combined we will reach out to the right audience for the course.
If you are interested in this course, we are running an early bird discount offer. The earlier you sign up, the better it will be! You can find more details here.
Creating a job alerts platform – Next important aspect for an analyst is to be aware of available career opportunities. In order to help our readers on this, we have launched a job alerts platform. On this platform, you can see some of the best jobs we come across. It is a very basic alerts page to start with. Over time, this will become a single place for looking and searching for analytics jobs (globally). I’ll keep you posted how this develops further.
We have kept job alerts out of the RSS feed / email subscription for now. In next dew days, we will provide a way to sign up for these individually.
In order to keep up with the pace of changes we are driving and make sure we deliver absolute best to our audience, I have decided to devote all my working hours to Analytics Vidhya. Yes, that’s right, over next few weeks, I’ll hand over my responsibilities at my full time employment and become self-employed.
We are absolutely in love with what we are creating for our audience. We hope that each of these initiatives will make Analytics Vidhya more meaningful for you. Do let us know what you think about these new initiatives. What would you want out of them? Any suggestions / feedback / questions? Please feel free to comment / reach out to us, we want to hear more from you.
If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"SAS vs. R (vs. Python) - which tool should I learn?","Kunal Jain",2014-03-28 00:53:00,5,"
							
										
						We love comparisons!
From Samsung vs. Apple vs. HTC in smartphones; iOS vs. Android vs. Windows in mobile OS to comparing candidates for upcoming elections or selecting captain for the world cup team, comparisons and discussions enrich us in our life. If you love discussions, all you need to do is pop up a relevant question in middle of a passionate community and then watch it explode! The beauty of the process is that everyone in the room walks away as a more knowledgeable person.
I am sparking something similar here. SAS vs. R has probably been the biggest debate analytics industry might have witnessed. Python is another worthy candidate to put in the mix now. The reason for me to start this discussion is not to watch it explode (that would be fun as well though). I know that we all will benefit from the discussion.
This has also been one of the most commonly asked question to me on this blog. So, I thought I’ll discuss it with all my readers and visitors!

Hasn’t a lot already been said on this topic?
Probably yes! But I still feel the need for discussion for following reasons:
The industry is very dynamic. Any comparison which was done 2 years back might not be relevant any more.
Traditionally Python has been left out of the comparison. I think it is a worthy consideration now.
While I’ll discuss global trends about the languages, I’ll add specific information with regards to Indian analytics industry (which is at a different level of evolution)
So, without any further delay, let the combat begin!
Background:
Here is a brief description about the 3 ecosystems:
SAS: SAS has been the undisputed market leader in commercial analytics space. The software offers huge array of statistical functions, has good GUI (Enterprise Guide & Miner) for people to learn quickly and provides awesome technical support. However, it ends up being the most expensive option and is not always enriched with latest statistical functions.
R: R is the Open source counterpart of SAS, which has traditionally been used in academics and research. Because of its open source nature, latest techniques get released quickly. There is a lot of documentation available over the internet and it is a very cost-effective option.
Python: With origination as an open source scripting language, Python usage has grown over time. Today, it sports libraries (numpy, scipy and matplotlib) and functions for almost any statistical operation / model building you may want to do. Since introduction of pandas, it has become very strong in operations on structured data.
Attributes for comparison:
I’ll compare these languages on following attributes:
Availability / Cost
Ease of learning
Data handling capabilities
Graphical capabilities
Advancements in tool
Job scenario
Customer service support and Community
I am comparing these from point of view of an analyst. So, if you are looking for purchasing a tool for your company, you may not get complete answer here. The information below will still be useful. For each attribute I give a score to each of these 3 languages (1 – Low; 5 – High).
The weightage for these parameters will vary depending on what point of career you are in and your ambitions.
1. Availability / Cost:
SAS is a commercial software. It is expensive and still beyond reach for most of the professionals (in individual capacity). However, it holds the highest market share in Private Organizations. So, until and unless you are in an Organization which has invested in SAS, it might be difficult to access one.
R & Python, on the other hand are free and can be downloaded by any one. Here are my scores on this parameter:
SAS – 2
R – 5
Python – 5
2. Ease of learning:
SAS is easy to learn and provides easy option (PROC SQL) for people who already know SQL. Even otherwise, it has a good stable GUI interface in its repository. In terms of resources, there are tutorials available on websites of various university and SAS has a comprehensive documentation. There are certifications from SAS training institutes, but they again come at a cost.
R has the steepest learning curve among the 3 languages listed here. It requires you to learn and understand coding. R is a low level programming language and hence simple procedures can take longer codes.
Python is known for its simplicity in programming world. This remains true for data analysis as well. While there are no widespread GUI interfaces as of now, I am hoping Python notebooks will become more and more mainstream. They provide awesome features for documentation and sharing.
SAS – 4.5
R – 2.5
Python – 3.5
3. Data handling capabilities:
This used to be an advantage for SAS till some time back. R computes every thing in memory (RAM) and hence the computations were limited by the amount of RAM on 32 bit machines. This is no longer the case. All three languages have good data handling capabilities and options for parallel computations. This I feel is no longer a big differentiation. Also, I might not be aware of the latest innovation in each ecosystem and hence I see all 3 as equally capable.
SAS – 4
R – 4
Python – 4
4. Graphical capabilities:
SAS has decent functional graphical capabilities. However, it is just functional. Any customization on plots are difficult and requires you to understand intricacies of SAS Graph package.
R has the most advanced graphical capabilities among the three. There are numerous packages which provide you advanced graphical capabilities.
Python capabilities will lie somewhere in between, with options to use native libraries (matplotlib) or derived libraries (allowing calling R functions).
SAS – 3
R – 4.5
Python – 4
5. Advancements in tool:
All 3 ecosystems have all the basic and most needed functions available. This feature only matters if you are working on latest technologies and algorithms.
Due to their open nature, R & Python get latest features quickly (R more so compared to Python). SAS, on the other hand updates its capabilities in new version roll-outs. Since R has been used widely in academics in past, development of new techniques is fast.
Having said this, SAS releases updates in controlled environment, hence they are well tested. R & Python on the other hand, have open contribution and there are chances of errors in latest developments.
SAS – 4
R – 4.5
Python – 4
6. Job scenario:
Globally, SAS is still the market leader in available corporate jobs. Most of the big organizations still work on SAS. R / Python, on the other hand are better options for start-ups and companies looking for cost efficiency. Also, number of jobs on R / Python have been reported to increase over last few years. Here is a trend widely published on internet, which shows the trend for R and SAS jobs. Python jobs for data analysis will have similar trend as R jobs:
Source: r4stats.com
In India, specifically, the gap in SAS vs. R is bigger. My estimate is that SAS would have about 70% of market share, R around 15% and Python less than 5%. However, the trends are similar to global trends.
SAS – 4.5
R – 3.5
Python – 2.5
7. Customer service support & community:
R has the biggest online community but no customer service support. So if have trouble, you are on your own. You will get a lot of help though. Similar for python, though at a lower scale.
SAS on the other hand has dedicated customer service along with the community. So, if you have problems in installation or any other technical challenges, you can reach out to them.
SAS – 4
R – 3.5
Python – 3
Other factors:
Following are some more points worthy to note:
Python is used widely in web development. So if you are in an online business, using Python for web development and analytics can provide synergies
SAS used to have a big advantage of deploying end to end infrastructure (Visual Analytics, Data warehouse, Data quality, reporting and analytics), which has been mitigated by integration / support of R on platforms like SAP HANA and Tableau. It is still, far away from seamless integration like SAS, but the journey has started.
Conclusion:
Clearly, there is no winner in this race yet. It will be pre-mature to place bets on what will prevail, given the dynamic nature of industry. Depending on your circumstances (career stage, financials etc.) you can add your own weights and come up with what might be suitable for you. Here are a few specific scenarios:
If you are a fresher entering in analytics industry (specifically so in India), I would recommend to learn SAS as your first language. It is easy to learn and holds highest job market share.
If you are some one who has already spent time in industry, you should try and diversify your expertise be learning a new tool.
For experts and pros in industry, people should know at least 2 of these. That would add a lot of flexibility for future and open up new opportunities.
If you are in a start-up / freelancing, R / Python is more useful
Here is the final scorecard:

These are my views on this comparison. Now, its your turn to share your views through the comments below.
If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Sr Business Analyst - Analytics - BFSI - Startup (3-5 yrs)","Kunal Jain",2014-03-25 19:37:00,3,"
							
										
						Looking for an exciting role in Insurance Analytics? Valiance Solutions is looking for Senior Business Analyst with 3 – 5 years of experience in Insurance domain for Decision sciences team of Valiance Solutions Pvt. Ltd., an Analytics & Consulting Start-up Company based out of Noida, India
 
Designation - Sr. Business Analyst, BFSI
Location - Noida, transferable to Mumbai for onsite work.
About employer - Analytics consultancy start-up. Founders profile.
Responsibility -
Developing statistical, Financial and judgment based models to enhance the business decision making.
Handle analytical engagements from start with clear framing of business problem in consultation with all stakeholders, choosing the right analytical framework, handling delivery and demonstrate business value.
Lead & manage team of business analysts with onsite and offshore mix.
Drawing managerial insights through Statistical Techniques and Predictive Modelling
Managing client satisfaction, feedback process, managing/tracking workflow
Drawing actionable recommendations from data for senior management.
Contribute to the overall growth of firm through participation in solution framework development for business problems with senior management.
 
Experience -
Insurance domain experience across below initiatives:
Customer Retention
Customer Acquisition
Cross Sell/Up Sell modeling
 
Skills required:
Graduate or Master’s degree from reputed University with concentration on quantitative field such as statistics or economics
3 – 5 years of experience in Advanced Analytics within consulting environment.
Ability to comprehend intricate and diverse range of business problems and analyze them with limited or complex data and provide a feasible solution framework.
Consulting Skills: Ability to impact business decisions through analytics and research
Knowledge of survival analysis for improving customer retention will be handy.
Should be follower of latest trends in analytics in his domain.
Technical Skillset - R, SAS, SPSSS, SQL, Advanced Excel
 
Interested candidates can contact:
Nidhi Dagar
Email: nidhi.dagar@Valiancesolutions.com
Contact: +91 120 4119409
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Head - Data Infrastructure, Banking (10-16 yrs)","Kunal Jain",2014-03-21 13:33:00,6,"
							
										
						Looking for a role within BFSI domain? If you have got 10+ years of experience in Data Analytics and are ready to lead a large team (~40 people) with people managers reporting to you, this role might be for you.
 
Designation - Head, Data Infrastructure
Location - Mumbai, India
About employer - Banking giant.
Responsibility -
Lead the 40 member Data Infrastructure team including 10 in-house and 30 outsourced resources
Build the data foundation needed for the rest of the team to perform their analytics and reporting functions
Acquire data from new sources, more granular data from existing sources and organize the same in to well optimized datamarts.
Interact with other members of the BIU as well as with internal customers across the bank to understand their needs and proactively address them.
Strategize and direct the data and hardware architecture changes needed to march the bank’s BIU in to the big data world.
 
Ideal profile -
9+ years of relevant experience in the technology for analytics, business intelligence, information management, data warehousing or datamarts.
Well-versed in analytic tools and technologies like SAS (or related software), SQL, SAP BO, rule engines, informatica ETL etc
Solid understanding of datawarehousing concepts, how to structure datamarts and optimize them for querying and / or analytic purposes
Track record of increasing responsibility and success in leadership roles
Excellent communication skills. Ability to interact with non-technical stakeholders and solve their data related problems
Great leadership skills. Very proactive. Interacts with other business leaders to both learn as well as influence. Partnership approach and collaborative attitude
Tremendous drive towards results. Ability to persist against obstacles and get things done
Displays great ownership of problems and results. Can do and positive attitude
Bachelor’s degree in a strong technical discipline
 
Skills required:
Building Data Infrastructure, Data Quality & Governance
Proficient in Enterprise Data Infrastructure, DataWarehouse and ETL
Informatica & SAS (mandatory)
You can find more details about the job and apply here: http://goo.gl/5qTWVp
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Head – Machine Learning, Hiring Domain start-up (8-14 yrs)","Kunal Jain",2014-03-20 14:22:00,5,"
							
										
						If you get excited by working on cutting edge analytics, have passion to build analytics infrastructure from scratch and are not worried by working in a start-up environment, this is the opportunity for you.Designation - VP, Engineering
Location - Mumbai
About employer - Its a funded startup ,looking to disrupt the recruitment industry in India. Confidential.
Responsibility - Build and deliver a recommendation engine for delivering personalization for clients of this funded start-up. Aim is to send only qualified relevant CVs to clients.
Recommended Work Experience - 8 – 14 years. PhD or MS Computer Science, in Algorithms, Data Analytics, Machine Learning or related fields
Skills required:
Candidate has spent the last few years working with data infrastructure, machine learning, and back-end architecture, preferably applied in Hiring domain
Strong Product Architecture & Development experience.
Theoretical knowledge of Mathematical Modeling/Discrete Mathematics.
Expertise on Information Extraction and Big Data frameworks like Hadoop.
You can find more details about the job and apply here: http://goo.gl/YXNpce
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Part III - Interview with Mr. Srikanth Velamakanni, CEO, Fractal Analytics","Kunal Jain",2014-03-18 15:02:00,3,"
							
										
						This is third and concluding part of the interview with Srikanth Velamakanni, Co-founder & CEO, Fractal Analytics.
In the first part, Srikanth shared how Fractal came into being, and how they overcame the initial challenges they faced. In the second part, we discussed how Fractal manages the career cycle of analysts at Fractal (hiring, training, engagement and attrition), and the challenges that occupied Srikanth’s mind.
In this part, we will discuss the key strategic bets Fractal foresees in the next few years, and how Srikanth sees the industry evolving. He has also given some career advice for people in this industry.
 
KJ: What are the strategic bets for Fractal in next few years in terms of tools, people and infrastructure?
SV: Following are the key bets for us:
The biggest bet from a vertical perspective, is entering new sectors. We are currently focussed on consumer packaged goods and financial services. We are now foraying into analytics for technology and life-sciences as well.
In terms of tools, we have placed some big bets on creating an internal environment where people can work in an error free environment. These tools are based on visual workflow and hence ensure quality by making sure errors get flagged in the process flow.
Another big thing for us is the entire area around human-machine co-operation. This is an exciting space. For example, take data harmonization problem => Data comes from different sources in different formats and with different context. Machines can’t match these sources accurately and human led interventions are very time consuming. The solution is to create algorithms where machines take care of maximum matching and humans intervene only to deliver on last mile challenges.
Another question we are constantly thinking about is “How to simplify front end for a complex back-end?” Our customers are business users and need not know all the complex algorithms running at the back end. For example Google has a simple interface to search which is enabled by very complex algorithms at the back end. We aim to achieve a similar outcome for our products / services.
 
KJ: What about Big-data?
SV: We are already processing things on distributed systems using some of the latest platforms like Hadoop and MongoDB. It is still a small team and we are handling data in magnitudes of terabytes. We are not processing petabytes of data yet. This is something we are building. Goal to have a system which can handle at least 1 Petabyte of data by end of this year.
 
KJ: If you were a fresher starting in analytics industry today, how would you shape up your career?
SV: My advice would be to take up a career track and be clear in how you would be successful in it. There is a lucrative future in all the four specializations we talked about. And like any other science, this industry will also move from generalists to those having a super-specialization.

So analysts should ensure that their career moves in a ‘T’ shape. They should have deep knowledge in at least one domain and have a broad perspective about the overall Analytics industry at the same time. At younger stage, people should be willing to move industries and learn quickly.
If you want to be a data scientist, you have to be at the fore-front of Machine learning / AI
People wanting to become Analytics Consultants should gain business knowledge of their domain. They can’t succeed until they understand the domain completely.
BI experts should understand how all the tools work. While the tools may change over time, the knowledge about them will eventually make you a better professional.
This industry is set to boom for the next 40 – 50 years. It is something like joining the IT industry back in the early 80s. This is a dynamic space with lots of opportunities. Any big company in this space can go out of business very soon because of the fast pace of the industry. But as long as you have the right talent, you have a bright future.
 
KJ: Any advice for Analytics Vidhya’s Audience?
SV: For people who are trying to switch to Analytics as a career, my strong recommendation would be to develop an understanding of probability and see if they are enjoying it. They have to take pleasure in understanding probability, to be an analytics professional. My suggestion to them would be to look up AI and Machine learning courses on Coursera. They are really good courses. Then they should see if this is something which excites them.
For professionals already in the industry, I would suggest that they learn continuously to build their career in a T-shape, like I just described. The other thing they should think about is, ‘which is the right Organization for them to join?’ And where can they build the required skills? This choice should not be based on their location. They should look out for the right company, the right culture, where they can spend 10 – 15 years of their career. I have seen youngsters tend to be a bit restless and they switch many jobs for pay and designation. I think pay is not as important and if you have the right skills and are in right Organization. The pay will eventually catch up.
 
KJ: Thanks a ton Srikanth for the quality time you spent in discussing these questions and providing your perspective. I think it will go a long way to help audience of Analytics Vidhya.
For those who have missed the first 2 parts of this series, you can read them here:
Part 1 describing how Fractal started, initial challenges and how they were overcome?
Part 2 describing how Fractal manages hiring, training, engagement and attrition of analysts.
 
I think all the information / perspective provided by Srikanth is invaluable. There is tons of useful advice in these 3 pieces of interview. Over next few days, I’ll continue to reflect on what I have learnt by talking to Srikanth.
If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Interview with Mr. Srikanth Velamakanni, CEO, Fractal Analytics - Part II","Kunal Jain",2014-03-12 14:59:00,4,"
							
										
						Last week, we released the first part of this interview. In that part, Srikanth had shared his experience with starting up Fractal, the challenges faced by the team and how they overcame the challenges. If you missed that part, you can read it here.

If you are someone like me (who wants to learn more about Analytics) and have read the first part, this part of the interview is for you. I gained immensely from this interview and I am excited about extending this experience with all of you.
In this part of interview, Srikanth shares their hiring, training, and managing strategy at Fractal; and how Fractal, expects their people to choose their own career, and manages employee attrition.
 
KJ: Fractal has produced some of the best Analytics professionals in Indian Analytics Industry. How do you hire people? What do you look at time of hiring? How do you train them once they are hired? How do you ensure that they contribute what they bring to the table?
SV: For Hiring:
These are the attributes we lookout for:
Most important thing to look for is Passion for Analytics. (Remember, this was first thing out of the 3 things to look out for in life). Frankly, when you get into depth of analytics, things get difficult. You need to get into details while slicing and dicing data. If you don’t have passion for analytics, you may not succeed when you get there.
Next, we look for strong understanding of probability, mathematics and statistics.
We also look for structured thinking like any consulting company will look for. Can the candidate take an amorphous business question and then structure it to come out with various hypothesis and a business solution?
Apart from these skills, we also look at values. People need to work in very collaborative manner with teams across the globe. So, we look for integrity and commitment to work .
We have created an environment in Fractal where we put extreme trust in our people. We don’t monitor their inputs. We are always on the look out for people who fulfill this extreme trust and not violate it. We look for people who can sustain a very high trust culture. We believe that not having to micro-manage people creates an extremely happy and creative workplace.
In fact, while recruiting people in the middle-management, we ensure we do not hire micro-managers.
 
Training:
We have a pretty robust on-boarding programme for 5 weeks. It includes 3 weeks of class-room training, followed by a 2 week program which involves, working on a business problem in teams. All the concepts (like structured thinking, presentation skills etc.) and tools learnt over in first 3 weeks are put to use in these 2 weeks. In last week of this induction programme, fresh recruits get introduced to the sector they will be working for,  where they get specific knowledge of the domain.
After this 5 week programme, new recruits get to shadow one senior, or rather, more experienced person on a project. So, they are part of a team, but we do not charge our clients for this period. This helps new recruits learn while they are working with other people. This doesn’t always happen, if the project is fast paced, given the fact that hiring in this industry is very difficult. However, we always ensure that a person has the right skills and exposure to the domain before he ventures out on his own. The moment they are out of an induction programme, they get to interact with CXOs in the industry, so we prepare them for this as well, in our induction programme.
 
KJ: Analytics industry is crippled with high attrition. How do you manage this, especially when you are investing so much in the training?
We have to invest in people, without expecting anything in return. If people leave, they leave. However, we have experienced that when you trust people, they respond. And this thought process always works, at least it has worked for us. In 2012 (Calendar year) the attrition at Fractal Analytics was 14%, in 2013 it rose a bit, but we got the figure down again in 2014.
The reason for this is that we have created an environment where we trust our people. There is no monitoring, people can come in any time and they can go any time. Our employees have that freedom. In fact simple expense re-imbursement policy, dress policy, no private parking spaces, no cabins, are some of the policies which help us create this culture.
We have created a system where analysts can move out of their project on their own. If an analyst is not interested in a particular project, we help him find another role within Fractal. There is complete flexibility to choose your own career path.
There are 4 career tracks we offer at Fractal:
Analytics Consulting
Data Scientist
Programme Manager
Big data engineer

We believe that “You are the CEO of your career, and we respect the choices you will make.” We don’t micromanage our employees’ decisions. Through these policies, we endeavour to take all the noise surrounding an analyst, away, so that he can focus on the work at hand.
We want our people to focus on creating the best analytical solution for our clients, and in turn make Fractal a great place to work.  It is our endeavour, to make Fractal the most respected player in this industry.
 
KJ: What are the challenges occupying your mind currently?
SV: One big challenge for us has been getting good people in offices outside India. Through our policies, we have got access to a very good talent pool in India, but this is not fully true for the US market. We have a relatively small office in the US. This is something we plan to change over time.
The other challenge we face, is that a lot changing in this industry rapidly. Half-life of knowledge is about 3 years. Hence, we need to be aware of the latest tools and techniques. In order to address this, we have created Fractal Academy where people can get trained in various aspects of analytics. We have integrated platforms like eDX and Coursera and people get credit in Fractal Academy for completing relevant courses on these platforms.
We have also made changes to our career tracks in line with changes in industry. Till some time back, we used to hire everyone as analytics consultant. The person was supposed to do everything from meeting & consulting the client to building the models and managing the programme. In order to keep up with fast pace of the industry, we now have 4 different career tracks for people:
Analytics consultants – Traditional client facing consultants who meet client, capture their requirement, design solutions and help implement the solutions.
Data scientists – These people are not facing clients, They work on algorithms, software codes and machine learning (e.g. Recommender systems).
Programme managers – BI programme managers create easy to use and visually appealing programmes so that information can be retrieved on click of a button.
Big data engineers – People working on big data technologies and platforms e.g. Hadoop
Now we are hiring people in these different roles.
In the concluding part of this interview, we will share strategic bets for Fractal over next few years, Srikanth’s view about how Analytics industry will shape up and his advice to audience of Analytics Vidhya. Stay tuned with us and we will be out with the final part of the interview shortly.
Read Part 1 of interview here
Read Part 3 of interview here
If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Interview with Mr. Srikanth Velamakanni, CEO, Fractal Analytics","Kunal Jain",2014-03-05 13:37:00,4,"
							
										
						The best part of running this blog has been, connecting with some of the best people in the Analytics Industry. I recently, got an opportunity to interview Srikanth Velamakanni, Co-founder and CEO of Fractal Analytics. The interaction was, undoubtedly, a great learning experience; something which I have tried to extend to all our readers, through this post.

Srikanth holds an MBA degree from Indian Institute of Management Ahmedabad and a bachelor’s degree in Electrical Engineering from Indian Institute of Technology, Delhi. Before co-founding Fractal in the year 2000, Srikanth was an Investment banker and had worked with ANZ and ICICI Bank, managing asset & mortgage based securities. Srikanth is passionate about learning and reading, especially about behavioral economics, positive psychology, and neuroscience. He loves listening to Audiobooks and is an analytics and technology evangelist.
In about an hour, which I spent talking to him, I developed deep respect for him as a professional and as a person. I hope that someday I would be able to make a similar impact in the Analytics Industry.
I plan to release the interview in three modules in the coming days. The structure for the same, would be as follows:
Part 1 – Starting Fractal, initial challenges and how they were overcome.
Part 2 – Building an Organization. Hiring, training, trusting your employees to build a world class Organization and dealing with attrition.
Part 3 – Future bets for Fractal and evolution of Industry. His advice to people entering the industry.
Here is the first part of the interview:
KJ: You started Fractal back in the year 2000. At that time, data science and analytics were not as popular as they are today. You were one of the earliest start-ups in the industry. So, what was the idea when you started? How did it happen? How did you decide that this is the idea you would want to pursue full time in the future?
SV: Sure. When we started, we obviously didn’t have analytics in our mind. We (founders of Fractal) had quit our jobs and wanted to do something together. We actually did a little bit of search and thought that we should be in the IT space. However, after talking to people (senior veterans) in the industry, we realized that until and unless you are focussed (on a particular aspect), it is not a good idea to be in the IT space.
We were doing soul searching and looked at various things. After a while we realized that we are good at Financial Services and we are good at Maths. We understand probabilities & statistics and had worked on problems like hand-writing recognition and face recognition during college.
While talking to a few people in ICICI, we learn’t that they were facing challenges in identifying the credit worthiness of customers. This got us very interested, as we knew, credit risk could be identified with a lot of statistical tools. So it turned out, that we were actually the first ones to build a statistical scorecard (for identifying credit risk), in India. This score card, as mentioned earlier, was for ICICI Bank (to predict default on their personal loans). We were pretty excited because this could enable them to do real time risk scoring.
When we finished this project, we realized that this was something exciting.
To think of it, what do you need in life? You need three things:
Pleasure – Whatever you are working on should be fun
Meaning – It should add meaning and value to the world.
Strength – It should be an area of strength
So it felt like this industry was meant for us.
What we did not know was how big this area was? Whether we could make money in this area or not? To be very honest, we were MBAs from IIM Ahmehdabad, but we did not do any major business planning in terms of how big this space was going to become.
The next thing we did, was to build a default prediction model for corporates using information regarding stock prices. We could simulate stock prices and see default ratios under various circumstances and could come out with pretty good models predicting when a company could default. In fact, we came out with models, which were better than Moody’s and S&P. Moody’s and S&P were both backward looking where as we were using predictive modelling to come out with default probability.
Another thing which was fascinating at that time, was the entire area of consumer behaviour. How do people change their behaviour? This was 2001 and it was recession time. We observed that consumer behaviour was changing dramatically in a few sectors. Specifically, when we started working with Hindustan Lever, their Chief Economist told us, “Here is all the panel data I have. Tell me what I am not looking at.” We looked at various aspects; we studied whether customers were changing products or brands they used to purchase before? And how were the purchasing patterns shifting?
Then we realized that this is it, this is the space we had to be in.
We did not have any role model. But we felt this was it was an exciting space to be in, and all the three elements (Pleasure, Meaning and Strength) looked to be in place, and we just kept going. We have never looked back since.
KJ: That almost brings me to the next question. This was a new area when you started. You were treading the path for the first time for yourself. When you start in that kind of scenario, there are challenges that you come across. The challenges could be, proving to customers that Analytics can or has added value. So, what type of challenges did you come across and how did you tackle them?
SV: Great question! Infact we did face a lot of significant challenges in our initial days. Right at the start while working with ICICI bank, we had to prove ourselves. We did not have any experience in building models. So we had to prove to them, that we know what we are talking about. Remember, this was about building a risk model. If you built a wrong risk model, it could cost the bank a fortune. It was not easy. ICICI bank asked us, “Why should we believe in you and not work with Fair Issac in the U.S.?” So we said that we will do some original research on the topic and come back to you with what we think about the space. We spent a couple of months writing a research paper on the subject. We took that to ICICI and told them that this is the state of the art research on the subject matter. They really liked what we had put together for them. This was the way we could earn their trust, so that they could share their data and we could build the first statistical scorecard.
Another instance was a conversation with Citibank. We met them in Mumbai and then in Chennai. They said that “We have massive team of Ph.Ds. sitting in New York, who know how to build all this stuff. You guys are 20 something and you have got no experience in the domain. So how can you beat what we are doing?”
So this time, we said “We understand that you might have a great team. We also believe in what we are doing. Why don’t we set up a challenge: You give us the data, we will not charge you anything. We will build the model and see if this adds value to what you are doing.” We have to give credit to them that they accepted the challenge and gave us all the data. We built a personal loan cross-sell model for Credit card customers and then also looked at the price sensitivity of the customers in the model. That was a new thing at that time. They had not done a lot of experimentation on various price points. So we had thin data to build the model.
Once we built this model, these models performed extremely well and all of us were surprised how accurate they turned out to be. At that time, they came back and said “You guys have clearly beaten what we were doing internally.” So that brought a lot of credibility to us. With clients like Citibank, ICICI bank and Hindustan Lever, we could now go and pitch confidently to other people about our services.
KJ: Interesting! This idea about pitching yourself against some of the best people in the industry sounds fascinating!
SV: In fact, we did a similar exercise with Capital One in the US. They challenged us whether we could predict when a customer would attrite (rather than just who would attrite)? We put together a survival analysis framework in terms of when would the customer attrite and what intervention could be made to retain a customer. This framework also provided answers like whether an intervention was effective or not and by how long would it increase the customer life-cycle. This framework was very fascinating and they loved what we had put together.
These are the kind of projects we have worked on. We have done a lot of stuff. We did not focus on making a lot of money. We focussed on doing a lot of cool stuff and focussed on building our credibility and expertise.
In next part of the interview, we will discuss Fractal’s hiring and training policy and how do they tackle employee attrition, which is one of the biggest challenges in the Indian Analytics industry today. For me personally, the key take-aways were the three elements which Srikanth focused on and some of the non-traditional pitches they made in the beginning of their journey. Stay tuned with us for more of this exciting and insightful conversation with Srikanth, which will be released in the next 2 parts.
Read Part 2 of interview here
Read Part 3 of interview here
If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Tools for improving structured thinking (for analysts)","Kunal Jain",2014-02-27 04:15:00,5,"
							
										
						There are 4 ingredients required to make a good an awesome business analyst:
Passion for Business Analytics
Structured thinking
Love for statistics and numbers
Business domain  knowledge

We recently shared a few tips to train your mind on analytical thinking, the tactics and practices mentioned there improve your number crunching abilities and help you apply analytical thinking in day to day activities. Today, I am going to share a few tools and exercises I use for improving structured thinking.
 
   What is structured thinking?
Structured thinking is a process of putting a framework to an unstructured problem. Having a structure not only helps an analyst understand the problem at a macro level, it also helps by identifying areas which require deeper understanding.
I had written an article on how structured thinking can help an analyst by reducing iterations and turn around time on projects. Today I’ll share some tools and practices which have helped me immensely in improving my structured thinking.
 
   Tools / Exercises for improving structured thinking:
Following are some of the best practices I have used and applied for better structured thinking. I am sharing simple tools rather than the most sophisticated mind map software out there. Idea is to get you started on this habit of structured thinking. Once you are used to the process, you can explore advanced tools and software:
 
   1. Layout a framework on paper before looking / creating your datasets: 
If there is only one takeaway you want to carry from this article, it should be this. As a practice, you should spend 1 – 2 hours of distraction free time (no emails, messages and phone calls!) just laying out the structure, possible hypothesis, story flow and a tentative business presentation. Here are a few pics of how I typically layout structure and story flow on paper (Business problem statement: Your Organization has observed increased credit risk in last few months. You need to analyze why this has happened and recommend next steps):
Putting framework on paper
Here is how tentative story looks at this stage:
Tentative presentation flow
 
   2. Use whiteboards for brainstorming (again before touching the data):
Typically, I do a session / meeting with stakeholders laying out all possible hypothesis and action items after I have spent time laying out things on paper. Call all stakeholders, layout discussion for them, make sure they are engaged (and not busy on smartphones), note thoughts on whiteboard and capture all the thoughts in the framework we created on paper (before this brainstorming).
By end of this brainstorming, you should have a clear framework agreed and discussed with stakeholders.
 
   3. Practice back of the envelope calculations (anywhere and everywhere):
This is the tool I use the most. Any business you come across (e-Commerce websites to the laundry service you might be using), try and chalk out their business model in your mind. Some of the questions you can use to get yourself started are:
What would be the revenues this business might be making?
How many customers are they touching? Repeat customers? Loyalty scheme?
What would be the cost structure?
How has the business scaled up / down in last few months / years?
Perform all these calculations at back of your mind (and not on paper). Layout a structure in your mind first, then answer these questions and come up with answers. A side benefit of doing this is that you improve number crunching abilities as well.
 
   4. Usage of a note / document to conduct meetings / discussions:
This is one of the tricks one of my mentors used to enforce on his team. As a rule, he did not look at any powerpoint presentation. For doing any discussion with him, you were enforced to create a note explaining the problem and the solution, send it to him before hand and then he would get involved in discussion. I think (am not completely sure), a similar practice is followed in Amazon for all meetings with Jeff Bezos.
If you have not tried this before, give it a shot. You will realize the amount of clarity which comes by preparing a simple note!
 
These tools and tricks have served me well over years. Do let me know in case you have any trick / tools, which have helped you become more structured in your thinking.
If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Maintaining fearless monk-like attitude while leading Analytics teams","Kunal Jain",2014-02-17 04:28:00,2,"
							
										
						One of my mentors had the following thought written in one of the presentation he was making:
“Your attitude, not your aptitude, will determine your altitude”
-Zig Zagler

Compared to some other industries, Analytics still has higher dependence on aptitude. As per my experience and observation, following is more appropriate for people in Analytics industry:
Your aptitude decides whether you can survive in Analytics industry and your attitude defines the altitude you achieve in life
In mathematical terms, if can be defined as something like this:

Altitude = b(Aptitude) x fn(attitude)

where b is a boolean function which becomes 1 over a certain threshold of aptitude. Once you have the right aptitude, your attitude defines how far do you reach in the journey. In more layman terms, your aptitude promotes you from an Individual contributor to a bigger role, but it is your attitude, which decides whether you can become a leader in this industry.
 
   What is the right attitude to carry while leading Analytics teams?
Before I proceed further, let me call out that this is my take on what is the ideal attitude to succeed while leading Analytics teams. If you have a difference of opinion, please feel free to start a discussion through comments below.
According to me, you need to maintain a fearless monk like attitude in order to do a good job while leading Analytics teams. Following are the reasons why I say so:
A monk aims to and is always curious to find out truth about a subject. This is the drive you need to find out those deeply burried insights.
A monk does not give up his search for truth in face of love, fear or any adverse situation. Similarly, an analyst needs to stay objective about his analysis without thinking about the impact of outcomes.
A monk is deeply passionate about learning and enhancing knowledge. You need to be on top of latest techniques and technologies in order to succeed as a leader.
 
   Characteristics of a fearless monk:
So, what are the characteristics of a fearless monk? I am sharing them with a hope that they will help bridge some gap between the demand and the supply of good Analytics leaders.
Here are the characteristics as I see them:
1. Stay calm, composed and objective with an uncluttered mind. As analysts, we often work in high pressure environment with tight deadlines. However, this should not impact the quality of the analysis being produced. You can not make an error, especially so when timelines are short! Maintaining calm, composed and uncluttered mind is the only way you can come out of this situation.
 
2. Keep it simple – Focus on what is important and work hard. As an analyst, you are expected to have passion for complicated problems and high attention to details. This makes us an easy prey in a situation where you can make things complex. More often than not, complex situations get solved by simple solutions. Once you know the problem, identify the 20% of area which will give 80% of benefit and work hard to make sure that gets delivered flawlessly.
 
3. Know your strength and weakness and stay confident. Everyone has their strengths and weakness. Knowing them, making sure that you focus on your strengths to deliver the results while being aware of your weakness can produce astonishing results. While writing it down is easy, only a person with deep understanding about himself can do this.
 
4. Enjoy the process and never shy away from expressing yourself. Until and unless you enjoy the process, you can not lead people. Most of the good leaders I know will have glitter in their eyes on mention of getting an opportunity to slice and dice data themselves. Whether it is data cleaning, collection or iterating your model for nth time, you have to love each part of the process. Also, during the process, you have to add the value you can. So if you can think of a solution, but not sure whether it will work out – at least try it out. Because, until it is tried out, you will never know whether it could have worked.
 
5. Lead by example. Your team gets the best form of motivation when this happens. Whether it is a difficult stakeholder to meet, or you want the team to spend time on learning. All of these are best demonstrated by leading the way. Once you have set an example, it is always easy for the team to follow.
 
6. Knowing your team members and backing them up. If you have ever observed a group of monks working together, you will know what I mean. There is infinite trust in the capability of team members. And this trust does not falter in face of difficult times, it actually makes things seamless and easy. You have to back up your team members when they are down after a bad meeting or after an undesired result.
 
7. Humility and levelheadedness – Last but not the least, you have to maintain humility as a leader. This is an aspect which defines you as a human and it develops on its own when you start caring about people. One of the best illustration of humility I have seen in recent past is last speech of Sachin Tendulkar before hanging up his boots from international cricket.
 
   A role model:
I am a big fan of Mahendra Singh Dhoni. For those of you who follow cricket (even by a distance), you would know whom I am talking about. For those of you who don’t follow cricket, here is his brief profile. If you have seen Dhoni play, he does not have the best playing technique (aptitude), but his success as a leader is down to his attitude. He is a perfect example of these characteristics personified.
 
What do you think about maintaining a fearless monk-like attitude? Any thoughts on the characteristics and how to foster them? Any practices to share? Please feel free to share them through comments below.
If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our facebook page.
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Progress on the resolutions for 2014 - Analytical trainings and books","Kunal Jain",2014-02-04 01:06:00,3,"
							
										
						One of my mentors used to say:
“Any project / initiative that does not gets tracked, does not happen.”
While this might sound like a strong opinion, it turns out to be true in most cases. So, new year resolutions without a tracking mechanism in place will tend to end up as is!
I plan to regularly update you all on how I am doing on my resolutions for 2014. Hopefully, this will keep me motivated in achieving them and you get my learnings on the way from these posts.
For those who might have missed or have subscribed recently, I took following 4 resolutions at start of this year:
More frequent usage of R and Python in my analytical endeavors
Continue contributing actively in creating a community of analysts (in India)
Re-start reading
5 kg weight loss

You can read more about these resolutions and how do I plan to track them here.
Coming to the important question, how has been the progress on these resolutions? Did I forget them after 1st week of January? 20th of January? or am I still on track to achieve all of these resolutions by end of year?
Well, the true answer lies somewhere in between. While I have not forgot the resolutions, I am still catching up with some of them. Here are the details:
1. More frequent usage of R and Python in my analytical endeavors
Between the two languages, I chose to focus on Python in first half of the year mainly because of 2 reasons:
Higher degree of acquaintance between the two languages
I want to compete on a few competitions on Kaggle this year, which has bigger Python community
So, I now have fully functional Python set up on my laptop. I have used it for doing small pieces of analysis. Additionally, I have registered for a Kaggle competition to construct an optimal portfolio of loans, which I plan to submit on Python.
 
Following are a few training and book recommendations which I used in last month:
Python for data analysis – A good book for people starting to use Python for data analysis. It builds up from some good examples and test cases in initial chapter.
Introduction to Python for Econometrics - A book by Kevin Sheppard (available freely) which has been updated recently
Statistical data analysis using Scipy – A series of Youtube video tutorial explaining usage of Scipy in statistical analysis.
2. Continue contributing actively in creating a community of analysts (in India)
January turned out to be best month till date for Analytics Vidhya, both in terms of inputs and outputs. We posted highest number of posts in a Calendar month (9) and saw a 35% growth in traffic over last month. The traffic has grown by 100% in last 4 months.
 
In addition to this, I have joined a couple of meetup groups in Delhi:
Big data meetup
AWS meetup group
 I am hoping to meet a few like-minded people in the coming events.
 
3. Re-start reading
I have managed to take time out for reading. Actually, I have been reading more than 2 hours a week! It feels great to get back to a past habit and it makes my week lot more fulfilling. Here are the books I read this month and my take on them:
Big data – A revolution that will transform How we live, work and think by Viktor Mayer & Kenneth Cukier – A book laying out basics about big data. Good if you are completely new to the topic. Don’t expect any technical details in this book. Good, if this is your first book on the matter.
The Lean startup by Eric Ries – Currently in middle of reading this. Loved the book till now. The principles laid out in the book apply to any business / start-up from day 0.
The everything store: Jeff Bezos and the age of AMAZON by Brad Stone. I respect Jeff Bezos as a leader and admire what he has created Amazon to be. This is the first book which tells so much detail about him and his management style.
 
  4. Weight loss – 5 kg 
This is the resolution which I have failed to keep up with. Although I have become more active during the day, I have not been able to put an exercise schedule. In addition to this, the sleep has been irregular and there have been days when I binged on junk food in middle of night! 
I am hoping that I will catch up on this resolution in coming couple of months and be in good shape by end of year. 
 
How has been your progress on your resolutions? Are you on track for keeping them? Or they are lying somewhere at the back of your mind? What will you bet upon – I will complete my resolutions or I’ll leave them mid-way? Stay tuned for next update (March end)
Also, have you undergone any training / tutorial in last one month or have any reading recommendation to share? If so, please do so with the group.
If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our facebook page.
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Tips for creating a winning dashboard","Kunal Jain",2014-01-27 12:10:00,2,"
							
										
						Recently I came across this article from Software Advice, a website that reviews business intelligence tools, called “Winning Dashboard Creation Tips from the Qlikview Open Data Challenge Champion“. While the article is from the winner of the Qlikview challenge, Mr. Alexandre Perrot, the tips which he has provided apply to any dashboard you create.


The following were three main takeaways for me:
1. Importance of structuring and creating layout / structure before you dive in your data. I had mentioned a similar practice in one of my previous articles here.
2. Importance on visuals and use of colours, especially to highlight the changes.
3. Keeping it simple and effective.
You can read the entire article on dashboard creation tips here.



Thanks Software Advice for conducting the interview and sharing the tips!
If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our facebook page.
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Debate between benefits from big data and compromise on user privacy","Kunal Jain",2014-01-21 12:50:00,3,"
							
										
						Today’s post is going to be different.
There is no technical subject matter I am going to talk about. But the article is far more thought provoking than any of the article I have written till date.
 A real life incident:
Let me start with a real life example to get your thinking process started:
About 6 months back, I bought a top end Android smartphone. After using it for a month or so, I accidently started Google Now on the phone. The interface looked very simple on first look (nothing more than a search bar and weather update). So I moved back from the application and started living my usual life.
I would have almost forgot this instance like multiple other applications which come with the phone and I don’t use. However, Google had something else in store. A week after I opened the application for first time, I got a notification on my home screen, suggesting that I am 15 minutes away from Home and the traffic on route is normal!
The notification took me by surprise. I never told my phone where my home is! Over the next few days, the application identified my Office, commute place, friends place, the websites I visit frequently. It now integrates my searches across devices. So if I search a restaurant on my laptop, my phone shows me the route to same restaurant!

The incident above is like a dream come true for a lot of analysts and a scary incident leading to loss of privacy to a lot of customers.
As an analyst and some one who specializes in predictive modeling, I am usually a proponent of big data and the changes it is bringing to our day to day life. However, I have to admit that Google took me by surprise and has made me think and reflect a lot more on how life is changing. It has ensued a debate between 2 sides of my personality.
Two sides of debate:
My first personality is that of a common man. I want my privacy, specially during some personal moments. These moments could be the time I spend with my family or when I am reading or may be talking to a friend. I don’t want interruptions or suggestions from any third party during this period. I want to relish the moment as it is. After going through the experience mentioned above and many more like that, I am not sure whether these moments will remain as pristine and unadulterated as I would want them. Would my reading experience be marred by suggestions about different things I might like? Would the phones pop up notifications about my friend when I am talking to them? or may be when I am talking about them to my wife? The possibilities are limitless!
The other side of my personality is a big proponent of technology and Analytics. I remain excited about how technology can be used to solve day to day problems. I come out with innovative ways of using data to create value (for customers as well as Organizations). I continuously think how behavioural modeling can help customers in breezing through day to day chores? How can I predict something before it actually happens.
How do we resolve this?
After a lot of thought and internal debate, I have come to a conclusion that we are standing at that juncture of time when both these personalities need to change. The first personality needs to be aware about the data trails I am creating and actively cut them out if I don’t want them. So, I have stopped carrying my smartphone when I play around with my daughter. I need to be aware that nothing gets erased from this new world we are living in, be it a comment on Facebook, a status update, my geo-location map (through the mobile device I am carrying). It is only a matter of next version of Android roll out before my calls get searched for what I am looking out for and I get app suggestions based on that.
The second personality needs to be cognizant about the presence of first personality and take actions which are in sync with values of first personality. Here are some rules I have come out with, which every analyst needs to keep in mind while designing a product or working on his next big data project:

1. Transparency:
This is the biggest takeaway. The bare minimum an analyst needs to make sure is that the customer is aware about what data is being collected and how can this be used. This needs to come out clearly. This is similar to apps (on smartphones) asking permissions before installing them. If you are collecting data with out asking customer explicitly, you are headed for disaster.
So, instead of using data through a pre-selected tick box (buried somewhere is my phone settings), I would have appreciated if the app reminded me of the data it will use, when I started it for the first time.
2. Develop a character of your Organization by keeping customer at the heart:
Let me try and explain. Years before Google started collecting information about usage from Android phones, Microsoft started this for MS Office. They asked me whether I would want to share my usage patterns with Microsoft, which will help them improve user experience further. I almost always declined. When Google asks me same thing, I am more open to sharing information.
It might be a personal choice. However, the reality is that I am more open to sharing data with Google because I can relate to the benefits they have provided me by using this information. I have benefited by sharing some of this information with Google.
The message is that if you don’t provide the benefit of this information back to the consumer, they will stop sharing this information.
3. Make change in subtle manner:
Big changes in user interface or the way new product gets rolled out can take customer by surprise. You have to build in these changes in subtle manner. In a way such that the customer still feels as much at home as possible. I think Google does a nice job at it. Here are some best practices:
Provide an option to user to switch back to old proposition, if it is not working for him
Roll out changes slowly and in steps. Recently Google organized Gmail in different tabs, a move which benefited me by making sure I spend maximum time on relevant mails. But they also started to show ads in one of the tabs. I thought it was a smart way to test. If customers feel distracted by these ads, they will pull them out. If customers get used to them, they can roll them out to different tabs.
Try and keep as much user interface unchanged as possible.
4. Test and roll-out:
Irrespective of how good an idea is, you should avoid making complete roll-outs without testing. There are multiple benefits from this:
You actually act based on how customer feels about the product
You can size the benefit / loss you have seen by moving to a new product.
I think until and unless Organizations and analysts adhere to these rules, it might only be a question of time before they face a bunch of disgruntled customers.
What do you think about these rules? Are there any other rules, you think Organizations should follow? Or which you follow while rolling out your big data projects. Do let me know your thoughts through comments below.
Image 1 credit: tnooz.com
If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our facebook page.
 
 
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Starting a big-data analytics practice? Answer these questions first","Kunal Jain",2014-01-13 03:50:00,2,"
							
										
						Are you a business owner who is wondering how can Analytics / Big data help me out? Or you are convinced that data mining can add value, but clueless about next steps? or have you attended multiple conferences under the headline of big data, but still have little idea about how to start it in your own Organization?
First of all, don’t be dis-heartened, you are not alone! Multiple surveys have shown that while there is a lot of talk on big data, only a few people are actually working on it.
How to start big data Analytics?
This article lays out 5 questions every business head should answer before starting this journey. These questions are not a solution to your technical questions related to big data and would not provide you a remedy to all your questions but they will provide you enough stimulation to challenge and really understand the core of big data analytics proposition for you.
 Question 1: What business are you in? 
This might sound a really silly question to start with, but this is most powerful one as well. You need to look for an answer beyond your BAU business model. Let me give you a real life example to try and explain what I mean.
What business is McDonald’s in? If you said selling fast food, you’d be wrong. In 1974, Ray Kroc, the man who started the golden arch empire asked an MBA class at the University of Texas, Austin this same question. Of course, everyone said hamburgers. Kroc’s answer surprised everyone: he said he was in the real estate business.
Here’s the way he explained it: his primary business focus was to sell franchises of the restaurant. He knew, however that real estate and location were the most important factors of success for each franchise. A poorly placed restaurant would go broke. McDonald’s is now the single largest owner of real estate in the world.
Here is food for some more thought. List out what you think each of the business is in:
Amazon
Tesco
Netflix
Coursera
You?
Look for an answer beyond the obvious. Netflix is not in business of renting out / streaming movies, it is in business of understanding viewing habits of people. Similarly, Coursera is in business of understanding learning behaviour of people.
So, now again, Which business are you in?
In answering so, you might figure out that the real differentiator to your business is different from the traditional business model you had in your mind.
In today’s landscape, a credit card provider is not in a business of a Financial risk management. It is important, but its true business is likely to be understanding consumer spending. If they provide a spending offer a month after the customer needs it, they are doomed!
Question 2: Can data / behaviour insights change the way you do or look at your business?
Now that you have probably listed down which business are you in, you need to check how much data do you own in that business. Following are the questions you need to answer:
Do you own the data and journey?
If yes, can you increase the ownership? Can you store more information which can provide you information about what the customer wants?
If no, how do you start owning the data and the journey?

If you get insights on consumer behaviour, what kind of changes can you make today? What do you think is the potential revenue opportunity there?
Question 3: Do you want to build in-house data capability or Outsource?
The answer depends on how much importance / differentiating capability you attach to Analytics in your core strategy and how much revenue opportunity you put on it. While it is easy to take decisions on the extremes, it might be more difficult to make decisions if you lie somewhere in between. There are several hybrid models, which might work if you lie somewhere in between.


Question 4: Do you need to invest in people or you have the required talent in-house?
This might as well be a Question 3 b. Once you are clear whether you need to Outsource or not and in what capacity, you need to do a reality check versus your current talent pool. Needless to emphasize the importance of this step.
Scenario 1: If you think you have the right talent in-house, you should ask:
Why have we not started the journey already?
What have been the results of past initiatives these people / team have led?

Scenario 2: You do not have the right talent inhouse. In this case, create a job description, find out relevant candidates, take time to understand their perspective, involvement in transition in previous companies and ability to lead change.
Question 5: What should be the plan for roll-out of this initiative?
Once you have all the above questions answered, you need to think about how would you go about penetrating Analytics to the level in your Organization you really want? Following are some of the guidelines in creating this action plan:
Identify specific problems (hopefully core of what business are you in) and work on those problems
Communicate your vision to your team and create a buy-in with them
Identify a few low hanging fruits to increase engagement with business
Once you have answers to these questions, hopefully you will have all the weapons you need to create an action plan. Here are a couple of case studies with a few CEOs and their answers to the five questions:
Case study: Credit card provider:
Question 1: What business are you in?
There are 3 parts which define the core of our business proposition:
Identifying the right risk profile of customers to be underwritten on right products
Managing the customer life cycle effectively and efficiently. Some of the questions we try and answer here are:
Can we anticipate when a good customer needs an offer to shift his big spends on to our cards?
Can we identify transactions which illustrate worsening risk for customer?

Managing Fraud:
This is another space where application of big data comes in place. What are the patterns which might indicate dubious behaviour?

Question 2: Can data / behaviour insights change the way you do or look at your business?
For us, mining data to bring out behavioural insights is the core of solving business problems. We place a very high value on any insight generated in each of these areas highlighted above. We typically own the entire data in this journey, which is a big advantage compared to some other industries. We are increasing moving to a space where data is looked at as an asset on Balance sheet of our Organization.
Question 3: Do you want to build in-house data capability or Outsource?
For us the answer was very evident. Once we were clear on the value, the question was about finding the right talent to build in-house capabilities.
Question 4: Do you need to invest in people or you have the required talent in-house?
We invested heavily in specialists to build our analytics expertise. The Leadership team was actively involved in screening the candidates themselves. Once the team was in place, managing their attrition has been a key priority for our Organization.
Question 5: What should be the plan for rollout of this initiative?
Since, we were convinced about the value Analytics can add and there are Organizations across globe who have already created differentiation based on data (e.g. Capital One, American Express), we started work on key strategic areas immediately. While we did not necessarily look out for low hanging fruits, speed to execution was really key for us. If we know about a trend which might signify potential fraud, it had to be implemented quickly. You cannot sit on information like this.
So, what do you think about these questions? Would they help you provide direction in setting up an Analytics practice? Are there additional factors you would want to answer in initial stages of starting a data practice? Do let me know your thoughts.
If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our facebook page.
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"My resolutions for 2014","Kunal Jain",2014-01-02 22:39:00,5,"
							
										
						In my last post, I mentioned how 2013 has been a phenomenal year for me. I can’t wait to continue the momentum in 2014. The year looks full of promises, a lot of learnings and value creation.
So what are my plans for 2014?

I plan to take a few resolutions and then track myself against them. I think that doing so guides and anchors my growth as a person. Last year, I completed 3 out of 4 resolutions I took. I am hoping to hit 100% achievement this year.
Guidelines:
While the web is full of suggestions on how to and how not to make resolutions, I will just mention the guidelines I follow to create my resolutions:
I usually keep only 3 – 4 resolutions in a year. Any more than that and there is a huge risk of not achieving majority of them (Yes, there have been years when I wanted to achieve more than 10 of them!).
The intention is to have measurable and objective resolution. So each resolution can be handed to a third person who can judge me on them at the end of year and tell me my success rate.
 
Resolutions:
So here are the resolutions I have kept for myself for 2014:
More frequent usage of R and Python in my analytical endeavors. I rely heavily on SAS and excel for the work I do. While I have learnt R & Python and used R sparingly, I am still some steps away from executing entire project on both these languages. By end of year, I plan to deliver at least 1 project each on R and Python independently.
Continue contributing actively in creating a community of analysts (in India): While the journey of Analytics Vidhya started last year, I plan to continue and increase the ways this platform becomes useful for development of a analyst community (specifically in India).
So, how would I measure this? The outputs I will measure are obviously readership and audience engagement on Analytics Vidhya. The inputs I will track myself against are number of posts, number of conferences attended / presented.
Re-start reading: I used to religiously read for ~30 minutes before hitting bed every day. However, this changed after we were blessed with Jenika (my daughter). My mid-night baby sitting and blog engagement have left little room for reading at night. I’ll re-start reading for an hour at least 2 days in a week.
5 kg weight loss: This is the only un-ticked resolution from last year. I am hoping to reduce some belly fat I have accumulated over last few years. My family and friends will tell you that I write this every year and the strike rate has been low. I am hoping to change it this year.
So, here is my reminder sheet for rest of the year:

That’s it! I’ll keep you posted on how I track against these during the year and I am hoping to hit 100% achievement on each of these.
What are your resolutions for the year? How do you plan to grow as a professional? If you have set aside resolutions, please feel free to post them and we can track ourselves collectively. If not, would you want to join?
If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our Facebook page
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Highlights of 2013 for Analytics Vidhya","Kunal Jain",2013-12-27 04:11:00,6,"
							
										
						2013 has been an outstanding year for me personally. Among other things, there have been 2 key highlights for this year:
Becoming a father to the cutest baby (Jenika) in this world; and
Starting up and running Analytics Vidhya
While there is no comparison between the 2, there have been times when I felt that I am raising 2 babies simultaneously. I still remember sitting along side baby incubator (while Jenika was being treated for Jaundice) and scrambling my notes @ 3:00 a.m. in morning to make sure that a post goes out next day.
So while I was creating a collage of Jenika’s photos from 2013, I thought it might be a good idea to quickly reflect on 2013 for Analytics Vidhya and bring out some of the highlights.

Here is what we achieved in 2013:
In the first 8 months of existence, Analytics Vidhya was visited by more than 20,000 unique visitors who made more than 33,000 visits to the blog.
We have published a total of 51 posts till date and are still counting
Our monthly visits has been consistently growing (except for the month when I took ~15 day break and Jenika was born).
I can go on and on….but you get the idea! We are one of the most read blogs in India in Analytics domain.
To summarize, the journey has been extremely fulfilling for me personally and every one involved in creation of Analytics Vidhya. We hope that we have created a difference in life of several analysts and the larger analytics community.
We thank our audience for their engagement, feedback and comments and hope that we have contributed to their Analytics learning in some way or the other. For those of you, who might have missed some of the awesome articles and discussions we have had, following are the key articles we posted on various topics:
 How to start and build a career in Analytics? 
How to start a career in Business Analytics?
What is Business Analytics and which tools are used for Analysis?
How to identify a good (and bad) Business Analyst?
Certifications and trainings for continued learning: 
Advanced Analytics certifications in India
Training recommendations from last 2 months
Analytics best practices:
How to become an Analytics rockstar?
How to create Financial models flawlessly?
The art of structured thinking and Analyzing
Common mistakes analysts make and how to avoid them?
Upcoming trends in data visualization
Four rules for creating insightful and actionable reports (and metrics)
Career related advice:
Taking up a job in Analytics? Ask these 5 questions first!
Common myths about a career in Business Analytics: Busted!
Five habits of highly successful analysts
How to create a High performance Analytics team?
Other resources for an analyst:
Must read books for Analysts (or people interested in Analytics)
Must read books on data visualization
Must read books and blogs on Web Analytics
Subject matter knowledge: Regression, Clustering and Segmentation:
Trick to enhance power of Regression model
Diagnosing residual plots in linear regression models
Importance of segmentation and how to create one?
Getting your clustering right (Pt 1)
Getting your clustering right (Pt 2)
Articles on SAS
4 tricky SAS questions commonly used in Interview
9 productivity boosting tips for SAS Enterprise Guide users
Articles on Qlikview
Creating a simple and effective Sales dashboard (with Qlikview)
How to create a waterfall chart in Qlikview?
Review: Tableau 8.1
Articles on Big data:
What is big data and how is big data architecture designed?
Interview with data scientist and top Kaggler, Mr. Steve Donoho
Articles on web analytics:
How to apply web Analytics for an e-commerce website?
Questions to ask while designing A/B (or multi-variate) tests
The sheer number of articles in this list is just a representation of the effort and quality time we have spent creating Analytics Vidhya. We hope that we have created enormous value for our audience and would continue to do so in 2014.
If you have any suggestions, feedback, instances how Analytics Vidhya helped you or what you would want more out of Analytics Vidhya please let us know. As always, we are eager to listen from you.
Wishing a Happy and Properous New Year to all our audience, team members and patrons.

If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our Facebook page.
Image credit – ebuilder.net and Godwallpaper
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Framework to check accuracy of your datasets","Kunal Jain",2013-12-22 22:00:00,1,"
							
										
						As the day was coming to a close, I thought of fitting in another meeting. Two analysts in my team had been working for creating a data set for one of the predictive models we wanted to build. The combined work experience (on predictive modeling) between the analysts was ~ 5 years. I expected to breeze through the meeting and leave for the day.
So, the meeting started. Five minutes into the meeting and I knew that the meeting will take much longer than I initially thought!
The reason? Let’s go through the discussion as it happened:

Kunal: How many rows do you have in the data set?
Analyst 1: (After going through the data set) X rows
Kunal: How many rows do you expect?
Analyst 1 & 2: Blank look at their faces
Kunal: How many events / data points do you expect in the period / every month?
Analyst 1 & 2: …. (None of them had a clue)
The number of rows in the data set looked higher to me. The analysts had missed it clearly, because they did not benchmark it against business expectation (or did not have it in the first place). On digging deeper, we found that some events had multiple rows in the data sets and hence the higher number of rows.
A high percentage of analysts would have gone through similar experience at some point or other in their career.
At times, either due to timeline pressures or due to some other reason, we overlook doing basic sanity checks on the dataset we are working on. However, overlooking data accuracy at initial stages of project can prove very costly and hence usually it pays off to be paranoid about data accuracy.
I usually follow a simple framework for checking accuracy of data points. In this article, I’ll share the process, I typically use for checking data sanity. The framework goes top down, which suits well. If you have any glaring mistakes in the data sets, they would be evident early in the process.
Please note that the remaining article assumes that you are working on a structured data set. For unstructured datasets, while the principles would still apply, but the process would change.

Step 1: Check number of columns and rows against expectations 
The first step as soon as you get any data set would be to check whether it has all the required rows and columns. Number of columns would be dictated by the number of hypothesis you have and the variables you would need to prove / dis-prove these hypothesis.
Number of rows on the other hand would be dictated by number of events you expect in the chosen period. The easiest benchmark would be based on your business understanding.
Step 2: Check for duplicates at id level (and not for entire row) 
Once you are sure all the columns are present and number of rows look within expected range, quickly check for duplicates at level of your id (or the level at which the rows should be unique – it could be a combination of variables)
Step 3: Check for blank columns, large % of blank data, high % of same data 
Now that you know all columns are there and there are no duplicates, look out if there are columns which are entirely blank. This can happen in case some join fails or in case there is some error in data extraction. If none of the columns are blank, look at the % of blank cases by each column and frequency distributions to find out if the same data is being repeated in more cases than expected.
Step 4: Look at the distribution across various segments – check against business understanding and use pivot tables
This step continues where 3 finishes. Instead of looking at frequencies of data points individually, look at their distributions. Do you expect normal, bi-polar or uniform distribution? Does the distribution look like what you expect?
Step 5. Check outliers on all key variables – especially the computed ones
Once the distributions look fine, check for outliers. Especially in cases where you have computed columns. Do the values on extreme look like as you had wanted? Make sure there are no divisions by zero, you have capped the values you would want to.
Step 6: Check if values of a few test cases are in sync
Once you have checked all the columns individually, check whether they are in sync with each other. Check whether various dates of cases are in chronological order (e.g. . Do the balances, spend and credit limit look in sync with each other for your credit card customers?
Step 7: Pick up a few rows and check out their values in the underlying systems
Once all the previous steps are done, it is time to check a few samples by querying the underlying systems or databases. If there was any error in data, you should have ideally identified it by now. This step just ensures that the data is as it was in the underlying systems.
Please note that some of these errors can be spotted through use of logs provided by your tool. Looking at the logs usually provides a lot of information about errors and warnings.
These were the steps I use to check the accuracy of data and they usually help me to spot any glaring errors in the data. Obviously, they are not the answer to every possible error, but they should give you a good starting point and direction. What do you think about this framework? Are there other framework / methods you use to check the accuracy of data? If so, please add them in the comments below.
If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our Facebook page.
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Interview with data scientist and top Kaggler, Mr. Steve Donoho","Kunal Jain",2013-11-29 03:10:00,6,"
							
										
						It’s our pleasure to introduce top data Scientist (as per Kaggle), Mr. Steve Donoho, who has generously agreed to do an exclusive interview for Analytics Vidhya. Steve is living a dream which most of us think of! He is founder and Chief data Scientist at Donoho Analytics Inc., tops Kaggle ranking for data scientists and chooses his areas of interest.
Prior to this, he worked as Head of Research for Mantas and as Principal for SRA International Inc. On the education front, Steve completed his graduation from Purdue University followed by a M.S. And Ph.D. From Illinois University. His interest and work include an interesting mix of problems in areas of Insider trading, Money Laundering, Excessive mark up and customer attrition.
On a personal front, Steve likes trekking and playing card and board games with his family (Rumikub, Euchre, Dutch Blitz, Settlers of Catan, etc.).
 
Kunal: Welcome Steve! Thanks for accepting the offer to share your knowledge with our audience of Analytics Vidhya. Kindly tell us briefly about yourself and your career in Analytics and how you chose this career.
Steve: When I was in grad school, I was good at math and science so everyone told me, “You should be an engineer!”  So I got a degree in computer engineering, but I found that designing computers was not so interesting to me. I found what I really loved to do was to analyze things and to use computers as a tool to analyze things. So for any young person out there who is good at math and science, I recommend you ask yourself, “Do I love to analyze things?”  If so, a career as a data scientist may be the thing for you. In my career, I have mainly worked in financial services because data abounds in the financial services world, and it is a very data-driven industry.  I enjoy looking for fraud because it gives me an opportunity to think like a crook without actually being one.
 
Kunal: So, how and when did you start participating in Kaggle competitions?
Steve: I found out about Kaggle a couple years ago from an article in the Wall Street Journal.  The article was about the Heritage Health Prize, and I worked on that contest. But I was quickly drawn into other contests because they all looked so interesting.
 
Kunal: How frequently do you participate in these competitions and How do you choose which ones to participate?
Steve: I’d have to say that I do about one each month if there are interesting-looking contests going on. I try to pick contests that will force me to learn something new. For example, 12 months ago I would have had to say that I knew very little about text mining.  So I deliberately entered a couple text mining contests. Once I made an entry, the competitive spirit forced me to learn as much as I could about text mining, and other competitors post helpful hints about good techniques to learn about. So it is a great way to sharpen your skills.
 
Kunal: Team vs. Self?
Steve: I usually enter contests by myself.  This is mainly because it can be difficult to coordinate with teammates while juggling a job, contest, etc.
 
Kunal: Which was the most interesting / difficult competition you have participated till date?
Steve: The GE Flight Quest was very interesting. The challenge was to predict when a flight was going to land given all the information about its current position, weather, wind, airport delays, etc.  After being in that contest, when I looked up and saw an airplane in the sky, I found myself thinking, “I wonder what that airplane’s Estimated Arrival Time is, and will it be ahead of schedule or behind?” I have also liked the hack-a-thons which are contest that last only 24 hours – it totally changes the way you approach problem because you don’t have as much time to mull over the problem.
 
Kunal: What are the common tools you use for these competitions and your work outside of Kaggle?
Steve: I mostly use the R programming language, but I also use Python scikit-learn especially if it is a text-mining problem.  For work outside Kaggle, data is often in a relational database so a good working knowledge of SQL is a must.
 
Kunal: Any special pre-processing / data cleansing exercise which you found immensely helpful? How much time do you spend on data-cleansing vs. choosing the right technique / algorithm?
Steve: Well, I start by simply familiarizing myself with the data.  I plot histograms and scatter plots of the various variables and see how they are correlated with the dependent variable.  I sometimes run an algorithm like GBM or Random Forest on all the variables simply to get a ranking of variable importance.  I usually start very simple and work my way toward more complex if necessary.  My first few submissions are usually just “baseline” submissions of extremely simple models – like “guess the average” or “guess the average segmented by variable X.”  These are simply to establish what is possible with very simple models.  You’d be surprised that you can sometimes come very close to the score of someone doing something very complex by just using a simple model.
A next step is to ask, “What should I actually be predicting?”  This is an important step that is often missed by many – they just throw the raw dependent variable into their favorite algorithm and hope for the best.  But sometimes you want to create a derived dependent variable.  I’ll use the GE Flight Quest as an example – you don’t want to predict the actual time the airplane will land; you want to predict the length of the flight; and maybe the best way to do that is to use that ratio of how long the flight actually was to how long it was originally estimated to be and then multiply that times the original estimate.
I probably spend 50% of my time on data exploration and cleansing depending on the problem.
 
Kunal: Which algorithms have you used most commonly in your final submissions?
Steve: It really depends on the problem.  I like to think of myself as a carpenter with a tool chest full of tools.  An experienced carpenter looks at his project and picks out the right tools. Having said that, the algorithms that I get the most use out of are the old favourites: R’s GBM package (Generalized Boosted Regression Models), Random Forests, and Support Vector Machines.
 
Kunal: What are your views on traditional predictive modeling techniques like Regression, Decision tree?
Steve: I view them as tools in my tool chest. Sometimes simple regression is just the right tool for a problem, or regression used in an ensemble with a more complex algorithm.
 
Kunal: Which tools and techniques would you recommend an Analytics newbie to learn? Any specific recommendation for learning tools with big data capabilities?
Steve:  I don’t know if I have a good answer for this question.
 
Kunal: I have been working in Analytics Industry for some time now, but am new to Kaggle. What would be your tips for someone like me to excel on this platform?
Steve: My advice would be to make your goal having fun and learning new things. If you set a goal of becoming highly ranked, it will become “work” instead of “fun” and then it will become drudgery. But if you set your goal to have fun and learn, then you will pour all your creative juices into it, and you will probably end up with a good score in the end. Kagglers are very helpful. We love to give hints in the forums and tell how we approached a problem after the contest is over. When I started on Kaggle, I just went back to all the completed contests and read the “Congratulations Winners! Here’s how I approached this problem” forum entry where all the winners gave away their secrets. I picked up a lot of great tips that way – both for what algorithms to learn and techniques I had not thought of.  It expanded my tool chest.
 
Kunal: Finally, any advice you would want to provide to audience of Analytics Vidhya?
Steve: Here are some thoughts based on my experience:
Knowledge of statistics & machine learning is a necessary foundation.  Without that foundation, a participant will not do very well.  BUT what differentiates the top 10 in a contest from the rest of the pack is their creativity and intuition.
I think beginners sometimes just start to “throw” algorithms at a problem without first getting to know the data.  I also think that beginners sometimes also go too-complex-too-soon.  There is a view among some people that you are smarter if you create something really complex.  I prefer to try out simpler.  I *try* to follow Albert Einstein’s advice when he said, “Any intelligent fool can make things bigger and more complex. It takes a touch of genius — and a lot of courage — to move in the opposite direction.”
The more tools you have in your toolbox, the better prepared you are to solve a problem.  If I only have a hammer in my toolbox, and you have a toolbox full of tools, you are probably going to build a better house than I am.  Having said that, some people have a lot of tools in their toolbox, but they don’t know *when* to use *which* tool.  I think knowing when to use which tool is very important.  Some people get a bunch of tools in their toolbox, but then they just start randomly throwing a bunch of tools at their problem without asking, “Which tool is best suited for this problem?”  The best way to learn this is by experience, and Kaggle provides a great platform for this.
Thanks Steve for sharing these nuggets of Gold. Really appreciated!
Bonus: In addition to this interview, Steve has agreed to answer a few specific questions from readers. For the benefit of everyone, I would urge you to keep them as specific as possible and avoid asking questions already answered as part of the interview. Please post your questions in the comments below. Steve will answer the questions once he is back from Thanksgiving holidays.
If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our facebook page.
 
Image (background) source: theninjamarketingblog
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Review: Tableau 8.1","Kunal Jain",2013-11-22 05:08:00,6,"
							
										
						As a Business Analyst, I have been a predictive modeler for most of my career. Majority of this time was spent on SAS along with tools like CART.
My usage of in memory BI solutions / mashups started with Qlikview about 2 years back. I found it simple to use, which along with its slicing and dicing capabilities helped faster data discovery. I also started using Tableau about a month back and was impressed by its native ability for geo-spatial analysis and simplicity of use (no coding required!).
I have used these tools in broadly 2 ways:
Deploying dashboards across an Enterprise
Using mashups to explore data – popularly known as data discovery
Yesterday, Tableau Software announced a new version of their tool (Tableau 8.1). The new release has a bunch of advances over the last version. I have spent some time going over the new features and here are my thoughts about them. Please note that this is my initial impression. I’ll update the article, in case my view changes.

List of new features:
1. Improvement in Analytical capabilities:
Tableau now integrates with R. This in my mind is one of the big bets for Tableau in this version. As an analyst, I always missed some of the advanced analytical functions in the mashups. For example, features like clustering and correlation matrix can make data exploration lot more meaningful. So, when I heard about this feature, I got really excited. What this means is that now I would be able to run clustering through Tableau with use of SCRIPT_ command. There are still some limitations to this integration:
It is not possible to export data from Tableau to R outside of SCRIPT functions
It is also not possible to import data from R to Tableau directly
This looks like a welcome feature which will expand the capabilities of Tableau. This also enables me to now monitor my models in R through a set of dashboards on Tableau.
Box and Whisker plots on click of a button. Again, as an analyst, I am used to looking at these plots and these would enable better representations about the distributions. However, they are primarily used by statisticians and analysts. The larger business community would not be able to take out inferences from these charts.

Ranking and (1 click) percentiles. These are some features which are widely used in dashboards. Having them easily accessible will help creation of better dashboards and analysis.
2. Better visualization:
I think Tableau already does a good job at visualization. Hence, I didn’t expect too much improvement. I think most of the improvements in this area are either incremental in nature or are filling gaps about features, which should have been there earlier. The list includes:
Copy content within workbooks
Presentation mode
Dashboard transparency
3. Improvements in data integration:
There are 2 features I would want to call out here:
Dateparse functions: Convert text strings into datetime
Google Analytics segments: Advanced segmentation now available in GA connector
I think both the features were expected from perspective of making the tool comprehensive.
4. 64 bit support:
Entire Tableau product suite is now available on 64 bit support. Given the amount of machines using 64 bit architecture, this was expected. Should provide some extra speed on these machines now.
5. Web authoring in android app: 
Now you can edit views in your android app, which would enable you to perform better slicing and dicing of data on the go. I think this is a cool feature, given the increased usage of mobile devices to access information.
Conclusion:
Overall, I think Tableau has made enough changes to continue its good run in the industry. Integration with R and web authoring in Android app are the highlight for me in this release. I think Tableau makes an attempt to improve its analytical capabilities, and improves it to some extent, but is still away from a place where it could become the only tool to perform all data exploration (especially for model building).
Would this release force other mashups to integrate seamlessly with advanced analytics engines? What are your views about Tableau 8.1? Were you expecting some additional features as part of this release? Please let me know through comments below.
If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our facebook page.
 
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Simple manipulations to extract maximum information out of your data","Kunal Jain",2013-11-15 12:47:00,6,"
							
										
						How would you distinguish between best, good and worst analyst from a group of analysts?

I would simply provide them same problem and data set and ask them to create a predictive model out of it.

Why is this a sure shot test?

This is the core of any analysis or any data mining activity. The degree to which an analyst is able to bring out insights and trends is what differentiates a good analyst from a bad one. This is the sole idea on which competitions such as the ones running on Kaggle are built. In larger scheme of things, it is this ability to extract information from data, which differentiates companies like Google, Amazon and Capital One from their competitors.


Steps in creating competitive advantage while building predictive models: 
There are 2 key steps involved every time you try and create this competitive advantage:

Cleansing and transforming datasets to create a comprehensive data set which captures all possible hypothesis
Choosing the right set of predictive modeling tools and techniques which work on the data set created in last step and bring out insights


Both the steps in this process are equally important. We’ll focus on the first step in this article and discuss step 2 at a later date.

Building a comprehensive data set takes time and effort. The more time you can spend in transforming variables and extracting information, the better your model would be expected to perform. You can not and should not rely entirely on your tools to identify trends, insights or relationships.



 

This article addresses a situation which we face on a daily basis as analysts. How do we extract maximum information from a defined data set? By no means, this is an exhaustive list of extracting every possible information. But, it is good enough to get you thinking on the right track. If you have any other thoughts / practices which you use to extract more information about the variables, please add them in comments

1. Create variables for difference in date, time and addresses
While you might be using date and time values on their own, you can create new variables by considering differences in dates and time. Here is an example hypothesis: An applicant who takes days to fill in an application form is likely to be less interested / motivated in the product compared to some one who fills in the same application with in 30 minutes. Similarly, for a bank, time elapsed between dispatch of login details for Online portal and customer logging in might show customers’ willingness to use Online portal.

Another example is that a customer living closer to a bank branch is more likely to have a higher engagement than a customer living far off.

2. Create new ratios and proportions
Instead of just keeping past inputs and outputs in your dataset, creating ratios out of them might add a lot of value. Some of the ratios, I have used in past are: Input / Output (past performance), productivity, efficiency and percentages. For example, in order to predict future performance of credit card sales of a branch, ratios like credit card sales / Sales person or Credit card Sales / Marketing spend would be more powerful than just using absolute number of card sold in the branch

3. Apply standard transformations
By looking at variations and plots of a variable along with output, you can see if applying basic transformations on variables creates a better relationship. Most commonly used transformations include Log, exponential, quadratic and trignometric variations. For example, Log(Marketing spend) might have a more representable relationship with Sales as compared to absolute Marketing spend.

4. Include effect of influencer
Influencer can impact on the behaviour of your study significantly. Influencer could be of various form and sizes. It could be an employee of your Organization, agent of your Organization or a customer of your Organization. Bringing the impact of these related entities can improve the models significantly. For example, a loan initiated by a sub-set of brokers (and not all brokers) might be more likely to be transferred to a different entity after the lock-in period. Similarly, there might be a sub set of Sales personnel involved who do a higher cross-sell to their customers.

5. Check variables for seasonality and create the model for right period
A lot of businesses face some kind of seasonality. It could be driven by tax benefits, festive season or weather. If this is the case, you need to make sure that the data and variables are chosen for the right period. This article can give you more details on how to tackle effects of seasonality while modeling.


What do you think of these manipulations? Are there some other transformations which you use while creating a predictive model? In case there are, please add them in the comments below

If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our facebook page.

Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Questions to ask while designing A/B (or multi-variate) testing","Kunal Jain",2013-11-02 22:44:00,7,"
							
										
						Testing / experimentation can help Organization find hidden information or insights about their own customers and business.
Sadly, not many Organizations realize the amount and value of learning which can happen through testing. On one hand, there are Organizations like Google, Amazon, Capital One where testing is an integral part of the culture. On the other hand, there are Organizations which are unaware of testing and how to use it for generating key insights about your business.
 
For the starters, what is testing / experimentation?
Simply put, testing is learning through experimentation. Let us take a simple example. Imagine that you are responsible for Sales conversion of new customers for an e-commerce retailer. Your website currently has a 4 step check-out process. Your hypothesis is that by condensing this form to 3 steps (there by increasing length of some steps), you will be able to increase the conversion.
While you can try and answer this basis data (by looking at competitors, past experience (if any)), you can not find out the answer until you test it out. So, the easiest solution is to take half of your customers through proposed 3 step process (commonly called Champion) and the other half through existing 4 step process (Challenger or Control group). Run the test long enough to have statistically significant reads on conversion and check out what works for  your customers and your website.
For reading more on need and importance of testing, click here
Schematic of AB testing
The process of testing:
While the process of testing is simple, it requires high level of diligence and attention to details. To some extent, the skill of testing and analyzing tests effectively is like a vintage wine. The more time you spend practicing, the better you become at it.
Following is a schematic of how life cycle of a typical test looks. Overall process can be divided in 4 parts:
Define
Design
Implement
Analyze
In order to make sure that you get the maximum value out of any testing, it is very critical that you get the design right.
Process of testing
While each of these step is critical, the focus of this post is to provide a list of questions to be asked at design stage in order to gain the most out of these tests. These questions can suffice as a checklist to make sure you have considered all critical aspects of test design.
Questions to ask when designing test:
What is (are) the key hypothesis you want to test? Which metric would you measure to test this hypothesis?
How many test cells do you need to test out your hypothesis? Each version of treatment should be considered as a test cell.
What is the sample size requirement for each test cell? How many data points do you need to make the reads statistically significant?
How long do you need to run the test? Ideally this should be the period in which you can read the difference + some buffer.
What controls do you have in place to make sure that testing happens as intended?
What is the monitoring plan for the test? How frequently would the test be monitored? Who will perform the monitoring? What would be the escalation matrix if monitoring does not happen as planned?
What are the key risks to the success of this test? How can you mitigate these risks?
Was there any test performed in past to test the same / similar hypothesis? What was the outcome of that test?
How would the assignment of customers / events to a test cell happen? Would it be random?
If the assignment to test cell is not random, how will you make sure that there is no bias due to method of assigning test cell?
If it is random, how is the random number generated? Does it require a seed? How can you make sure that you do not provide same seed every time?
Is there a need to provide consistent treatment every time the event happens? How are you ensuring that? If there a CRM solution which you are using for this?
Are there any co-variates you need to measure during the testing? What is the impact of measuring the co-variates on sample size requirement?
Are your stakeholders aware of the testing procedure and requirement? How do you plan to make sure there are no strategic changes which happen while the test is running (which can impact the reads)?
Are there any confounding errors which could result in mis-interpretation of results?
How long will it take for results of test to come out? Is there any gestation period required?
What will you do if something goes wrong during testing? Will you stop the test and re-run? or you would continue with the test?
What is the maximum duration for which you can run the test? What level of lift and confidence interval with which you can read these results?
Last, but not the least, what is the expected benefit from the test?
Answering these 19 questions in a disciplined manner will help you identify any gaps in your test design. More often than not, that would mean significant savings in terms of resource and opportunity costs.
What do you think about these questions? Do you think they can help you create a better test design? Are there any other question you think should be part of this list? Please share your thoughts on these questions in the comments below.
If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our facebook page.
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Analytics training recommendations from last 2 months","Kunal Jain",2013-10-24 04:15:00,5,"
							
										
						One of the best part about being in Analytics industry is the opportunity (and need) to continuously learn new things and upgrade yourself. 
I am sharing list of training recommendation which I have undergone recently or am in process of undergoing. Each of these trainings have helped me with my Analytics learning and have helped me become a better analyst.

Computing for Data Analysis – Coursera:
I have used SAS for most of my Analytics career and had wanted to learn R for some time. I had downloaded it multiple times in past, but had struggled to get past initial stages. Finally, this course from John Hopkins University came to rescue.
A great starting course for R, which has left me with confidence of using R more often.
 
Tableau tutorials:
Another tool which I wanted to learn was Tableau. I have used Qlikview extensively for some time now, and hence I wanted to try out Tableau. While the tool is pretty user friendly and has great visualization capabilities, my favourite feature was how quickly it can perform geo-spatial visualization. The tool comes with in-build capabilities to recognize city and state names (I have been informed that this works till Tehseel level in India – impressive!) and you can map them in a matter of seconds.
Here is the link to tutorials which are provided on Tableau website. I had watched them selectively and found them useful.
 
Social Network Analysis – Coursera:
While I had used some tools here and had done some amount of reading on social network analysis, this course starts from basics of social networks and leaves you by providing understanding of Facebook and Twitter are using these techniques to understand their users better.
Good first course on Social network, which has left me wanting for more. I am hoping that i’ll be able to join Probabilistic Graphical Models (run on Coursera) as a follow up to this course.
 
Market Motive:
I plan to sign up for Web Analytics course from Market Motive in next couple of weeks. It comes at a higher cost than the trainings mentioned above. However, I have heard some great reviews about this course from people who have took it.
Will keep you posted on how it comes through.
 
Have you undergone any training recently and would want to recommend it? Please feel free to add them in comments below.
If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our facebook page.
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Must read books (and blogs) on Web Analytics","Kunal Jain",2013-10-17 23:02:00,5,"
							
										
						I love reading!
By reading something every day before sleeping, I not only continue my learning, but also end my day on a fulfilling note. This is third and final post in series of must read books (click here for Business Analysts and visualization).
The demand for web analytics professionals is set to increase. Here are some of the reasons why I expect so:
Increasing internet penetration across the globe
Growing e-commerce industry (especially in developing countries)
User data available at a very granular level (starting from where user clicked to how much time he spent reading about a product)
Integration across mobile devices and interfaces
All these reasons are enough to ensure that application of Web Analytics grows by leaps and bounds. With increased application, the world would need more professionals in this industry.
So, if you plan a career in Analytics and are not up to pace with Web Analytics, start learning it today. It is a dynamic field where things change frequently. Keeping that in mind, this list contains both books and blogs, as I think they need to be used simultaneously for effective learning.
There is a plethora of reading material on this subject. However, I have shortlisted only the best resources which would definitely add value to your web analytics learning. All these books and blogs are tremendous source of knowledge and have helped me increase my understanding of web Analytics:

  Books and blog by Avinash Kaushik 
Meet Avinash Kaushik, Digital Marketing Evangelist at Google, author of some of the best books on web analytics, blogger – Occam’s Razor (one of the most popular blog on web Analytics) and co-founder of Market Motive Inc.
No list of resources on Web Analytics can be complete without mentioning work of Avinash Kaushik. Whether you are new to web analytics or a pro in this field, you will still find tons of practical advice on his blog or in his books.
Here is a brief description of the books he has written:
Web Analytics: An Hour a Day
Through this step-by-step guide on how to implement a successful Web analytics strategy, Kaushik leads you on a path to gaining actionable insights from your analytics efforts. Discover how to move beyond clickstream analysis, why qualitative data should be your focus, and more insights and techniques that will help you develop a customer-centric mind-set without sacrificing your company’s bottom line.
Web Analytics 2.0
This book starts where the first one ended (though you can read them independently). It provides specific recommendations for creating an actionable strategy, applying analytical techniques correctly, solving challenges such as measuring social media and multi-channel campaigns, achieving optimal success by leveraging experimentation, and employing tactics for truly listening to your customers. The book will help you become a super analysis ninja!
  Advanced Web Metrics with Google Analytics by Brian Clifton 
This book can be easily called out as Bible of Google Analytics. Brian formerly led the Google Web Analytics team for Europe, the Middle East, and Africa, and he initiated and helped launch the online learning center for the Google Analytics Individual Qualification (GAIQ).
This book will teach you each and every aspect of Google Analytics, starting from basic reporting to creating segmentation, tracking social and mobile visitors, use of new multichannel funnel reporting features and much more. If there is only one book you want to read on Google Analytics, this is your book.
 How to Measure Social Media: A Step-By-Step Guide to Developing and Assessing Social Media ROI by Nichole Kelly 
If you face any of these questions and do not have a concrete answer, this book will provide you the answers:
“What is the ROI on social media campaigns you have run and invested in?”
Do they make repeat purchases more often than other customers?
How much revenue did our activities on social media platforms generate this month?
Are social media prospects more likely to convert to customers?
Which status update delivered the highest conversion rate?
How long do we retain new social media customers?
Do they spend more or less than customers from other channels?
This book will give you simple step-by-step techniques for creating measurable strategies and getting the data to prove they deliver. You’ll also get helpful hands-on exercise worksheets.
List of blogs to follow:
1. Occam’s Razor by Avinash Kaushik
2. Analytics Talk by James Cutroni
3. Official Google Analytics blog
4. Adobe digital Marketing blog (formerly known as Omniture Industry Insights)
5. Since I have not included any text on search, Search Engine Watch becomes a must follow to be up to speed with development on Search
Special mention (book):
Search Analytics for Your Site: Conversations with Your Customers by Louis Rosenfeld was a very useful book when I read it. However, given the recent rise in “Not Available” keywords on search, I have strayed away from recommending it as must read.
On my reading list:
Web Analytics Action Hero: Using Analysis to Gain Insight and Optimize Your Business by Brent Dykes
What do you think about the list? Are there any other recommendations you would want to share on the subject? Please add them in the comments below.
You might also like:
Must read books for Analysts (or people interested in Analytics)
Must read books on data visualization
If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our facebook page.
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Four rules for creating insightful and actionable reports (or metrics)","Kunal Jain",2013-10-10 20:51:00,5,"
							
										
						Here is a typical situation during performance reviews:
“All the business leaders and stakeholders are present in a room. A performance report / MIS is projected. The colorful report contains a whole list of metrics starting from business inputs (like resources deployed, cost of marketing etc.) and the output (like Sales, applications, X-sell, up-sell).
Following is a snapshot of one such report for a credit card lender:

 
While the report might be doing a good job of what is happening, there are some serious problems with this report. Can you spot some of them?
The biggest challenge in using this report is that it only answers what happened, it makes no attempt to answer why and how.
So when you use this report for performance reviews, there are only 2 possible outcomes:
Business users answer why and how basis their knowledge and ability to put some story
The performance review ends up being a summary of what happened with no actions or insights about what is working well and what is not.
Both these outcomes are far away from the objective of any performance review.
 
So, how do you make sure that your reporting has meaningful metrics which bring out insights and drive actions?
The remaining post assumes that you have good business understanding and context, if that is not the case, please do so first. Here are four rules which I have learnt over years which will help you create insightful and actionable reporting:
 1. Use absolute numbers only when comparing against targets or benchmarks. For all other metrics, ratios and percentage are more powerful and meaningful:
The only purpose of keeping absolute numbers should be when you want to compare them against targets or benchmarks, or they are there for some contexts. Otherwise, keeping absolute numbers don’t add value in reporting / analysis.
For example, if you simply state that we sold 1400 Credit cards this month (lets say it is lower than plan), it only tells business that  we did not achieve plan. Comparing against targets or performance last year / last month can provide a context (again only what happened), but you need to move to some ratios to understand why and how of it. Following are some of the questions you can ask and report through metrics:
Has Sales per Sales person improved or deteriorated? If it has improved compared to expectation / benchmark, you know that the drop is because of lower number of Sales person deployed. Otherwise, it is driven by something else.
Are there particular channels whose performance is lower than expected? Possible meaningful metrics to look at are (number of cards sold through a channel / expense of the channel) or (number of cards issued / number of cards applied)
Looking at percentage of various cards (products) will give an idea whether this is driven by lower Sales of particular products.
 
 2. Segment till  you can!:
Some of the questions in previous point touch on this briefly. Averages can not drive action, you need to analyze and understand things at a segment level. Click here to understand how to create a segmentation.
In summary, each segment would differ differently and you can not take action until you report / analyze things at segment level. If you know that the drop in this question is driven only by a channel or product which has not performed well, it is easier to take actions on that basis.
 
3. Use visualizations and drill downs instead of just reporting numbers:
Use of graphs, visualizations and drill downs can just take actions and insights to the next level. The main benefit of graphs over just numbers is that it gives you lot more context compared to just staring at numbers. A simple look at the graphs can answer various questions like:
Has the drop occurred this month or we are recovering from a drop in past?
When did the performance start to worsen?
Further, drill-downs can represent the segmentation effectively and pin point the areas of improvement. For example, the drill down in image below conveys same information in a better manner.

 
4. Tie in metrics to the fundamental level of action:
While you can start the reporting at high level, the actions can only happen when relevant metrics at right level are looked at. For example, looking at Approval ratio by Regions / Geographies will help you answer what, but looking at Approval ratio by Sales people can help trigger instant actions. You can provide better training to these Sales people or ask them to re-look at their target segment.
Similarly, until you analyze and report Keyword level performance, you can not optimize Search campaigns.
 
What do you think about these rules? Do you have any rules which make your reports more insightful and actionable? Please add them through comments.
If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our facebook page
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"News: Edvancer Eduventures start CBAP course in Mumbai","Kunal Jain",2013-10-07 04:35:00,2,"
							
										
						In continuation to our previous articles (here and here), another training institute, Edvancer Eduventures has started offering a range of analytics courses in India. Currently they are running a “Certified Business Analytics Professional’ (CBAP) and will be launching 2 more courses R and Big Data in coming months.

We had an opportunity to talk with the CEO, Aatash Shah and here are some excerpts from the talk:
AV: Who is the course aimed at? 
Aatash: This course is aimed at both graduate freshers / final year students and working professionals. It is targeted at those looking to start a career in analytics either fresh from college or making a lateral shift.


AV: How does the course differentiate compared to other courses available? 
Aatash:  We do not believe in making our students learn everything under the sun in analytics if they are unlikely to implement those skills in their career for a long time. Instead we believe in training when it is required and hence our idea is to have modular courses which can be taken as the need arises.
Secondly, our course is much more application oriented, designed towards solving business problems using analytics. We teach with a hands-on approach from day one with over 15 real world case studies which will explain each and every concept. We also have a 20 hour guided simulation of an actual analytics project which will bring together all they have learnt in the course in an effort to make their learning as practical as possible.
We provide students life-time access to the course material on our learning management system and free access to the SAS language tool for 90 days.
The other differentiating aspect is the delivery mode. We have three modes of delivery, classroom (in Mumbai for now), online self-paced and soon we shall have the online live instructor led mode too. We also have a comprehensive learning management system with a discussion forum which can be accessed from any internet enabled device allowing any-time, any-where learning.


AV: Is there a way I can test the interface or content? 
Ans: Sure, you can visit www.edvancer.in/course/cbap/ where you can view the curriculum and the preview of the content which has snippets of the various modules.


AV: Is there any placement support which is provided or in case you have any work opportunities / live projects or cases / internships? 
Ans: We do offer placement support and also have an employer’s page on the website from where employers can access CVs of our students. Though having just started off there are no statistics regarding the same.


AV: What is the profile of trainers and promoters?
Ans: About the trainers- We have three trainers presently, all professionals from the analytics industry. One of them is currently the head of a reputed MNC analytics consulting firm in Mumbai and he loves teaching. Another trainer has 8 years of experience in analytics in the banking and retail industry while the third has over 3 years of experience in analytics.
About the promoters:
Aatash Shah, Co-Founder and CEO
Aatash has over 6 years of experience in the investment banking industry. He is a post-graduate in management from IIM Lucknow and is also a BE in Electronics Engineering from Mumbai University.
Lalit Sachan, Co-founder
Lalit has over 2.5 years of experience in business analytics and is extremely passionate about decision making through data analytics. He is a post-graduate from Indian Statistical Institute, Kolkata and a B.Tech from IIT Kanpur.
You can go through the details of the course content and can register for the same following this link. Alternately, if you have any questions for me or Aatash, please feel free to mention them here and we’ll try and answer them.
If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our facebook page
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Segmentation of customers for effective implementation of analytical projects","Kunal Jain",2013-10-05 06:28:00,7,"
							
										
						According to a survey conducted by Bloomberg in 2011 (on companies exceeding $100 Mn in revenues), 97% of these companies have embraced Analytics in some form. However, only one in four organizations believes its use of business analytics has been “very effective” in helping to make decisions.
This gap in investment and desired results might be higher in developing markets like India. While, there are bunch of reasons which contribute to this gap, one of the key reason contributing to this gap is effective customer management.
There are various kinds of customers we interact with, while working on analytical projects. They possess different understanding of analytics and have varied expectations from it. Keeping a similar engagement with these customers is like preparing for a disaster.
Over last few years, I have developed a segmentation which I have found very useful while interacting with various customers. This segmentation not only helps you on how to best interact with various customers, but also lays maps these customers on a journey to convert them into proponents of Analytics.
Segmentation of Analytics customers:
Following is the segmentation I use for engaging with customers (In case you want to read more on Segmentation & how to create one, click here):

The customers are divided in 4 categories basis their understanding / maturity about Analytics and the potential they see in Analytics. Following is a brief description of each of this dimension:
Analytical maturity of the audience: This dimension represents customer’s understanding about how Analytics can contribute to decision making. A customer with high maturity would not only understand what Analytics can deliver, but also what it cannot.
Some examples of aspects which I expect an analytically mature customer to understand are:
What is predictive modeling (from business point of view) and how can it help a business?
Does the customer understand what efforts will it take to implement a project?
There is a minimum sample size required for performing any analysis and pulling out recommendations.
How open is the customer to the idea of testing?
Does the customer understand the difference between Analysis and reporting?
Expectation from Analytics: By expectation, I mean the point of view a person carries towards analytical projects and thinking. A few questions which can help us map a customer on this dimension are:
What kind of benefits does the customer expect from Analytics? How critical are they to his function?
What kind of time frame does the customer have in mind for getting these benefits?
Is the customer looking forward to project implementation or he is implementing it under some pressure?
Once you map your customers on these 2 dimensions, they can fall in any of these 4 categories:
 
Analytics Proponents:
These are the group of customers who understand what Analytics can deliver and would be ready to advocate the practice in the Organization. These are the people who will come to you with specific and clearly defined problems, would be ready to collaborate / spend time and finally implement the solution. If you have any customer who falls under this category, you should make sure that you deliver / over-deliver on these projects. Hopefully, they will help you increase Analytics penetration in future through their influence.

Challengers:
Challengers are people who understand Analytics, but are not convinced that it can deliver on the projects in Organization. The best way to start engagement with them is to try and have an open discussion with them and understand their concerns. Being aware of these concerns would be the first step in alleviating them.
There are times when it would not be possible to have this discussion and even if it happens, it may not be entirely honest (especially if it is early in the engagement). In that scenario, the best way is to take up small but impactful projects and deliver them. Once the customers see some delivery, they would start their journey to become a proponent.

Citizens of wonderland:
These are often amusing group of customers. They don’t understand nuances of Analytics completely, but carry high expectations from Analytics. While in short term, these customers are easy to interact, the difference in expectation and maturity will cause difficulties in long term.
Some of the situations you will find yourself with these customers are:
How can Highest propensity leads of a campaign not convert? (vs. Why is the conversion rate of Highest propensity leads lower than expected?)
We have arrived at these recommendations after a lot of analysis and discussions, lets implement it on entire population (vs. we should test our recommendation to size the benefit and understand whether the recommendations work or not?)

How to interact with these customers?
Educating them about Analytics might be the best way for the long term. However, a balance needs to be maintained so that they remain excited about the analytics journey and become more aware of their understanding gaps.
If the customer in this category is really critical to success of your function, you can check with him if he is open to idea of joining a short Executive Programme about Analytics (2 – 3 days is good). If he is not a critical customer, you can ask one of your proponents to educate him over time through regular interactions.
Over time by bringing more education to these customers, you should aim to convert them to proponents.

Skeptics:
These are the most difficult set of customers. Converting them into proponents is a relatively long journey. If you have projects (of similar / more potential impact) from customers in any of the three segments mentioned above, you should focus on them first. The idea is to create enough proof of concept to improve the expectation customer has from Analytics. Another way this can be done is by talking them through case studies from same industry.

Analytics implementation Strategy:

Once you have identified your customers to this grid, I would suggest following in order of priority:
Start by making sure that you meet all the commitments / expectations for your proponents.
Next, try and convert challengers & Citizens of wonderland in proponents of Analytics
Finally, start working on expectation skeptics might have from Analytics.
Following are some interesting set of metrics which can be tracked to measure success of Analytics function:
Percentage of customers who are proponents provides a view on success of Analytics journey in an Organization.
% of customers who are either proponent or Challenger can give idea of Analytics penetration in an Organization.
% of skeptics in an Organization / function can tell you the readiness of Organization to adapt Analytics. If the number is high, the readiness is low.

Some additional thoughts:
A few customers might test you for capabilities first by giving small and less impactful piece of Analysis. They may look like skeptic at the outset, while they might actually be Challengers.
Converting a customer from one segment to another takes time. Typically, it would take 3 – 6 months for a customer to move from one box to another.
Hopefully, you will find this segmentation useful to create a strategy and roadmap for Analytics journey in your Organization. Do you think this segmentation will be helpful for driving Analytics in your Organization? Please feel free to share your thoughts in comments below.
If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our facebook page
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Five habits of highly successful analysts","Kunal Jain",2013-09-28 22:49:00,7,"
							
										
						I have interacted with various successful analysts over last 7 years. During these interactions, I found out some common habits in them. After observing these habits, I have tried ways to inculcate these habits myself. Some of these habits were also emphasized by my mentors, while the others were learnt the hard way!
This article lays down 5 habits I have observed in highly successful analysts and provides some of the ways in which I practice these habits. On one hand, I hope that some one should have made me aware of these habits at start of my career and the long lasting impact these could have. On the other hand, these habits have helped me immensely and I hope they will do the same for any one adapting them:
Habits of successful analysts
Habit 1: Keep high bar on project delivery. Walk that extra mile and deliver your best
Quality thinking differentiates a high performing analyst from a low performer. Successful analysts provide enough quality “brain time” to any project they deliver. By brain time, I mean distraction-free time devoted to analytical problem solving. This is the time when you strive for going beyond what is expected. This is the time when an analyst asks some of these questions to himself:
Is there a better way to structure this problem? Will that make the solution better or more intuitive for business?
Is there a better way to present / summarize the findings of the project? How can I visualize the outcomes in best possible manner?
Instead of simply highlighting the insights from a project, can I use these insights and chalk out business actionables? Can I size the impact from these actionables and tell them to business upfront?
Is there any aspect of analysis I might have ignored?
Is there any implicit assumption I have made which is impacting the result?
Remember, the value of analytics is recognized through the value it generates for the business and the amount of time spent asking these questions will have direct impact on the business value created.
There have been times when I have changed the presentation flow, performed additional analysis, verified and re-verified all the numbers / insights till the night before project presentation. All of that is done to make sure the project creates the impact it deserves. And, nothing beats the feeling you go through after creating that impact.
So, don’t leave any stone un-turned and make sure there are no gaps in your thinking on every project you work on.
Habit 2: Segment, till you can!
Successful analysts never work on averages. Every time they see an average, they think if there is an underlying segmentation at work, which could explain things better? By not segmenting the average, there is value left on table. Successful analysts never do that.
This article contains various examples and the way to create a segmentation.
Habit 3: Triangulate numbers, perform back of the envelope calculations and think what do they mean for business
As an analyst, you deal with numbers day in and day out. You need to pick out that one cell in which the formula is wrong from a file containing thousands (if not millions) of formulas. The only way you can do it is by triangulating numbers and by making sense of what they mean for business.
While this might sound obvious, you will be surprised to see the number of times this is overlooked. While triangulating numbers deserves a post in itself, I’ll briefly mention some of the questions I ask to triangulate numbers:
Ask yourself, can I reach this number through a different framework / calculation? Do the numbers tie up or they are different by a magnitude?
Are there process dependencies which can give you a sense of numbers? Can you issue 2000 credit cards every month, if you only get ~1800 applications every month?
What do these number mean for business? Do they tie in with the infrastructure and resources business has?
Triangulation is like any other skill, it will look difficult to start with. But the more you practice, the better you become at it.
Habit 4: Test out your hypothesis (even if you think they make complete business sense)
There are times when you tend to overlook the need for testing. Just adding a live chat functionality to your website? Sounds like a good thing to do with no down side. Test it out and you will know. The customers might not like it! Here is another example:
One of the leading travel portal in India saw this in their data: 90%+ flights booked have departing location same as city from which tickets are being booked (determined by the I.P. address). They thought of making this location pre-populated (obviously with an option to change). This sounds like a nice idea which would help provide a better customer experience. Thankfully, they tested it out. Booking conversion dropped by double digit percentage with in weeks of making this change. Possible reason: The customers are used to filling To and From location. Removing one of them adds to the confusion.
So, next time if you are implementing results from any analysis or hypothesis, test it out!
Habit 5: Learn something about analytics everyday
While this habit will not bear immediate results like other habits mentioned above, you will benefit the most from this in long run. Analytics is a dynamic and evolving field. A new tool / technology / update arrives almost every 2 – 3 months. Being up to date with latest updates in industry helps not only stay on top of it, but creates a huge gap from analysts who don’t stay updated.
Some of the topics I enjoy reading whenever I get time are:
What are the latest developments in Big data?
How to analyze unstructured data from Social media? How can we make visualizing this better?
What are the statistical concepts behind the algorithms used by various tools?
How can you design and analyze a design of experiments?
The list is endless.
Start reading and you are bound to run out of time!
Additional Habit: One additional habit worth mentioning is “Being customer centric”. Hearing what the customers are saying and keeping that in mind is a trait of every successful professional. The same applies to an analyst. Since it is not an analytics specific habit, I have not mentioned it in the list.
While the list of habits could have been lot longer, I have deliberately restricted it to 5. If you think any other habit, please add them in comments.
If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our facebook page
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Taking a new job in Analytics? Ask these 5 questions first!","Kunal Jain",2013-09-21 22:04:00,7,"
							
										
						Taking up the right job can accelerate your career.
On the other hand, getting into a wrong job can de-rail you for couple of years. The impact of these decisions can be far higher than what it might look getting into the job. Please note, by right job, I simply mean it is right fit with your goals (of making a career in Analytics industry). Similarly wrong job means bad fit with your goals.
Let me take an example to explain what I am saying:
While I was lucky to get into Capital One from campus placements, many of my friends were not that lucky. One such friend was Mehul (name changed). Since he could not get into Capital One, he took up job from another MNC with designation as Analyst.
A few days in his job and the truth dawned up on him. He realized that his role was part of IT team. He was expected to write queries for business requirements, check system maintenance and perform pre-defined reporting for business. The word analysis was completely missing in the way he described his job! He still decided to stay for at least 12 months before making a switch.
After those 12 months, he has made several attempts to enter Analytics industry, but haven’t got a break through till date. Today companies mention lack of hands on Analytics experience as the major reason for not  considering him! Sad! but this is a reality check for people who are taking up jobs in hope of becoming an analytics rockstar in their career.

In order to make sure that people make right choices with the roles, I have come up with a list of questions, which you should ask to your employer before taking up a job in Analytics. The aim of these questions is to make sure you know what you are getting into. I assume that you would use this in addition to the research you are doing any way (rather than in isolation). For example, I have not included questions asking for “Stability / History of the company”. I am assuming, you will do that at your end.
I think that using these questions will not only help you make the right choice, it will also tell the employer that you are dead serious about the role and industry! So here is the list of questions:
1. What is expected out of the role – Analysis or reporting?
You need to find out the expectation from the role. There can be various levels of expectation from a role (in order of increasing value addition):
Creating business reports (pre- determined format) or refreshing dashboards with right data.
Creating business report, looking at trends and providing a commentary (Note: I mention looking at trends and not Analyzing)
Taking up a project (business problem), creating framework to analyze data, collate various hypothesis, mining data for insights and giving them to business
Along with what is mentioned in previous point, owning the project implementation.
All these positions will be called Analysts! However, the learning in the last role would be far higher than the first one.
So, how do you find the expectation from the role?
Your first step is to read the Job description thoroughly and map the job on Business Analytics spectrum. If you are still not clear, asking these questions from the interviewer can help:
Please describe typical day for this role? What kind of problems will I work on day to day basis?
What is the impact this role can create in the company?
Who are the main customers (internal / external) for the role / team?
These should leave you with enough details to map the role correctly. If you are still not clear, next point might help.
2. Ask for a recent project done be person in the role:
You should ask the interviewer to describe work done by the person in this role recently. This should give you a good flavor of the work which is expected out of the person. Following are the aspects you should specifically understand:
How critical was the problem to the business? Was it creation of a report? Creation of a monitoring platform? or Segmenting the entire customer base to create strategy for the company?
What kind of tools and techniques were used during the project? What was required – a complex data modeling? Clustering? Predictive modeling?
What were the quality of insights that came out of the project? How much impact did the project create? Was the benefit monitored?
If it was a strategic project, did the company test it before implementing it completely? Good companies will test out the insights before implementing them.
 
3. How big is the analyst community? Can you talk to them? Do you know some one?
Answer to this question would help you understand following:
How much peer to peer learning can happen? Usually analytics is best learned through brainstorming with other analysts on the job.
The perspective your customer and leadership would have towards analytics. The bigger the community, higher would be the mind-space in Leadership team
If you can connect to analysts in the company, that would help immensely.
4. Which function does the position / team report to?
While there might be exceptions to this, but the reporting function usually influences the kind of projects you work on. If the team reports to IT, the nature of projects would focus more on tools and dashboards (rather than customer insights). If it reports to Operations, you might get higher share of Operations Analytics. Typically, direct reporting of Analytics team to the CEO (or through Center of Excellence) brings the best mix of the problems.
5. If it is replacement hiring, why did the last person leave?
This is a tricky question and you have to judge the right way and moment to ask this. An informal lunch / coffee post interview might be more suited than during the interview. But, asking this question might give you some good insights:
Is the manager / interviewer comfortable while talking about it? If he is, he is likely to be comfortable having open and honest discussions later on.
Does the reason sound reasonable? How did the Organization react to it? If the person left for higher education or starting his own venture, did he / she get the required support? If the answer does not convince you, do not ask further, but try and understand it later on.
5. If it is a new team / setup, what is the vision / goals for the team?
For new setups, it is always worth understanding the vision / goals for the team. If the person is not clear about them or is unable to articulate it, consider it as Red flag. If the leader can’t articulate / excite a new team member about the team’s vision, chances that the team would be clear / excited about their role and responsibilities are bleak.
Asking these questions should give you a good read on fit between your expectation and interviewers expectation. You should only join the potential employer when you get satisfactory answers to at least 4 of these questions (with full match on the first question.)
If you have any thoughts on these questions, or use any other questions to judge new roles, please add them in comments below.
If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our facebook page.
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Must read books on data visualization","Kunal Jain",2013-09-16 23:13:00,2,"
							
										
						It is not a co-incidence that all highly successful analyst have excellent data visualization skills. As a matter of fact, I think data visualization and creative story telling differentiates best of analysts from good analysts.
Be it design of a dashboard or a presentation after building an earth-shattering predictive model, visualization ends up being the face of all the hard work you have put in. Hence, it makes sense to spend time thinking about what is the best way of visualizing the data or insights from the data.
Sadly, not many analysts spend time on improving and thinking about visualization.
Look at it from this perspective, we sweat ourselves to get that extra percent of lift through our models or to make sure all the data flowing into the dashboards is right to the second and third decimal place. But, we usually spend a fraction of that time thinking about how this outcome would be presented and the impact it can create.
In this article, I am listing down the books, which have helped me improve immensely in data visualization. Further, some of these are resources for lifetime as I still continue to refer them, when I am stressing my grey cells over how to create that extra impact through my work.

For beginners:
1. The Visual Display of Quantitative Information by Edward R. Tufte:If there is one book you should definitely read on visualization, it is this book! First published in 1983, a classic book on charts, tables and various practices in design of data graphics. The book contains 250 illustrations of the best (and a few of the worst) statistical graphics, with detailed analysis of how to display data for precise, effective, quick analysis. Some of the well-known concepts like data-ink ratio and sparklines were outlined by Tufte in this book. The book also mentions various sources of graphical deception. A definite must read for every analyst!
2. Visualize This: The FlowingData Guide to Design, Visualization, and Statistics by Nathan Yau:This book from the creator of flowingdata.com provides approaches to tell stories with data and offers step-by-step tutorials and practical design tips for creating statistical graphics, geographical maps, and information design. It also provides details on tools that can be used to visualize data-native graphics for the Web and tools to design graphics for print.
3. The Wall Street Journal Guide to Information Graphics: The Dos & Don’ts of Presenting Data, Facts, & Figures by Dona M. Wong: A super practical guide to effective communication through graphs and charts. Concise, well written and easy to navigate, this book is a must read for people who have just started making presentations
4. Show me the Numbers: Designing Tables and Graphs to Enlighten by Stephen Few:A clear, concise and comprehensive book on how to design tables and graphs. It gives you the tools to create effective tables and charts, and the understanding on how and why these tools work. The book has a lot of practical advice which can be applied with Excel and hence can be put in practice straight away.
5. Now You See It: Simple Visualization Techniques for Quantitative Analysis by Stephen Few: This book starts where the previous one ends. Few talks about principles of visualization and their applications. Again, most of the learnings can be applied on Excel.
6. Information Dashboard Design: The Effective Visual Communication of Data by Stephen Few: A must read book for anyone who designs and creates dashboards for a living or wants to make a career in BI. This book will teach you the visual design skills you need to create dashboards that communicate clearly, rapidly, and compellingly. Key take-aways from the book are common dashboard design mistakes & ways to minimize distractions and confusions.
For advanced readers:
7. Envisioning Information by Edward R. Tufte:  If you love the subject of data visualization, you will love this book. Tufte takes on a high-dimensional complex data and plots them on maps, charts, scientific presentations and courtroom exhibits. Topics in the book include layering and separation, micro/macro design, applying color and witing narratives. Some of the examples include sources as diverse as Gallileo’s observations of Saturn, a 3D map of a Japanese shrine, a visual “proof” of Pythagoras’ theorem, color studies by the artist Joseph Albers, and a New York train schedule.
8. Visual Explanations: Images and Quantities, Evidence and Narrative by Edward R. Tufte: Tufte starts discussing more advanced topics on arrangement of images, words and numbers in space and time. Visual Explanations centers on dynamic data–information that changes over time. This book includes an engaging narrative narrative of the 1854 Cholera Epidemic and a study on the Challenger space-shuttle tragedy.
Special mentions: Cartographies of time: A history of the timeline by Daniel Rosenberg & Semiology of Graphics: Diagrams, networks, maps by Jaques Bertin – Both these books are classic and give you a good perspective on how the field has evolved. You might still find some nuggets of gold here!
 
In my list of books to be read: Visual Thinking by Colin Ware. I have read some great reviews about the book, but have not been able to read it till now. It’s on my reading list.
If you have not read these books, please find them and read! On the other hand, if you have any other books which are not there in the list, please suggest them in the comments below.
If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our facebook page.
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"How to create a High performance Analytics team?","Kunal Jain",2013-09-12 23:08:00,5,"
							
										
						Lets assume that you are CEO of a MNC with operations across the globe!
Over last few years, you have heard a lot of buzz around analytics and about a lot of companies benefiting from analytics. On the other hand, you also come across companies, which invested big time in Analytics, but the unit continues to remain a cost center even after years of presence.
While the idea of creating an Analytics team sounds like an obvious thing for your company, you wonder:
Why companies falter in creating value through Analytics? 
It’s a question which many leaders across the globe face on a regular basis. Of late, similar debates and discussions are happening with regards to starting a big data practice. The aim of this post is to bring out some of the best practices analytics leaders follow to create a high performing Analytics team.

Keep a high bar on recruitment:
If there is only one advice, you would want to follow, this is the advice. If you get this right, everything ahead becomes easy to implement. If you fail at this, you will never be able to create a High performance team, no matter how much you excel in remaining tips.
So, how do you make sure that you are recruiting the right profile? Judge the analyst on the following skills:
Structured thinking
Business understanding and problem solving
Attention to details
Ability to triangulate numbers & do back of the envelope calculations
Communication skills – Ability to tell stories based on numbers
You can read further on how to judge on these skills here. In addition to these skills, check how curious the person is? How many questions does he / she asks. Remember, you can teach technical skills. You can manage performance. What you can’t do is force curiosity or passion.
Most of the highly analytical companies hold multiple rounds of case studies, role plays and interactions to judge the analysts on these traits. While these might sound easy to implement, they might not be. It means saying no to any back door entry. It also means, letting go of an analyst not clearing the thresholds, even though the work might suffer.
Provide right tools, trainings and resources:
Once you have the right people on-board, you should provide them with best in class training and resources. These training could be delivered externally (e.g. Training from SAS institute) or internally (e.g. Structured thinking and writing training to be provided by a seasoned analyst).
Some of the highly recommended training with in first 3 months of an analyst joining the team are:
Technical tool training: The analyst would be working on these tools day in and day out. A training upfront can build the momentum straight away. The best way to deliver these is by providing a basic module followed by a period of application. This can then be followed by a more advanced module.
Structured thinking and writing: This is another must do training for an analyst. A good training here can increase the productivity of an analyst many fold. You can find some more tips on structured thinking here. Some of the topics which can be covered in this training are “The Pyramid Principle”, Importance of hypothesis driven analysis. Common frameworks for presentation and story-telling.
Basic Statistics: A basic knowledge of statistics can go a long way in understanding basics of Business Analytics. Similar to technical training, advanced statistical concepts can be left for later.
Subject matter expertise for business understanding
 
Create a culture and community that fosters analytical thinking:
Now that you have trained your potential analysts, you need to provide them with a culture that accentuates analytical thinking. Following are some of the best practices, I have observed in analytical leaders:
Provide business ownership to the analysts: By doing so, you make sure that Analysts start working closely with business and there is high alignment with the customers.
Identify high value problems and challenge the analysts to solve them: Lets accept it! Analytical resources are scarce and expensive. Hence, it is very critical that you engage them in really high value problems and to continuously challenge their thinking to bring out the best in them.
Provide opportunity to step up for bigger roles: Best Organizations groom talent internally rather than hiring externally. So every time you have a step up opportunity for an analyst in your team, provide it to him. Handhold him in initial days and provide him regular feedback on how he is performing (mentioned in detail later).
Create a community – Have knowledge sharing sessions with in team, create forums to share best practices, perform team activities, discuss and solve case studies. Creating an active community of analysts is the solution to manage analyst attrition. The more closely knit the community is, lower are his chances of attrition.
Encourage healthy debate, brainstorming sessions: Brainstorming and whiteboard sessions bring out the best of ideas among analysts. They not only improve structured thinking, but also bring out some out of box thinking to the table.
Some other good practices include having a flat hierarchy, Imbibing testing culture within team
 
Manage the performance:
Once you have a thriving community in place, all you need to do it manage the analysts to make sure they are workinng on most important and challenging problems on one hand and their is healthy work load to bring out the best in them. Following are some of the must do actions to manage the analysts:
Have clearly defined and agreed objectives and expectations.
Continue to challange the analysts with most important & value generating problems.
Provide regular two-way constructive feedback. Additionally, this should be done at end of every project.
Continuous monitoring of benefits through implemented projects.
Provide development and training time to analysts on a continuous basis
Try and keep ad hoc work and multiple projects out of system. Analysts should focus at least 50% of their time on a single project.
Provide break for analysts to unwind and come back refreshed for working on high value problems.
If you are setting up an analytics team, this article provides you with a list of best practices. If you already have an analytics team, just check which of these practices are missing in your Organization and how you can implement them. Additionally, if you have any thoughts or practices which I might have missed, please add them in comments.
If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our facebook page.
 
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Upcoming trends in data visualization","Kunal Jain",2013-09-07 17:45:00,7,"
							
										
						Recently, we were blessed with a baby girl. Among a lot of other things, one thing which keeps mesmerizing me is the continuous change happening in the new born. In 10 days, she looks very different compared to how she looked when she was born. I am told that she will change further and I should expect changes continuously!

While these changes look apparent to a person looking at the baby after a break (say 5 – 10 days). At times, people taking care of baby day in and day out fail to notice them. On a similar note, analysts working in analytics industry day in and day out fail to notice some of the changes visible to an outsider. Thankfully, the rate of change in analytics industry is months and not days.
The aim of this post is to highlight some of the key changes happening in the way we visualize data and provide some resources to use so that you can start capitalizing on these changes. While visualization itself is a topic which deserves multiple books and blogs, the purpose of this post is to highlight some key changes happening and provide ways to prepare for them. But before we get into these changes, let’s quickly understand what’s driving these changes?
Drivers for changes in data visualization:
There are 3 key drivers driving changes in visualization:
Volume of data: With the data to be analyzed becoming larger every second, right visualization has become critical day by day. Gone are the days when you could scroll through the data and get a fair idea about data quality and values. While this might be scary for some analysts, increased data has created opportunities to slice, dice and represent the data in ways, which were not possible for a common analyst in past.
Variety of data: Along with the increase in volume, there are new kind of datasets which are becoming more and more mainstream. The need to analyze user comments, sentiments, customer calls and various unstructured data has resulted in use of new kind of visualizations. Use of graph databases and visualization to represent unstructured data is an example of how things are changing because of increased variety.
Technological advancements: Improvement in technology continues to trigger analytics industry in ways never thought of before. There are 2 main ways in which this is impacting visualization:
Sensor, sensor, everywhere: With cost of sensors coming down significantly and advent of mobile devices, there is huge amount of data available, which was not available before. Smartphones can help geo tag every activity you perform, be it search or clicking photos. These sensors can not only provide your location, but can also reveal great amount of insights in customer behaviour. One of the Motor Insurance company has started giving discounts to people who drive more safely (as measured by sensors in your car / smartphone).
Increase in computational power: When I was working on my Masters thesis (8 years back), creating an unstructured grid to simulate aerodynamic flow over objects could take days. Today the same computation can be done within minutes. Similarly, visualizations which took hours in past can now be done in matter of seconds. This has enabled creating visualization which might be computationally intense. Creating visualizations to track down chains of social media discussions, their impact on website traffic and eventually customer behaviour was simply unimaginable in past due to lack of computational power.
 

Upcoming trends in visualization:
Increased use of geo-spatial visualization and analysis: Showing geographical spread on a map is one of the simplest, yet most effective way of representing information. It can be used in a simple application like looking at website traffic (example of chart below) to more advanced applications like mapping of bank branches and demographics of customers to make sure there is a right fit. Usage of these charts has increased significantly in last couple of years due to availability of better tools and increasing amount of data available with latitude and longitude.

Use of maps (including heat maps) can be achieved through variety of tools and libraries. Some of them are mentioned below:
Google Maps – Simples form of charts in form of Geo charts
Qlikview / SAS and a list of other tools through add ins and extensions
Additional mapping softwares / libraries like Modest Maps, Leaflet, Polymaps, CartoDB, Kartograph, D3.js
Graph representations are in: Graph representations can be used to represent a lot of stories intuitively. They represent the way we think about our network. Following is an example of one of the visualization mentioned in HBR blogs. The visualization tells impact of social media discussions on website traffic of New York Times. Tweets and re-tweets are shown as lines and dots, while the click through volume on each article is shown in black below. As the spike happens while no twitter discussions were active, the traffic is likely coming from some other source.

In order to create graphs for some of the visualization, you can use libraries like D3, specialized tools like Gephi, Neo4j, or any of the advanced tool like R, SAS etc.
Use of infographics (specially interactive) for narrating stories: While the complexity of performing advanced analytics has increased many fold, outcomes of these complex analysis are best explained through simple stories. Powerpoint has been a preferred source for this in past. However, that’s changing rapidly. Increasingly, people are using infographics to narrate stories. While I don’t see infographics replacing Powerpoint in near future, they provide an interesting way to narrate an analysis. Following is an example of infographic from ibtimes. 
There are various tools to create these infographics like visual.ly, ana.gram etc. but I would personally recommend visual.ly because of its ease.
Increased use of data mashups for exploratory analysis: Data mashups refer to tools which can import and join data on the fly. They don’t rely on the middle layers of an ETL system to import and join data before it can be used. Further, these tools have enhanced visualization capabilities to perform data exploration and discovery before investing time in predictive modelling. A lot of well-known vendors have entered this space as these tools can cut down exploration stage to a fraction of what was required in traditional tools. Some examples of these tools include SAS Visual Analysis, Qlikview etc. The picture below shows a few screenshots of SAS Visual Analytics.

Use of timeline to show evolution and changes: While there is nothing new in this visualization and it can be created even in a simple spread sheet, its application and usage has increased suddenly. Ability to show interactive visuals with better graphics has increased appeal of timelines significantly. Following is an example of visualization through time line.
In order to create timeline, you can use tools which create infographics or specialist tool like Timeline
 Creating visualization with mobile devices in mind: With increase in smartphones and smart devices, a lot of customers view these visualizations on the go. Further, penetration of mobile devices is going to increase further. So, whenever you make a new dashboard / visualization, just think how will it look on various mobiles and tablets? There are various tools, which have capability to create apps or dashboards customized for mobile devices (e.g. SAS Visual Analytics, Qlikview etc.)
I hope this article gives you a good start to utilize some of these upcoming visualizations. If you are aware of any other trends or tools / libraries to create these visualizations, please add them in comments.below.
If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our facebook page.
 
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"A small break to celebrate!","Kunal Jain",2013-08-27 22:24:00,3,"
							
										
						Let me not give it away simply….
 
Assuming that today’s date is T and T – 1 represents yesterday.
 
Further, If I represent the hours in which I sleep by 0 and the ones in which I am awake by 1, this is what has happened to my sleeping pattern lately:

 
Any guess, why this happened?
Well, I can’t hold it any further now…
We have been blessed with a baby girl and it feels great to be a father!
I am taking a small break to celebrate the occasion, but I promise to be back soon!
Till then, enjoy learning Analytics!
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Common myths about a career in Business Analytics: Busted!","Kunal Jain",2013-08-23 05:29:00,6,"
							
										
						Some time back, I wrote an article on “How to start a career in Business Analytics?“. The article was well received by people who want to enter Business Analytics. It is still one of the most popular articles on Analytics Vidhya. In response to this article, I received a lot of queries about career in Analytics. While some of them were good queries, some of them were recurring myths.
Hence, I decided to do a follow up article. Not only so, in order to debunk these myths entirely, I decided to publish the articles in all relevant forums.

Here are the myths I received through queries / comments / emails and my take on them:
You need to be an engineer to start a career in Business Analytics: The truth is that you don’t. All you need is ability to think structurally and comfort with number crunching. As long as you can put structure to unstructured problems and perform back of the envelope calculations, you are as good as any analyst out there.
Having said that, companies prefer people from quantitative background as they are expected to be better with numbers. By quantitative background, I mean people from any of these disciplines: Engineering, Economics, Maths, Statistics, Physics or MBAs with graduation in these fields.
Analytics is about working with large datasets / Companies work with big data day in and day out: This rosy picture is far from reality in most of the Organizations. Experts estimate penetration of big data to be in low single digit percentage among Organizations. Most of the time analytics team work on specific problems, which may or may not involve large datasets. The requirement of the role is to be able to put structure across unstructured problems and be able to use numbers to understand business and the changes required in strategy.
You need to be a programmer: I was a good C++ programmer when I started my career in Analytics. Sadly, none of those skills have been utilized in last 7 years and might not be utilized in future. You only need to learn programming for the tool you use for your analysis (e.g. SAS, R, SQL etc.), but you don’t need to be a good programmer before hand to learn these. Also, most of these tools have a Graphical User Interface (GUI), which you can start using with out knowing programming.
Learning Analytics is all about learning a tool (SAS / SPSS / other tool): A tool is just a tool to perform Analysis. It can not perform analysis on its own. You need to understand the fundamentals required for performing analysis like:
What are the things you need to keep in mind while performing Regression?
What can you infer from the coefficients and outcome of t-tests?
How do you prove or dis-prove a business hypothesis?

Once you understand these, applying them through any tool can help you start your journey of Analytics.
Its difficult to find a job – In fact it is the other way around. Analytics industry is struggling with attrition and shortage of talent. According to the McKinsey Global Institute (In a May 2011 report): “By 2018, the United States alone could face a shortage of 140,000 to 190,000 people with deep analytical skills as well as 1.5 million managers and analysts with the know-how to use the analysis of big data to make effective decisions.” If you have the right skills, you will be highly sought after (at least in the current market conditions).
Are there any other myths that you are aware of? In case you are, or are unsure whether it is a myth or a fact, please add it below.
If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our facebook page
 
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Importance of Segmentation and how to create one?","Kunal Jain",2013-08-17 23:59:00,7,"
							
										
						Average is one of the biggest enemy of analysts!
Why do I say so?
The amount of reporting which happens on averages is astonishingly high. Sadly, working with averages never reveals actionable insights until broken down in segments. Let’s go through a typical example to demonstrate what I am saying:
Let us assume that you head Customer Management division of Credit cards for a bank. Two metric of immense importance to you are:
Monthly spend people do on their credit cards – Indicates usage of credit card for customers
How much of their credit limit are customers utilizing? – Increasing trend might mean increasing risk or higher satisfaction. Decreasing trend might mean the other way.
You look at the following report and feel that everything is under control. You reach a quick conclusion that there is no problem in continuing your engagement as they run today:

In a practical scenario, these metrics would be an aggregate across various cards, but for simplicity, lets say that there are 2 kinds of cards:
Card A: Aimed towards people with good credit history. They will tend to have higher credit limits, lower risk and hence lower lending interest rates.
Card X: Aimed towards people new to Credit or people with bad Credit history. These will have lower Credit limits, higher risk and hence higher lending interest rates.
Again, just to simplify, lets assume that you have an even mix. The minute you split the metrics by segments, a different story emerges:

As you can see, what is actually happening is very different from what you would interpret from Average metrics. Actually, usage of cards with your low risk customers is on decline where as on the high risk customers is on increase – might be a scary situation!

What is segmentation?
Segmentation is a process of breaking a group of entities (Parent group) into multiple groups of entities (Child group) so that entities in child group have higher homogeneity with in its entities.
Following is a simple example of customer segmentation for a bank basis their age:

In this case you take a single group (customers of bank) and segment them in 5 child groups (basis their age). Incorporating this segmentation in your analysis can then drive various insights and ultimately actions in interest of your business like:
Are customers buying right kind of products?
What are the opportunities to sell an additional product to the customer? If the person became a customer as “Young Professional“, has the need changed as he is now a “Married Professional“
What kind of marketing channels would appeal to which kind of customers? How much to spend in each channel?
General guideline to create the child groups is that they should be “Heterogeneous with other groups, but homogeneous with in group“.
How to create a segmentation?
While there are multiple techniques to create a segmentation, the focus of this post is not on technical knowledge. I’ll layout the process used to create a segmentation and keep the technical details for a later point. This will enable you to create and implement a segmentation, even if it is not the best technically. You can obviously learn more details about the techniques and apply them in conjunction with the process mentioned here:
Step 1: Define the purpose of the segmentation. How do you want to use this segmentation? Is it for new customer acquisition? Managing a portfolio of existing customers? or Reducing credit exposure to reduce charge-offs? Every segmentation is created for a purpose. Until this purpose is clear, you will not be able to create a good segmentation.
Step 2: Identify the most critical parameters (variables) which influence the purpose of the segmentation. List them in order of their importance. Now, there are multiple statistical techniques like Clustering, Decision tree which help you do this. If you don’t know these, use your business knowledge and understanding to come out with the list. For example, if you want to create a segmentation of products and focus on products which are most profitable, most critical parameters would be Cost and Revenue. If the problem is related to identifying best talent, the variables would be skill and motivation.
Step 3: Once these variables are identified, you need to identify the granularity and threshold for creating segments. Again, these can come from the technique developed, but business knowledge could be deployed equally well. As a general guidance, you should have 2 – 3 levels for each important variable identified. However, it depends on complexity of problem, ability of your systems and openness of your business to adapt a segmentation. Some of the simple ways to decide threshold could be:
High / Medium / Low with numerical thresholds
0 / 1 for binary output
Vintage / Age of customers

Step 4: Assign customers to each of the cells and see if there is a fair distribution. If not, tweak the thresholds or variables. Perform step 2, 3 and 4 iteratively till you create a fair distribution.
Step 5: Include this segmentation in your analysis and analyze at segment level (and not at macro level!)

Example of creating Segmentation:
Let us say that you want to create HR strategy to identify which employees should be engaged in what manner so that you are able to reduce attrition and offer what the employee actually wants.
Define purpose – Already mentioned in the statement above
Identify critical parameters – Some of the variables which come up in mind are skill, motivation, vintage, department, education etc. Let us say that basis past experience, we know that skill and motivation are most important parameters. Also, for sake of simplicity we just select 2 variables. Taking additional variables will increase the complexity, but can be done if it adds value.
Granularity – Let us say we are able to classify both skill and motivation into High and Low using various techniques. This creates a simple segmentation as mentioned below:
If the distribution is skewed highly in one of the segments, we can change the threshold to define High and Low and re-create the segmentation.
Finally, you can now start analyzing employees to answer following questions:
Which kind of employees are having highest attrition? Is this HL / LH / LL or HH?
What is the average life span for each of these categories?
How many training each of these segments get in a year?
How many HH employees have been recognized for their work and contribution? What can be changed?

Hope this gives you a fair idea about creating and implementing a segmentation.
A few additional notes:
Before closing the article, would like to mention a few additional points to keep in mind:
Try and keep a fair volume distribution in various segments. If this does not happen, then you will end up analyzing on this data, which can result in wrong inferences.
Segmentation is always done as a means to achieve something. It can not be an objective in itself. So before starting any segmentation, always ask are you clear about the objective.
Techniques of segmentation help, but you can achieve more than 70% of results with a good business understanding.
So next time if you see any reporting happening at an overall level, STOP. STOP and think what you might be looking over and how can you improve this to bring out more actionable insights.
If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our facebook page
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"How to create compelling analytical stories using infographics?","Kunal Jain",2013-08-13 00:28:00,2,"
							
										
						Let me go back a few years:

After spending slightly more than a year in my previous role, I moved into a new role in my Organization. This new role was my first experience in Customer Management analytics. The amount of data was more than I had handled ever before. The impact I could have on my employers’ profitability was huge.
I worked hard to deliver good quality analysis but the impact from these analysis was lower than what I could have created.

This is a common situation many analysts face during their career.
The transition from being a number cruncher to a story teller (of course, based on numbers) is more difficult than it sounds. Thankfully, Infographic came to my rescue. Although, the purpose of infographics is to share content and increase visibility over the web, they can be used to create stories in effective manner.

You can find various tips on how to create good infographics over web, however the purpose of this article is to tell “How you can use Infographics to create compelling stories from your analysis?”
What is an infographic?
As per Wikipedia:
Information graphics or infographics are graphic visual representations of information, data or knowledge intended to present complex information quickly and clearly. They can improve  cognition by utilizing graphics to enhance the human visual system’s ability to see patterns and trends. The process of creating infographics can be referred to as data visualization, information design, or information architecture.
Simply put, you need to answer the following question:
If you had only one page to tell your story, how would you tell it?
Elements of an infographic:
Infographics are important because they change the way people find and experience stories – especially now, when more and more infographics are being used to create editorial content on the web. Infographics create a new way of seeing the world of data, and they help communicate complex ideas in a clear, crisp and beautiful manner.
A good infographic must:
present complex information quickly and clearly
integrate words and graphics to reveal information, patterns or trends
be easier to understand than words alone
be beautiful and engaging
.
How can infographic help you create a story?
You might almost think that what does an infographic have to do with creating and presenting a story? Well, it does! As a tool, it forces you to create and present your story on a single page. It ensures that you remove all the clutter around the story (in your mind and on paper as well) and present it in simple & graphical manner.
Doesn’t that sound like a story?
One of my mentors used to advice “If you can not present a project in 10 slides, you are not clear about what you have done.” Once I was comfortable creating story in 10 slides, the milestone moved to 3 slides!
With use of infographic, the challenge moves to a single page.
Let us take an example, Economist recently produced an interactive infographic on how Google Chrome has become most widely used browser. This is how it looks:

Now imagine the number of slides you would have required to tell this story?
So, how do create a story with an infographic?
Just follow following steps:

Create headline and wire frames on a paper (preferably A3): The headline should summarize the project and wire frames should capture the story of the analysis.
Populate message in each wire frame: In each of the frames, write the message you want to convey through the chart / graphic. Don’t draw the charts yet! Just see if the story is complete from customer / audience point of view.
Remove / Combine unnecessary wire frames: Next, ask this question for each and every wire frame: “Can I remove this wire frame without impacting the story?” or “Can I narrate my story without this graphic?”. If the answer is “Yes”, then remove them. A few tips to make this easy for you:
If you have performed an analysis, but it does not fit in the story, remove it.
There is no rule on minimum number of wire frames. Lower the better! If one chart can tell the entire story, just use one.
If you can combine multiple charts or frames, combine them. Since you will emphasize on one or two key charts in an infographic, they can be relatively complex. However, remember that complexity should be avoided to the extent possible.

Represent using best charts: For each of the remaining frames, think hard about what is the best chart / graphic to represent this view. Some of the general guidelines like maximize data-ink ratio, selecting the right chart should be followed during this step. If need be, feel free to use variety of charts like geocharts, Venn diagram, Area chart etc.
Provide priority wise space: Next, rank order each wire frame in order of importance. Once done, make sure that space allocated to the wire frame is in proportion of its importance. Most important chart in the story deserves the highest area on the paper.
Narrate story in less than a minute: Speak out the story on this page in less than a minute. If it takes longer or is hard to comprehend, SIMPLIFY and re-structure until you achieve this.
Create the infographic: Once happy with the story, as an optional step you can actually go ahead and create an infographic. There are multiple tools on the web to create the same.
Using this process, you can use infographics to create stories out of your analytical project. To start with, can you create an infographic summarizing this post?
If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our facebook page
 
Image 1 credit: residentsvoices.net
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Common data preparation mistakes and how to avoid them?","Kunal Jain",2013-08-06 19:17:00,3,"
							
										
						A few days back, one of my friend was building a model to predict propensity of conversion of leads procured through an Online Sales partner. While presenting his findings to stakeholders, one of the insights he mentioned lead to a very involved discussion in the room. The insight was as follows:
The higher the number of times a lead is shared by partner, higher are its chances of conversion.
Following arguments were presented during the debate which ensued:
Group 1 (Pro-insight) main hypothesis:
Higher the number of times a lead has visited the sales partner website, higher is the chance that he would be interested in buying the product.
Group 2 (Against-insight) main hypothesis:
If we were not able to convert a lead into a customer on multiple occassions before, the customer might not be interested in our product and hence it sounds counter-intuitive that these customers have higher conversion.
Upon deeper examination the analyst mentioned casually:
If a lead shared multiple times converted into a sale, all the instances were tagged with positive outcome, irrespective of when the lead was sourced.
So, if a lead was sourced 2 years back and was shared again a month back following which sales occurred, the data set on which the model was trained tagged both the outcome as positive. The analyst was not even aware of the mistake he had made. Unknowingly, he had assigned a higher weightage to instances where same lead were shared multiple times and they got converted.
Needless to say, the model was re-built after tagging positive outcome to the recent lead sharing instance and the insight turned out to be false. Imagine the impact this error might have had, in case it went un-noticed. The company would have ended up focusing on leads which had similar or lower chances of conversion!
As in the example above, mistakes made at data preparation stage could be very costly. They will not only result in iterations of the model, but can lead to wrong insights and strategy.
The purpose of this post is to call out various mistakes analysts make during data preparation and how to avoid them.
Data preparation process:
During any kind of analysis (especially so during predictive modeling), data preparation takes the highest amount of time and resources. Practitioners estimate that 50% – 80% of total project time is typically spent in preparing and cleaning data.
Data preparation process can be divided in three stages:
Identifying the data set: Identifying the variables and the period on which the data will be trained, tested and validated.
Cleansing the data set: Once the variable and period is decided, you need to clean the data set for any anamolies it might have. Removing outliers, treating missing values are some of the common operations performed at this stage.
Adding transformed / calculated variables: Adding more meaningful variables in the modeling process. These could be ratios, calculated and transformed variables. If done correctly, these variables can add tremendous power to your analysis.

While there are multiple kind of errors which might happen during data preparation stage, I am focusing on the key ones to avoid.
Mistakes made in identifying the data set:
Historical data not available accurately: This is a common system constraint in Organizations where there is no warehousing in place or in case when base systems overwrites data there by erasing historical information.
Since, the analysis is meant to happen on historical data, your variables should look like as they did at the point event happened (and not as of now). For example, if a customer was living in a non-metro city at time of event and has moved to a metro city now, your modeling data set should call out city type as non-metro.
Data colllected only for positive outcomes: Let us take an example which happens commonly across companies. Let us say you get 100 applications for credit cards and you reject 60. You issue cards to 40 people and only enter the data for these 40 applications into your system. You end up saving space and these variables do not have any impact on existing customers. Makes sense? Probably not. Think what would happen when you want to create a smarter model to screen applications. It will need the variables for all 100 applications in order to create a better model. If you create model with values for 40, each of these will turn out significant!
Absence of non-biased data set: Ideally, you need a non-biased data set to bring out correct insights. For example, if you are building a model to predict probability of conversion of lead, you are inherently assuming that all leads have been treated similarly, which may or may not be the case. Actual process owners might be working more on a particular type of leads. This error can be mitigated by including some additional variables like number of attempts, but they may or may not exist.
Including data from a period which is no longer valid: Business strategy, process and system change happen frequently. Some of these changes might make historical data non-usable. These could be changes like removal / addition of fields captured in past, a new segmentation or prioritization in place, Change in website layout impacting customer journey.
If your data is impacted by any of these changes, it is best that you exclude all data points pertaining to past strategy / process or system.
Variables which can change because of change in customer behaviour: If you are including variables, which are collected from customer, but are not verified, then you need to be careful about creating insights basis these varaibles. If you create a segmentation basis customer self stated income, customers might start under-stating or over-stating income depending on the treatment of these segments.
Building model on thin data: You need a minimum sample size in order to avoid mixing signal with noise. Looking at confidence intervals and comparing them with differences in various segments can give you a fair idea whether there is enough data or not. For example, Segment A has a response rate of 10% +/- 2% and segment B has response of 12 +/- 1%, there might be a fair overlap in these segments indicating that either you need to try different segmentation or you need more data points.
Mistakes made in cleansing the data set:
Not removing outliers: Outliers can skew inferences very significantly. If there is no reason to believe that a particular event leading to outlier will happen again, it should be left out of your data set. While some modeling techniques are better at handling skew from outliers (e.g. decision tree), most of the techniques can not. Further, models are meant to work in cases where you have large amount of data points, clearly this will not be the case with outliers.
Not removing duplicates: If your data set contains duplicate records, they need to be de-duped before you perform any analysis on them. By not doing so, you end up giving extra weightage to instances which occur multiple times. The example mentioned at the start of this article is a special case of this error.
Not treating zero, null and special values carefully: Treating these values carefully can create a huge difference to your model. For example, excel assigns missing values by 0 (which is 01/01/1900) in dates. If you are using this date to calculate time difference, your model is bound to give erroneous results. Some better ways could be to replace these values by mean, median, start of the period, end of the period. Similarly, there could be special values which might have a different connotation in system. Not understanding and accounting for them will lead to errors.
Mistakes made in adding transformed / calculated variables to the data set:
While the mistakes mentioned in the previous sections would end up giving you wrong results, not spending enough time on this step will end up giving sub-optimal results. Some of the common mistakes people make in this step are:
Adding ID as a variable: At times people use numeric ids as an input in their variables. Every Organization and system has its own way to create ids. Using them blindly in modeling will end up giving wierd results. The only scenario where I think IDs can be used is when they are allocated sequentially and there is no other variable to capture the time when the allocation happened.
Not being hypothesis driven in creating calculated / transformed variables: You need to create meaningful variables basis business understanding and hypothesis. Just trying out new variables without having a hypothesis in mind will end up consuming a lot of time without giving any meaningful additions.
Not spending enough time thinking about transformations: Since data cleaning takes a lot of time, analysts are typically exhausted by the time they reach this stage. Hence, they tend to move on without spending enough time thinking about new possible variables. As mentioned, this might end up hurting the quality of your model significantly. One of the best way to mitigate this is to start with a fresh mind. So, if you have finished all the data cleaning, take a break, give a pat on your back and start thinking about transformations / calculated variables from next day. You will see some quality variables coming out if you spend time at this stage.
These were some of the mistakes I have seen people make at data preparation stage. If you can think of any other common mistake, please add them here.
If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our facebook page
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"How freshers can ace interviews for Business Analytics roles?","Kunal Jain",2013-07-31 22:36:00,4,"
							
										
						Campus interviews can be very competitive, especially so, if you want to secure a job with the best companies. Further, if you are a fresher, the experience of giving interviews can be unnerving at times. The combination of these factors can make even the best analysts falter when being top of the game matters the most.
How do you ensure that you excel in campus interviews?
Thankfully, you can train yourself to make sure that you present your best when it matters the most. The purpose of this post is to provide tips, using which you can blaze through any analytics interview:

First and foremost, how are analytics interview different from other interviews?
Interviews for analytics positions could be quite different compared to interviews for Sales or Operations. This is because the skills required for success in analytics are different from the ones in other functions. Analytics interviews for freshers are typically meant to test some (if not all) of the following skills:
Structured thinking
Attention to details
Numerical skills
Problem solving capabilities
Communication skills
Hence, they differ in design and assessment compared to other interviews. In order to test these skills, companies rely on case studies, strategic / analytical role play, puzzles and aptitude tests.
 
Lay the ground work and train yourself
The pressure to get your first job could be immense and hence it makes sense to be fully prepared for what you are likely to face. Your preparations should begin at least 3 – 4 months before your placement season starts. You need to prepare and train just like an athlete does for his / her competition.
Following are list of activities you should complete before even applying to a company:

Prepare your CV: Your CV is your window of opportunity to get shortlisted. While preparing a good CV is not the focus of this post, I would want to highlight 2 points for making your CV better:
CV should ideally be 1 pager, maximum 2.
Highlight the impact of your projects, internships and contributions through numbers. How much profit did the company make because of your contribution? What was the impact of your internship?

Once completed, get the CV reviewed by some of your Seniors to make it flawless.
Identify and prioritize your target companies: Identify the companies you would want to join before hand. Check if these companies are coming to your campus. If not, you should reach out to the right people in these companies and find out how you can apply? Once identified and prioritized, go through the website of these companies, past annual reports and recent press clippings (if any)
Revise MBA concepts and frameworks: If you are passing out of a B-school, this should not take any extra effort. However, if you are a non-MBA graduate, go through and understand some of the common business frameworks. Some of the famous ones are 4Ps, 3Cs and Porter 5 forces. Keep special focus on frameworks which can have numerical part (e.g. Profit = Revenues – Cost). While there is no need to memorize these frameworks, you need to apply these frameworks to solve business problems.
Solve Case studies: If you are a non-MBA graduate, get some of the B-school case studies and apply the frameworks mentioned above to solve these case studies. I would suggest smaller case studies (which can be completed in an hour or so) rather than elaborate ones. Also, don’t ignore the numerical part of these case studies, it is as important as strategic thinking. Spread these case studies over days rather than doing all of them in a go.
Form an interview group: A lot of these activities are best done in groups. You can ask your partners to take a case interview, perform a role play, pool puzzles and case interviews etc. Needless to emphasize the importance of right group here. Try and get people from diverse background but common aspirations.
Pool interview puzzles and solve them: Arrange as many interview puzzles as possible and then solve them with your group. Discuss them and see if you can find a shorter way. What could you have done better to reduce time?
Solve aptitude tests: Taking up aptitude tests which score you on logical reasoning and numerical skills is the best way to brush up your numerical skills. They not only simulate a possible round of interview, but also create an environment of pressure. Finding and taking mock CAT and GMAT exams could be a good way to do these tests.
Read about analytics: While this may or may not have a direct impact on any of the rounds you face, it will provide you discussion points after the interview. They can also be used to answer some of the questions like “Why do you want to enter Analytics?” or “Why does Analytics excite you?”
Finally, get into the routine: Your mind needs to be on top of its productivity during the interviews. If the interviews are expected in evenings or over the weekend, solve your case studies and perform role plays during same time. Try and simulate similar environment so that you are at home on the final day.


On the day of interview
Following are some important tips to observe for your day of interview:

Take good sleep the night before: On the night before, take a sound sleep. If you have not read or prepared some thing yet, there is no point of preparing it now. Your mind should be in its best productive state during interview, so a night out or late night coffee would not help.
Dress up comfortably: Wear the dress you are comfortable in. If the shirt is tight, change it (but keep it ready in time).
Reach the venue before time: There is no point in creating hassle in your mind by getting late or rushing at the last minute. Plan your time for commute, keep buffer for traffic jams and reach the venue with a relaxed mind.
 
During the interview
Enjoy the process: You should judge the interviewer as much as they are judging you. Enjoy this period of interaction as much as you can. Anticipate
Focus on framework & interpretation of numbers: For case studies and role play, focus on the approach and framework. The interviewer would be interested in your framework and strategic thinking. Numerical accuracy is important, but not the most important thing. Also, try and interpret the numbers thrown at you. Is the cost higher than revenues? Is the company making operation loss or profit?
Be honest and call out assumptions: One attribute every manager wants in his analyst is honesty and integrity. No one expects you to know solution to every problem in the world. So, if you don’t know the answer, say so. If you are making assumptions, say so. There is no point in bragging about something you don’t know.
Think out Loud: In evaluation of case studies and role plays, your problem solving approach has higher weightage compared to the actual answer. Hence, it is important that you always keep interviewer up to date with your thought process. As a good practice, layout the approach in front before actually proceeding with the case study (or a part of). So, even if the answer is wrong, interviewer understands that the approach was right.
Take notes / solve the case by laying out things neatly on paper: One of the best ways to make sure you follow a framework is to lay out things neatly on paper. For example, if the question is to see how a company can become profitable, I would say that the company has 2 options: Reduce cost or increase revenues. Then I’ll divide the paper in 2 half and list down various levers for each part accordingly. You can then discuss each lever. This not only ensures that a framework is followed, but also that you look at the holistic picture.
Ask questions whenever you can: If you are not clear about any thing, ask! There are times when questions are kept vague deliberately. It is to test how do you deal with ambiguous situation. Towards the end of the interview, ask questions which can help you understand the role better. What is the role? How does a typical day or week look like? What is the profile of people in team are some good questions you should ask if you don’t know the answer to.

 
Do these preparation sound over-whelming? They probably are. But, I can assure you that if you follow these in disciplined manner, you will ace through the interviews and reach your dream companies.

The more you practice these, the better you will become in clearing interviews with flying colors.

As usual, if you have any other tips, feel free to add them in comments below.

If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our facebook page



Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Nine productivity boosting tips for SAS Enterprise Guide Users","Kunal Jain",2013-07-28 22:49:00,1,"
							
										
						SAS Enterprise Guide is a versatile tool for everyone from novice analysts to experienced programmers. It has revolutionized the way people use and access SAS (especially new users). With use of Enterprise Guide, it is possible for any newbie to start using capabilities of SAS without spending too much time on training.
During my early days with SAS, Enterprise Guide turned out to be a huge help, using which I could not only accelerate my learning, but also make sure that there is little or no impact on delivery timelines of projects.

What is SAS Enterprise Guide (EG)?
Simply put, SAS Enterprise Guide (EG) is a Graphical User Interface (GUI) to access capabilities of SAS. It is a point-and-click, menu- and wizard-driven tool that empowers users to analyze data and publish results. It provides fast-track learning for quick data analysis, generates code for productivity and speeds your ability to deploy analyses and forecasts in real time. EG enables you to preview the code it writes at the back end, thus enabling the user to learn SAS in parallel.
While EG is easy to use and learn, there are basic tips, which can boost your productivity while using it. These tips will ensure that you use EG in efficient and effective manner. So, go ahead and read them:
Tip 1: Layout process flow on paper before starting on EG
As mentioned in other articles, it is a good practice to plan and structure your analysis on paper before you actually use the tool. In case of EG, you can take this to next level. Before starting any project, I draw out the expected process flow with great details on paper. This not only helps me structure my thoughts, but also clears up any confusion before starting the work and also helps me sense check the outcome at a later stage.
A typical process flow would look something like this:

As you can see, I even include things like number of rows expected, filter applied, nature of joins etc. The more details you can add at this stage, the easier it becomes to execute.
Tip 2: Brainstorm the process flow with a colleague
This is especially important if you are new to EG. I would suggest to continue doing this for first 3 – 6 months at least and then for any big project. Some of the aspects you should consider in this brainstorming are:
Is this process flow efficient? Are there steps which could be eliminated, combined, broken to make it more efficient.
Which steps will take the longest time to run? Are there ways to reduce time taken by these steps? Can you join tables in a different order? Can you use an alternate indexed table which has the same information?
 What are the checks you can do after every big step to make sure that the outcome is correct?
Tip 3: Compress the data sets
While EG makes data manipulations easy, data is stored ineffectively. Use following system option to compress at start of your project and it will be applied to all data sets:

options compress = yes;

Tip 4: Give names to queries and data sets 
One of the downside of using SAS EG is that it is really bad at giving names to queries (or query builder step) and data sets. If you continue with the names provided by EG, you will not be able to make any sense out of process flow, when you open next.
A valuable addition here is to add names which describe the query or data set better. For example, a query filtering data basis city can be named something like “filter_city_dataset1<U+2033> rather than the default “query_for_dataset1<U+2033>. This helps immensely when you open any code at a later date.
Tip 5: Import a csv file (.csv) instead of Excel file (.xls)
As a common situation, we typically import a lot of data from Excel (and Access) because traditionally these tools have been used for data storage and manipulation. One of the best practice is to store the data you want to import as Comma Separated Values (.csv) and then import it in SAS. Doing so, typically takes only fraction of time compared to importing data from Excel or Access native formats.
Tip 6: Store data in permanent memory after every big step
If you have finished a step which has takes a lot of time to run, you should store the outcome into permanent memory rather than keeping it in WORK.
How does it help?
In case you lose your connection with the server, you don’t need to run the entire big step again
If you want to make changes in this project at a later date, you can manipulate only the part in which change is required, rather than running the entire project again.
Tip 7: Arrange the project in multiple process flows
One of the best ways to keep process flows easy to understand is to divide them in functional process flows and keep them separate. For example, if you are creating a financial model, you can have one process flow for income, one for expenses, one for profit calculation and so on and so forth.
Tip 8: Document the process flow as soon as you finish it
This is simply good house-keeping. Every time you finish a project, make sure you spend some time documenting it. Add a simple note in each process flow including at least following:
What does the process flow achieve?
What are the input and output data sets?
What are the assumptions you have made? Where to make changes in order to change them?
Tip 9: Disable automatic opening of data
By default, EG opens any data set you bring in the process flow. While this helps you look and explore the data before you write a query, you end up using network and resources (especially for big data sets). If you want to explore any big data set, it is best done on a sample.
To disable automatic opening of data, go to Tools -> Options -> Data and then de-select the option which says “Automatically open data when added to project”. Press OK to save.
These were some tips which I have used and benefited from in my usage of EG. Please feel free to add any more  tips you might have used to boost your productivity while using SAS EG.

If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our facebook page
 
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"What is big data and how is big data architecture designed?","Kunal Jain",2013-07-24 23:38:00,4,"
							
										
						Consider following fact:
Facebook currently has more than 1 Billion active users every month
Let us spend a few seconds to think what information Facebook typically stores about its users. Some of this is:
Basic demographics (e.g. date of birth, gender, current location, past location, College)
Pact activities & updates of users (your photos, comments, likes, applications you have used, games you played, messages, chats etc.)
Your social network (Your friends, their circles, how you are related etc.)
Interests of users (books read, movies watched, places etc.)

Using this and a lot of other information (e.g. what did a user click, read and how much time did he spend on it), Facebook then performs following in real time:
Recommend people you might know & mutual connections with them
Use your current and all past activities to understand what you are interested in
Target you with updates, activities and advertisements you might be most interested in
Over and above these, there are near time activities (refreshed in batches and not in real time) like number of people talking about a page, people reached in a week.
Now, imagine the kind of data infrastructure required to run Facebook, the size of its data center, the processing power required to suffice its user requirements.The magnitude could be exciting or scary, depending on how you look at it.
Following infographic from IBM brings out the magnitude of data requirement / processing for some similar Organizations:

This kind of size and scale was unheard by any analyst till a few years back and the data infrastructure some of these Organizations had invested in was not prepared to handle this scale. This is typically referred as Big data problem.
So, what is big data?
Big Data is data that is too large, complex and dynamic for any conventional data tools to capture, store, manage and analyze. Traditional tools were designed with a scale in mind. For example, when an Organization would want to invest in a Business Intelligence solution, the implementation partner would come in, study the business requirements and then would create a solution to cater to these requirements.
If the requirement of this Organization increases over time or if it wants to run more granular analysis, it had to re-invest in data infrastructure. The cost of resources involved in scaling up the resources typically used to increase exponentially. Further, there would be a limitation on the size you could scale up to (e.g. size of machine, CPU, RAM etc.). These traditional systems would not be able to support the scale required by some of the internet companies.
How is big data different from traditional data? 
Fortunately or unfortunately, there is no size / parametric cut off to decide whether the data is “big data” or not. Big data is typically characterized basis what is popularly known as 3 Vs:
Volume – Today, there are organizations producing terabytes of data in a day. With increasing data, you will need to leave some data without analyzing, if you want to use traditional tools. As data size increases further, you will leave more and more data without analysis. This means leaving value on table. You have all the information about what customer is doing and saying, but you are unable to comprehend! – a sure sign that you are dealing with bigger data than what your system supports.
Variety – While volume is just the start, variety is what makes it really difficult for traditional tools. Traditional tools work best with structured data. They require data is a particular structure and format to make sense out of it. However, the flood of data coming from emails, customer comments, social media forums, customer journey on website and call-centers are unstructured in nature or semi-structured at best.
Velocity – The pace at which data gets generated is as critical as the other two factors. The speed with which a company can analyze data would eventually become competitive advantage for them. It is their speed of analysis, which allows Google to predict the location of flu patients almost real time. So, if you are unable to analyze data at a speed faster than its inflow, you might need a big data solution.
Individually, each of these Vs can still be worked around with help of traditional solutions. For example, if most of your data is structured, you can still get 80% – 90% of business value through traditional tools. However, if you face a challenge with all three Vs – you know you are dealing with “big data”.

When do you need a big data solution?
While the 3 Vs will tell you whether you are dealing with “big data” or not, you may or may not need a big data solution depending on your need. Following are scenarios where big data solutions are inherently more suitable:
When you are dealing with huge semi-structured or unstructured data from multiple sources
You need to analyze all of your data and can not work with sampling them.
The process is iterative in nature (e.g. Searches on Google, Facebook graph search)
How do big data solution work?
While the limitations of traditional solutions are clear, how do big data solutions solve them? Big data solutions work on a fundamentally different architecture which is built on following characteristics (illustrative below):
Data Distribution and parallel processing: Big data solutions work on distributed storage and parallel processing. Simply put, files are broken into multiple small blocks and stored in different units (called racks). Then, the processing happens in parallel on these blocks and results are merged back together. The first part of the operation is typically called a Distributed File System (DFS) while the second part is called Mapreduce.
Tolerance to failure: By nature of their design, big data solution have built in redundancy. For example, Hadoop creates 3 copies of each data block across at least 2 racks. So, even if an entire rack fails or is unavailable, the solution continues to work. Why is this built in? This feature enables big data solutions to scale up even on cheap commodity hardware rather than expensive SAN disks.
Scalability & Flexibility: This is the genesis of entire big data solution paradigm. You can easily add or remove racks from the cluster without worrying about the size for which this solution was designed for.
Cost effectiveness: Because of use of commodity hardware, the cost of creating this infrastructure is far lower than buying expensive servers with failure resistant disks (e.g. SAN)


Finally, what if all of this was on cloud?
While developing big data architecture is cost effective, finding right resources is difficult which increases the cost of implementation.
Imagine a situation where all of your IT / infrastructure worries are also taken care by a cloud service provider. You focus on performing analysis and delivering results to business rather than arranging racks and worrying about extent of their usage.
All you have to do is pay as per your usage. Today, there are such end to end solutions available in market, where you can not only store your data on cloud, but also query and analyze it over the cloud. You can query terabytes of data in matter of seconds and leave all the worry about these infrastructure for some one else!
While I have provided an overview of big data solutions, this by no means covers the entire spectrum. The purpose is to start the journey and be ready for the revolution which is on it’s way.
If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our facebook page
 
 
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Must read books for Analysts (or people interested in Analytics)","Kunal Jain",2013-07-21 00:30:00,7,"
							
										
						One of the ways I continue my learning is reading. I read for 30 minutes before hitting the bed every day. This not only makes sure that I learn some thing daily, but also ends my day in a fulfilling manner. Over the years, I have read a variety of books on various subjects. In this article, I will share a list of 7 must read books, which I think should be present in every Analyst’s bookshelf.
I believe that each and every book listed below has helped me learn about Analytics and expect that they will immensely help people who want to learn about this field.
These books are chosen, not because they have best technical subject matter, but because they help you understand the impact of analytics. While none of these books will tell you how to build the best logistic regression model, they will definitely help you appreciate the need and impact of building a model.

So, if you wish to learn about analytics, do read these books and see how analytics is transforming the world around you:
1. When Genius failed:  The Rise and fall of Long-Term Capital Management
This book is one of my all time favorite. It starts from a period when no one had heard of analysts being employed on Wall Street. A fast paced thriller, which is difficult to put down after reading first 50 pages. This book not only brings out how you can compete on data based decisions, but also why you need to keep human behavior in mind while taking decisions on data.
Here is a brief description about the story:
When it was founded in 1993, Long-Term was hailed as the most impressive hedge fund in history. But after four years in which the firm dazzled Wall Street as a $100 billion moneymaking juggernaut, it suddenly suffered catastrophic losses that jeopardized not only the biggest banks on Wall Street but the stability of the financial system itself.
2. Scoring Points:  How Tesco Continues to Win Customer Loyalty
Written by Clive Humby, one of the founders of Dunnhumby (company which does Analytics for Tesco), this book brings out some of the practical challenges faced by Tesco and how they overcame them to create one of the biggest success story of customer loyalty.
A must read for any one dealing with customer data to study their behavior and then turning them in business insights.
3. The Signal and the Noise: The Art and Science of Prediction
In his recently published book, the political forecaster Nate Silver, who accurately predicted the results of every single state in the 2012 US election, reveals how we can all develop better foresight in an uncertain world. From the stock market to the poker table, from earthquakes to the economy, he takes us on an enthralling insider’s tour of the high-stakes world of forecasting, showing how we can use information in a smarter way amid a noise of data – and make better predictions in our own lives.
The book tells not only about the errors made by analysts, but also how to avoid them.
4. Web Analytics 2.0: The Art of Online Accountability & Science of Customer Centricity
Another recent addition by Avinash Kaushik, author of the leading web analytics and research blog Occam’s Razor. This book is a must read for any one working in Online industry or wants to learn web analytics. It provides specific recommendations for creating an actionable strategy, applying analytical techniques correctly, solving challenges such as measuring social media and multichannel campaigns and employing tactics for truly listening to your customers.
5. Predictive Analytics: The Power to Predict Who Will Click, Buy, Lie, or Die
What could be better than learning about Predictive Analytics from one of the most influential personalities in this domain. In this rich, entertaining primer, Siegel reveals the power and perils of prediction by including case studies from across the globe.
Aimed towards a common man, the book explains predictive modeling and its basics in lay man terms.
6. Moneyball
Another one of my favorites, this book is based on application of statistics to baseball. Inspiring story of a baseball team manager with low budget to run the team, who uses statistics to identify undervalued players and carves out a winning team out of them. The book has also been converted into a movie, but I prefer the book over the movie.
7. Freakonomics
This one is a classic, probably the first book where I read on how data and analysis can be used to revel unknown insights.  While analytics has evolved significantly since the time this book had come out, this book is still worth a read for the analysis it presents and delivers from the tools of previous era.
These ones just missed…
Apart from the ones mentioned, there were a few other books which I had considered in creating this list and are worthy of mention. The Intelligent Investor by Benjamin Graham, Competing on Analytics by Thomas Devenport, The Black Swan by Nassim Nicholas Taleb fall in this category. All of these are good reads if you are interested in the subject.
This is my list of must read books for every analyst. In case you have any other recommendation or want to add more on any of the books mentioned, please feel free to add through comments.

If you like what you just read & want to continue your analytics learning, you can subscribe to our emails or like our facebook page
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"How to become an analytics rockstar?","Kunal Jain",2013-07-16 22:24:00,3,"
							
										
						I still remember first day in my first job.
I walked in the office with high ambitions and little understanding of what it takes for an analyst to become successful.
The decision to take up the job was based on the fact that some of my college alumni (whom I thought highly of) thought highly about the company and the role.
It took me almost 6 months to understand what is required to achieve success in Analytics industry. It happened only because I had the luxury of some really good mentors and loads of enthusiasm to learn new things. Not every person is lucky to have them early in their career.
While the understanding has obviously evolved over the years, most of my initial learning has stayed. I have obviously practiced and improved these learning during this time. In the remaining article, I’ll outline these success ‘mantras’ for becoming an analytics rockstar. These ‘mantras’ are as effective for a newbie as they are for a seasoned professional.

For sake of better understanding, I have mentioned these in flow of an analytics project:
Achieve flawless business (& business process) understanding
Unless you understand the business, you can not succeed in creating a solution for it. You have to understand the process end to end, why do things happen in the manner they do? What are the alternatives? What are the gaps? What are the opportunities? Are there places where execution differs from intended strategy? Why?
Before starting any project or working on any problem, following are some of the best practices I follow to achieve better business understanding:
Ask, ask and ask until the entire process makes sense and you have all the answers
Draw out the process flow and identify any gaps and opportunities.
If you are new to the area, spend a day on field (or the place of action) to understand the process end to end
Nominate yourself as a customer / business lead to see how the process works for customer.
Share your understanding with business and make sure there is no gap in understanding. Even a whiteboard / paper based discussion is fine. No need of any fancy presentation at this stage
Structure, structure and structure….and then some more structure
Structured thinking and writing is by far the biggest learning for any analyst. I have written an entire article on this before, but can not stop emphasizing it again. Putting a structure helps you deal with open ended business problems and statements like:
How do we keep write-offs from lending portfolio below 3%?
Who are our ideal customers and how much can we invest to acquire them?
The article written before has some best practices mentioned in order to improve structured thinking. Find them here.
Call out your assumptions
Remember to call out your assumptions in doing the analysis. Remember the implicit rate of interest? or the product margin you assumed? All of that needs to be mentioned to the stakeholders and decision makers. 
Why is this important?
This is important because you need to tell all the ingredients to your stakeholders. If you do this right, this will help you understand the errors coming in because of wrong assumption vs. gap in framework later on. Especially, in scenarios where you are entering the unknown, you will be able to go back and see what went as per expectation and what did not.
One of the best practice which helps to achieve this is to create a holistic list of assumption in your document / presentation and keep it as ready reference.
List down all the hypothesis and then prove, discard or suspend them
This is the most interesting part for business owners. Some of the answers they would look out for are:
Which business hypothesis stand validated with data / analysis?
Which hypothesis business was not aware of? What opportunity can they translate into?
Which hypothesis stand rejected? What is the impact of these corrections / changes?
In addition to this, list down all the hypothesis you had but have not been able to prove / dis-prove them due to lack of data, time or resources. Decision owners should know the blind spots before taking a decision.
Keep implementation in mind and take business feedback on the solution as early as possible
A solution is only as good as it can be implemented. Hence, keep your business owners in loop on the solution you are proposing. Until they feel excited about the solution, it is unlikely to fly. Following are the best practices to ensure this:
Keep it simple: It is always easy to create the next Pandora’s box with the tools and data. But what would you do if your CRM does not support it? or if there is no campaign management solution? Even if there is, can you create so many segments automatically? More often than not, keeping it simple makes it easy to implement solution. You can always propose that Pandora box in next iteration.
Apply 80 / 20 ruthlessly: If applying 20% of analysis outcome brings in 80% of benefits, apply them and focus on how you can boost these 80% benefits further.
Presentation – What is the story?
All your hard work gets culminated in the way you present it. So, give it the necessary thought space. Nothing beats preparation here. Consider following aspects before you present:
Are you presenting the story (coming out of numbers and analysis) or you are presenting what you did? If you are doing the later, STOP. It won’t help! Even if you have done various pieces of analysis and they don’t fit in the story, don’t present them. Also, if this happens regularly, think about how good was your structure in first place?
Who are you presenting to? What would they be interested in? Have you called out the solution to their needs in this presentation?
Follow the practice: “Tell them – what you will tell them, tell them – tell them, tell them – what you have told them” in structuring your presentation. Start with summary, present the findings and finish with summary of recommendations and benefits from the project.
Learn something new regularly
While, all the ‘mantras’ mentioned above help you become better with every project, this one improves you in long term. Make sure that you spend time learning something new. Analytics is a dynamic, fast paced industry. New software and solutions arrive every alternate month. Spend quality time regularly on your own development and learning. This will set you up for long term success. Here is what I do:
Read for 30 minutes daily before going to bed.
Create a learning group with people from diverse background and share each others learning through the week
 
If followed in disciplined manner, these ‘mantras’ will become your guide to make sure you are on your path to achieve success. If you know of any other practices which can help analysts accelerate on their path to success, please feel free to share them below.
If you like what you just read & want to continue your analytics learning, you can subscribe to our emails or like our facebook page
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"How to start a career in Business Analytics?","Kunal Jain",2013-07-13 23:32:00,7,"
							
										
						Every time I attend any analytics forum or interact with students, two questions stand out on account of number of times they are asked:
I am an undergraduate (or any other) student & want to pursue career in Analytics, what should I do?
or
I want to build a career / shift my career in Business Analytics, how should I go about doing so?
Further, I receive multiple queries through mail / social media / comments asking the same. In order to make sure these queries are addressed in best possible manner, I thought it’s best to write an article on this and provide a starting platform for everyone.

While the subject itself is wide, I’ll provide an overview of various things which can be done and focus on specific sub-topics through additional posts later on.
Before I delve into the ways of building an Analytics career, let me quickly cover why to build a career in Analytics first.
Why build a career in Business Analytics?
Analytics, as an industry is set for exponential growth. With more and more data being available in digital form, need for smarter, faster, data based decisions is only going to increase. Consider following facts to substantiate what I am saying:
According to Harvard Business Review (October 2012 edition), job of a data scientist is the sexiest job of 21st century.
According to the McKinsey Global Institute (In a May 2011 report): “By 2018, the United States alone could face a shortage of 140,000 to 190,000 people with deep analytical skills as well as 1.5 million managers and analysts with the know-how to use the analysis of big data to make effective decisions.”
Imagine what would be the number across the globe…
I understand Analytics will become big, but I don’t know how do I start a career in Business Analytics?
Its difficult for a newbie to find out the best way to start a career in Analytics due to following reasons:
The industry in nascent stage, hence, it is difficult to find proper guidance.
“Analytics” or “Business Analytics” are losely used terms, which makes it difficult for people to understand and appreciate the role
There are limited structured & well laid out path which you can follow to enter the field.
While the first factor will improve with time and I had written an article mentioning second point here, this post aims to address the third point.
Two approaches to build a career:
Like any other employment skill, there are 2 approaches to enter / start an analytics career:
Approach 1 – Get hired by a company which trains you (on job / internal trainings) on the necessary skills. These would be companies which have Analytics in their DNA and use it for their day to day decisions. While this approach is better from long term perspective, it takes time and investment (especially if there is no structured training in the companies). Some of the companies known for using cutting edge Analytics (in India) are:
Technology leaders: Google, Facebook, Linkedin
Banking, Financial Services and Insurance (BFSI): Capital One, American Express, ICICI, HDFC
Telecom companies: Idea, Vodafone, Airtel
Analytics Consultancies: Fractal, Mu-Sigma, Absolutdata, ZS Associates

I plan to post an article on how to prepare for interviews for Analytics jobs in coming weeks. Stay tuned, if you are expecting to face an interview with any of these companies!
One of the alternate route to get into these companies can be internships. So if you have a 2 – 6 month break, give a try to becoming an intern in these companies.
Approach 2 – Get Business Analytics related certification: While these certifications provides you with the technical skills required, these would not be able to compensate for experience on the job. Following are some options of training available (in India):
For people with work experience, various leading academic institutes run certification courses. I had covered them in more details in my previous post here. If you have the required experience and resources, I would recommend the course from ISB
For freshers, there are certification courses run by SAS training institute, Jigsaw Academy (Online), ADSA, Analytics Training Institute. Getting these certification courses can increase your chances of getting hired in some of the best companies.

What else can be done?
While these are specific paths for starting a career in Analytics, there are smaller steps you can take to increase your awareness. Some of them are mentioned below:
Continue reading on the subject: Keep reading about the subject through various blogs and communities. Modernanalyst.com, kaushik.net are some websites which can help you stay up to date.
Attend Industry forums: There are various industry forums, meet and conferences which you can attend to gain understanding of the subject. SAS Forum (held annually) is one of the places to meet some of the best people in industry. Videos and proceedings of SAS India Forum and Global forum might be available on internet.
Try internships: Internships are awesome way to start your career. They provide you a flavor of the work before you take a plunge and the employer gets to assess you without significant investments.
Connect to people: Since it is a small industry today, most of the people know each other. Find out how can you connect and talk to people in industry. They are your best sources of information.
Go through training material available on the web: Google provides basic training for Google Analytics free. There are various websites running basic courses which can be accessed freely. Some universities provide access to course notes freely. Consume as much as you can.
Books: Read some good books on the subject like Scoring Points (on Tesco); The new Science of retailing, The Signal and The Noise are some good books on the matter.
If there are any other resources which you have found useful to learn more on the subject, please add them in comments below.
Image credit: aibworld.net
If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our facebook page
 
You may also like: Common myths about a career in Business Analytics: Busted!
 
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"How to create Financial models flawlessly?","Kunal Jain",2013-07-07 19:42:00,1,"
							
										
						Recently, I met one of my friend working in strategy team of a bank over lunch. I felt bad for something which he mentioned casually. Here is what he said:
Every time there is a Financial model required by our leadership, we end up doing multiple iterations and thus creating numerous versions of the model. While this not only ends up taking more resources than required, it also delays the important business decisions.
This situation is not unique to his company, It happens in most of the Organizations across the globe. In the remaining article, I’ll explain what is a financial model, the process involved in creating one and some of the best practices to make them flawlessly.
What is Financial modeling and how is it used?
Financial modeling is a process by which we calculate or estimate financial numbers in various situations or scenarios. Financial models can vary from simple calculations to complex simulations which can take hours to run. A simple excel file in which you project your monthly income and expenses is a basic financial model.
Following is a sample excel model to project yearly savings with annual interest rate of 12%:

These models are used across the globe for business planning and taking important decisions for the business. Following are some questions which business owners typically answer with help of financial modeling:
Will it be profitable to offer new Credit cards to existing customers?
At what price point can a start up break even in 3 years? 5 years?
How many tele-callers do we need to call all the customers once every day? How much will be the cost?
How would the business model look when we acquire a competitor?
While a lot of organizations might be using advanced tools for a lot of granular analysis (e.g. Customer level predictions and recommendations), most of the macro / strategic analysis continues in spreadsheets.
Challenges in Financial modeling
While building financial models is inherently not difficult, there are some common mistakes which people make and then think that financial modeling is difficult. Some of the common one are mentioned below:
A single Financial model is looked as panacea to all business problems. While this might be good intent, it becomes difficult to implement practically. Financial models are used best to answer specific questions. If you want to check whether a particular tranche of customer is profitable or not, don’t try and evolve the model to answer whether the business overall is profitable or not.
Inputs and assumptions are not called out explicitly: Any financial model is built basis some inputs and assumptions. Typically, people use hard coded values for assumptions and collect inputs at various places. This leads to confusion. Also, their is a risk that you will end up making wrong conclusion, if you missed changing one of the inputs. As a best practice, call out all the inputs and assumptions in single place and link the code / file accordingly.
Inconsistency in outcomes: Another common reason for errors is that the analyst looks at the financial model as just a mathematical exercise. He doesn’t tie up numbers or models impact of changing numbers in one department over another. If you want to buy more raw material or keep more inventory, not only you would need more space, you would also need more human resources and maintenance.
Process of building a financial model
Below is a structured approach to financial modeling. Following these in disciplined manner would ensure that the common errors mentioned above are avoided and you achieve the desired result in a single attempt. At the start, these steps might seem time consuming and extra effort, but, if followed diligently they would end up saving multiple days and iterations.

We will cover best practices involved in each step below
Step 1: Understand business requirements
It is very important that you understand all the business requirement at the outset. If you don’t, your financial model is doomed for failure. Some important questions to ask business users are:
What are the answers business is looking from the model?
What are the levers in control of business owners? What is out of their control?
What is the period for which they want to take the decisions?
What are the constraints (resources / budgets / capacity)?
What are the dependencies across various departments?
Once you have a fair understanding of what business wants, agree on set of inputs, output and assumptions. It is a good practice to keep all your inputs, output and assumptions together. If you are using excel, keep them in a sheet each. If you are using a coding environment, define inputs and assumptions at the outset. By doing so, you make sure that creating scenarios later on is easy and can be done without creating any confusion.
Step 2: Finalize dimensions and granularity
This should come directly from business need. Granularity of a financial model is usually determined by level at which business wants to take decision. Is it at country level? Product line level? Product level? If time permits, I recommend creating model at one level more granular than the customer needs initially. So if customer is looking at country level financial model, try creating something at Regional level.
This ensures that not only the decision can be made at the required level, but you also get a texture of how to implement that decision. It answers the next level of questions for the business owners.
Step 3: Apply business logic and formulas
Once all the requirements are clear and granularity is decided, just apply business logic to build the model. By business logics, I mean the the mathematical translation of business understanding. What parameters and values will be impacted by various inputs? What is the profit margin? Are there any over head expenses which you have not included?
One of the good practices here is to keep the entire worksheet / code logically linked with out any hard coded values. If there is constant which is being used, it should go in either inputs or assumptions. In order to simulate any scenario, you should not be required to change inputs in multiple sheets or places.
Another benefit of keeping all calculations formula driven is that you need to only make sure that your inputs are consistent and the business logic are correct. If you ensure these two things are done diligently, your model will always remain consistent.
Step 4: Test scenarios
Once the framework of the model and base scenario is ready, you need to test whether the model holds under various scenarios. What are the boundary conditions when business logic would no longer hold true? You need to define the minimum and maximum values for various inputs and set of inputs.
Once this is ready, you should train your business users on the limitations of the model.
Step 5: Document
Finally, document all the needs, granularity, calculation and limitations in a document. Again the recommendation is to do need based documentation. The purpose of this documentation is to make sure that any one can pick this document and understand the model. A typical document has following sections:
Background
Business need
Problem statement
Scope of the model
Areas out of scope of modeling
Business logic and calculations
Limitations
References (if any)
Once this documentation is ready, your model is ready to roll out.
As mentioned before, if you follow these steps diligently, it will avoid a lot of re-work. Further, the more you practice them, the better you will become at financial modeling. Keep practicing these and become flawless financial modeler!
In case you are aware of any other good practices, please feel free to reach out to me or write them in comments below.
If you like what you just read & want to continue your analytics learning, you can subscribe to our emails or like our facebook page
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Advanced analytics certifications in India","Kunal Jain",2013-07-06 23:13:00,7,"
							
										
						One question a lot of MIS professionals face day to day is:
“How do I shift my career to work in Advanced Analytics?”
A lot of people have asked this question to me. While there was no clear path for this around 3 years back, the scenario has changed today. Currently, professionals in MIS industry can undertake various certifications in advanced analytics to jump start their analytics career. This article summarizes various certifications available to people in India and rank orders them based on their merits.
Consideration set:
This analysis is aimed for people with work experience. I plan to write a similar article for freshers some time later. Further, I have only considered courses from educational institutes well known across India. I have excluded certifications from Software companies (as they tend to focus on software training rather than the concepts behind) and other smaller training institutes.
Following table summarizes various certification options available as of today:

Note: There are courses running currently in IIM Bangalore and IIM Lucknow as well, but I am not sure whether they will continue as there is no information on websites of these institutes.
Following are details for each of the courses with link to their websites:
Certificate Programme in Business Analytics (CBA), ISB
The program is a combination of classroom and Technology aided learning platform. Participants will typically be on campus for a 5 day schedule of classroom learning every alternate month for a span of 12 months, which would ideally be planned to include a weekend.
In the month of no classroom connect, the classes will be conducted over a technology aided learning platform. The contact hours in this platform would be 24 hours a month and every alternate month. One more highlight of the programme is the Action learning project whose runs for 3- 6 months and asks students to solve a real business problem from an organization. For more information, go to:
http://www.isb.edu/certificate-programme-in-business-analytics
Advanced Certificate Program in Business Analytics (ACPBA), SJMSOM, IIT Bombay
Jointly designed by the Shailesh J Mehta School Of Management at IIT Bombay, and HughesNet Global Education, the program is spread over one year and covers both the theoretical and practical aspects of the discipline. Industry experts also contribute to the course by giving lectures and presenting real-life consultancy projects. Towards the end of the course, there is 3 day on campus interaction between students and experts. For more information, go to:

http://www.hugheseducation.com/ACPBA
Executive Program in Business Analytics, IIM Calcutta
Again designed with HughesNet Global education, this executive long distance program is designed to expose participants to state of the art tools and techniques of analytics. The program coverage would include discussion on topics such as Data Mining, Design of Experiments, Survey Sampling, Statistical Inference, Investment Management, Financial Modeling, Advanced marketing Research etc.
There are 2 sessions of 3 hour interaction every week (Thursday and Saturday). For more information, go to:
http://www.hugheseducation.com/EPBA
Advanced Business Analytics & Optimization Program (Two Certifications), CEP, IIT Delhi
This program is designed by Ivory education along with DMS, IIT Delhi. It is designed to build Advanced Skills in Business Analytics and Optimization and is divided in two parts. It follows a step-by-step approach where theoretical concepts will be followed by case studies and practical implementations.
http://www.ivoryeducation.com/dms-iit-delhi-advanced-business-analytics-business-optimization-program/
Verdict
 So what would I do if I was a MIS professional?
If I had 4 – 5 years of experience and the resources, I would go for the course offered by ISB. There are some clear benefits of this course:
You get ISB brand on your CV
Method of teaching is mostly classroom, there by giving a lot better learning experience
They require higher work experience than most other courses, so you can expect to learn lot more
If I had lesser work experience (resources could still be figured through education loans), I would go for the course run by IIM Calcutta because of more time spent to cover similar course material. Hopefully, this would mean that you have more time for interaction and hands on experience.
So, following is the summary of my recommendations for advanced analytics certification:

Which one would you join if you were a MIS professional? Also, If you are aware of any certification programmes, which could be helpful, please feel free to mention in comments or to write to me.
P.S. All views mentioned here are my own and are in no way biased or influenced by any of the institutes above.
If you like what you just read & want to continue your analytics learning, you can subscribe to our emails or like our facebook page
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Common mistakes analysts make during analysis and how to avoid them?","Kunal Jain",2013-06-30 21:26:00,1,"
							
										
						Quite often I come across situations where people end up making wrong inferences based on half baked analysis. Or when people force fit data to what they already believe. A popular quote on the subject says:

If you torture the data long enough, it will confess.
- Ronald Coase, Economist

I personally disagree with the quote and firmly believe the other way “If you slice and dice the data in unbiased manner, it will reveal the truth.”
Having said that, I am aware of situations and people who intentionally or non-intentionally end up making wrong inferences. Following is an example which brings out one such case:
Example of a typical error
A BFSI Organization had come up with potentially ground breaking idea. It looked brilliant on paper and people  internally were excited about implementing it. Since it was a strategic shift, the team decided to run a pilot in selected locations. First reads from the pilot came out after 5 months of implementation. The MIS team analyzed the data and quickly concluded success of pilot basis following graph:

Almost any metric you look at told same story unanimously. The whole project team was gung-ho about the success of the project and were getting ready to implement the strategy Pan India.
This is when a wise analyst entered and asked to look at pre and post scenario together before concluding any thing. This graph had a different story to tell:

The entire story changed after looking at this graph. A clear winner strategy suddenly turned into a loser! Imagine the loss company would have suffered, had it implemented new strategy Pan India. Actually, locations were nominated by Regional Directors and they chose locations where they had higher confidence of good implementation (hence they had better performance before hand).
The purpose of this post is to bring out some of the common mistakes analysts make and how to avoid them:
1. Drawing Inferences only basis mean (without removing Outliers)
If you believe that any event does not represent a normal outcome, you need to filter it out from your analysis. For example, if a friend of the CEO or a member of Board of Directors invests a big amount through one of your products, that should be removed from data. Typically used definition to identify outliers is multiple of standard definition (2x or 3x).
To put numbers on this example, let us say Sales size for your product varies between Rs. 100 – 10,000 and there are 500 such cases such that the average works out at Rs. 5,000. If there is one sale of Rs. 5,00,00,000, the average all of a sudden becomes ~ Rs. 1,05,000. If you don’t get this kind of Sale every month, you will end up concluding that average sales size has gone down.
Look at another example why looking at mean might be misleading:

Following practices would help you to make sure you are not drawing wrong inferences by looking at mean:
Look at the distribution and filter out any outliers from your analysis
Observe the skew of (mean/median) to figure out amount of skew coming from bigger or smaller values. Higher or lower the value (compared to 1), higher is the skew. In case of high skews, look at both mean and median before making and conclusions.
2. Comparing different sets of population, segment or cluster
The example at the start of the article is perfect example of this. This error can happen whenever you do not use non-random way to distribute the population. Whenever you do so, please make sure that populations are similar on all the key parameters.
How to avoid this bias / error?
It is best to stay away from comparing non-random distributed population. If you need to do so due to logistic & resource constraints, compare Pre vs. Post before you arrive at any conclusion.
3. Drawing inferences on thin data (and extrapolating it)
This happens when you want to bring out insights for every possible segment of data. In doing so, you end up with segments or clusters which have small population and the reads may not be statistically significant (or will contain a lot of noise). Following is another example of this kind of error:

How to avoid drawing inferences based on thin data?
Always plot confidence intervals to the values to calculate or extrapolate. This will give you a sense of whether the extrapolation is accurate or erroneous. Usual practice is to look at 90% confidence intervals before reaching any conclusion.
4. Wrong applications of the inferences
It is human tendency to generalize insights and findings. You extend or apply a learning based on different set of population or circumstances to a different set. Those insights may or may not be relevant. If your credit scoring model was based on a population coming from a particular channel (e.g. Branches), you can not apply it to a different set (e.g. Online). You will end up getting things wrong. Here is another example which brings out this error:

How to avoid wrong application of inferences?
Always check if you are extending your models to a population which it has not seen it past. If you are aware of any changes, continue to monitor the population on key variables (e.g. for credit scoring model, you need to monitor age, income, unemployment rate, credit file match % etc.)
If any of these distributions are different compared to the population on which you are basing your learnings, think again!
5. Correlation does not mean Causation
Correlation is a tool every analyst uses on a frequent basis. The biggest caution while using it is that it should not be treated as causation. If two events are happening close to each other, it is not necessary that they are happening because of one another. For example, if my age is increasing with time and my brother’s age is also increasing with time, it does not mean my age is driving his. Picture below shows another such example:

How to avoid this error?
Just treat correlation as correlation and not causation.
 
These were the 5 most common errors in my experience. The more you put your thought in spotting these errors, the better you become at it. Initially, I used to make these errors myself. Now, I can spot them effortlessly.
If you know of any other errors people make, please feel free to add your comments below.
Image credit: All cartoons taken from XKCD.com
If you like what you just read & want to continue your analytics learning, you can subscribe to our emails or like our facebook page
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"The art of structured thinking and analyzing","Kunal Jain",2013-06-24 21:31:00,2,"
							
										
						It took me 3 months to complete my first analytics project. If I would have worked on a similar project 6 months into the job, I would have completed it with in a month. Today, I can complete the same project in less than a week.
So what has changed during this time?
I have started thinking in structured manner
I have gained experience in best data management practices
My business thinking has evolved over time
While the last two points improve only with time, structured thinking can improve quickly through simple training and disciplined approach towards analysis.
But before we go forward, let me bring out the benefits from structured thinking through following graph:

Here is how to read this graph:
Red line in the graph shows how time to complete a project (in weeks) has come down with experience
With in each of three blocks (< 1 year; 1 – 3 year; 3+ years), the area of color shows the factor responsible for drop in time.
For example, during the first block, time required to complete the project comes down from 12+ weeks to 3 weeks and 75% of this drop is because of structured thinking.
As you can see, structured thinking is a a big differentiator between a good analyst and bad analyst. Not only this, you can not become a good analytics manager until you can put structure to complex and ambiguous problems. Hence, this post is aimed to help you progress on path of structured thinking.
What is structured thinking?
Structured thinking is a process of putting a framework to an unstructured problem. Having a structure not only helps an analyst understand the problem at a macro level, it also helps by identifying areas which require deeper understanding.
Why is it important and how can it help?
Without structure, an analyst is like a tourist with out a map. He might understand where he wants to go (or what he wants to solve), but he doesn’t know how to get there. He would not be able to judge which tools and vehicles he would need to reach the desired place.
How many times have you come across a situation when the entire work had to be re-done because a particular segment was not excluded from data? Or a segment was not included? Or just when you were about to finish the analysis, you come across a factor you did not think off before? All these are results of poor structured thinking.
Following are some results of poor structured thinking:
Multiple iterations of work 
Change of scope and work half way through
Longer turn around time
Difference in expectation between analyst and stakeholders
So how to enhance structured thinking?
Following are some of the best practices which I have learned over time. These practices have helped me and a lot of other people immensely to structure our thoughts and making sure we stay on track during our projects:
Create a Scope of Work document: This document helps by bringing all stakeholders on same page at the start of any project. A typical document can be any where between half a page to maximum of 2 pages. Following are typical sections which I keep in this document:
Background
Problem statement
Customers, Sponsor and stakeholders
In scope and out of scope areas (please note that both are equally important).
Reference to any work which has happened in past

Once you create this document, make sure you circulate it to all stakeholders and take their thought at this stage only. If their is any confusion or contradiction to the scope, sort it out now! There is hardly any point in going further, until you define the scope clearly.
Create presentation and lay out the analysis without touching the data: You will be surprised to see the power of this technique. It might sound counter-intuitive to people outside analytics industry, but this technique helps you remove any biases and makes sure you think of all the aspects before you zero in on any specific area. For example, if I want to understand why charge offs have increased suddenly in credit card portfolio over last month, I would lay it down in a structure similar to this:
Once this structure is exhaustive and complete, you can simply plug in the data for each set of variables and see what is the reason.
The beauty of this practice is that you can layout any problem by spending an hour on it. If you do this right, solving the problem would then be a simple walk through. If you don’t, there is a big risk that you might focus on only internal factors (that too only from a specific area).
Perform back of the envelope calculations: Once you have the entire structure laid out, perform quick and dirty back of the envelope calculations to see if some things should be analyzed first or can be removed. As long as they are 80% right, go ahead. For example, if charge offs have doubled with in a month, it is unlikely to be driven by unemployment (unless you are aware of huge scale layoffs in the economy). Similarly, if Credit limit increase only impacts 2% of your portfolio, the charge offs from this programme need to increase by 50x, if portfolio risk has doubled, which is unlikely unless you have given free Credit without looking at population.
Layout the data requirements and hypothesis before looking at what data is available:  If you layout these after looking at what is available, there is a high chance that you restrict your thinking to those variables and data points which may not bring out the best outcome.
Finally… practice, practice and practice: There is no point reading all this stuff, if you don’t apply it (too bad that I am saying it so late)! The more you apply it, the better you will become at it. Take any problem / business scenario, break it down on a piece of paper, see how things add up. How much profit or loss can you expect from a strategy change? What is the most critical factor? What assumptions need to hold out in order to make profits? Just apply structure to any problem you come across and see the fun. In matter of hours, you might know more than the  owner of the problem.
So, next time when you come across a problem to solve, don’t dive into the data. Put a structure around it, make sure you cover all the aspects, understand what is the most important factor, layout possible hypothesis and data requirements and then solve the problem. You will be surprised to see the reduction in time, if you follow these steps religiously.
As usual, if you know of any other practice, which has helped you think more structurally, do let me know.
If you like what you just read & want to continue your analytics learning, you can subscribe to our emails or like our facebook page
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"How to implement an analytics solution for a business problem?","Kunal Jain",2013-06-19 05:30:00,4,"
							
										
						During a panel discussion in Gartner Business Intelligence and Analytics Summit early this year in Barcelona, vendors estimated:
70% of Analytics projects fail to meet expectations
While the exact number is difficult to find and would vary significantly from organization to organization, poor implementation of analytics projects is definitely a huge challenge for Analytics leaders across the globe.

For an industry based on scientific and logical thinking, these kind of failure rates are appalling. More surprisingly, analytics community has not learnt from its mistakes and continues to see poor implementation project after project.
This post aims to bring out some of the common mistakes people make while solving problems through analytics and suggest a few ways to minimize these mistakes.
Common reasons for failure of Analytics projects due to analysts
Most of the analyst related failures arise because analysts have a tendency to assume similarity across projects. A predictive model which works wonders for one company can lead to disasters in other (if copied blindly). Similarly, each Organization has its own way of storing and dealing with data. Same status code could have different definition.  Each and every project should be thought through and planned independently. One solution fit all approach doesn’t work in analytics. Following are the top 3 errors which I think cause most of the failures:
Not planning for and spending time to understand business process: An analyst needs to spend a lot of time to make sure he understands the business processes and gaps before he can start working on an analytics solution. More often than not, process improvements might give you a better solution than applying analytics to a process you are not clear about. Ironically, people do not budget the required time to be spent on understanding business processes in their project planning. Have provided some good practices towards the second half of the article.
Unclear and non-specific requirement capturing: This reason stands out for the amount of failures it causes. If you are preparing the requirements document and are not clear about any aspect of the problem, ask out! If you do not clarify things now, you will come across surprises which will de-rail your project from timely completion. In one of the project for implementing a BI tool across an MNC (buying more than hundreds of licenses), the analyst spent less than a day, in total across all the departments and created the BRD (Business Requirement Document) on basis of which entire project pricing was arrived at. End result of the project – Shelved after putting in more than 9 months of effort of 5 member team.
Half baked thoughts on implementation: You may create the best of predictive models, but without a clear implementation in mind, these models are useless. A common mistake while building predictive models is to include variables which can not be controlled. For example, you find out that people who are Self Employed are more profitable and incentivize your Sales teams to bring more of these customers. What are the chances that people who were mentioning occupation as Professional or Salaried would start calling themselves Self employed? How do you avoid this to fool your model?
Common reasons for failure of Analytics projects due to stakeholders
Limited buy in from customers or stakeholders across levels: You need to ensure a complete buy in from a customer in order to make sure that your project stands a chance to achieve success. If the stakeholders are not convinced about the idea, it will reflect across as poor business understanding of analysts, analysts not being aware of strategic changes impacting their project etc. which will ultimately lead to project failure.
Not providing required time and inputs to the analyst: A lot of people would want analysts to work on their business problems, but they find it difficult to give them the required time due to their regular commitments. This becomes specially critical at key stages of project like scoping & creation of BRD (see next point for further details), hypothesis generation for predictive model, user testing for an application etc.
Hasty review of BRD (business requirement document) / scope document: You would be surprised to see the amount of failures which happen because of a poor BRD review. While creating a BRD might be the responsibility of an analyst, there would always be gaps which only a business person will find out. Unless a BRD is reviewed line by line, you can be rest assured that it will come back to hit you and cause a lot of re-work or will set you up for failure.

Guidelines to ensure successful implementation of analytics projects
These guidelines should minimize chances of failure for analytics project, but they can be applied in any project management role.
Plan, plan and plan: The more you plan upfront, higher are the chances of your success. Keep enough time to understand business process, capture business requirements, brainstorm various hypothesis and finally making sure that the project is implemented exactly in the fashion you had wanted.
Requirement gathering / Business Understanding stage:
Spend time with all stakeholders and understand their perspective. At the outset, meet and understand the requirements of the stakeholders. Often, different stakeholders might require different objectives to be fulfilled. Understanding perspective of various stakeholders helps you outline any differences at the start.
Spend at least a day experiencing the process. If it relates to a call center, go for call listening. Look at how the process works, is the calling based on dialler or manual? If the problem is pertaining to Sales, go for Sales calls.
Work on requirement gathering iteratively, don’t wait for that perfect document to be created. If you have a working version, discuss that with stakeholders and take their feedback.
Put in regular catch ups with stakeholders, ideally weekly
Attend relevant business meetings – May be 1 or 2 weekly business meetings. They provide you rich understanding of challenges which the group faces. Also, it comes in handy when you are thinking  of implementation
Review of BRD: As a thumb rule, a business owner should spend at least 30% of the time the analyst has spent creating this scope document.

Analytical modeling stage:
For predictive modeling, make sure that you have captured all business hypothesis. You should spend that extra hour with the stakeholders to make sure nothing is left in their mind. Another practice which I find useful at this stage is unrestrained thinking. Don’t restrain yourself by data which is available, look at what is available and what is not only after you have exhausted all possible hypothesis.
Think through about implementation at this stage: You need to be clear of project implementation at this stage. How would the data flow? What would the business owner need to change? If you make this change, would it influence customer behaviour? Can it fool your model?

Implementation stage:
Similar to start of the project, go back to business process and check whether project has gone live as you wanted. Look at the changes with minute details
Monitor the project intensely: For initial period, track things closely till the time you get comfort that things are going fine.
Update your stakeholders about your findings in weekly reviews.
If implementation falls apart, accept it and quickly move on to see how can it be corrected.

Hopefully, following these guidelines will help you deliver your analytics projects as per expectation. I have found them very useful while working on various analytics projects. If you have any practices, which you follow to ensure successful implementation of projects, do let me know.
Happy implementation!

If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our facebook page
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"What is Business Analytics and which tools are used for analysis?","Kunal Jain",2013-06-11 22:57:00,3,"
							
										
						Business Analytics has become a catch all word for any thing to do with data. 
So if you are new to this field and don’t understand what people refer to as “Business Analytics”, don’t worry!
Even after spending more than 6 years in this industry, there are times when it is difficult for me to understand the work a person has done by reading his CV.
Here is how an excerpt from a typical JD might look like:

Analyze, prepare reports and present to Leadership team on a defined frequency
or
Lead multiple analytical projects and business planning to assist Leadership team deliver business performance.

On one hand, this creates confusion in mind of person applying for a particular role. On the other hand, it leaves the selectors with a difficult role to understand and judge what a person has done in past.
Now, if I got this as description for one of the jobs I had applied to, I would be scared! Scared, not because I don’t know the subject, but because, these could mean anything. The work could refer to preparing basic reports at a junior level to performing multi-variate deep dives on various subjects.
So, what do you do when you are in such a situation?
Well, the first thing you should do is understand Business Analytics spectrum. Once you have understood it, ask which part of the spectrum, the role applies to and then decide whether it suits your skills or not.
Following is a good representation of this spectrum:

Let me explain each of these areas in a bit more detail.
Reporting – Answer to What happened?
The domain of Analytics starts from answering a simple question – What happened? This activity is typically known as reporting. These are typically the MIS which people want to receive first thing in the morning. It is a snapshot of what has happened. Following is an example of how a typical report might look like:

Tools used in reporting:
Majority of elementary reporting happens on MS Excel across the globe. More evolved Organizations might pull the data through databases using tools like SQL, MS Access or Oracle. But typically, the dissemination of reports happens through Excel.
Skills required for reporting:
MS excel
Business understanding
Ability to perform monotonous task with diligence
Detective Analysis – Answer to Why did it happen?
Detective Analysis starts where reporting ends. You start looking for reasons for unexpected changes. Typical  problems you work on are “Why did the Sales drop in last 2 months?” or “Why did the latest campaign under-perform or over-perform?”. In order to find out answers to these questions, you look at past trends or you look at distribution changes to find out the reasons for the changes. However, all of this is backward looking.
Some of these insights, which you find out after looking at backward analysis can be used for business planning, but the purpose of analysis is typically to find out what has worked and what has not.
Tools used in detective analysis:
Typically used tools are MS excel, MS Access, Minitab, R (basic regression). You tend to use advanced Excel and Pivot tables while dealing with these problems and typically creating time series graphs helps a lot.
Skills required for detective analysis:
Structured thinking
MS Access, Excel, basic regression
Business understanding
Dashboards – Answer to What’s happening now?
Dashboard is an Organized and well presented summary of key business metrics. They are usually interactive so that the user can find out the exact information he is looking for. Dashboard, in ideal state should provide real time information about performance. Following is an example of how a dashboard might look like:

The whole science of creating data model, dashboards and reports based on this data is also known as “Business Intelligence“.
Tools used for creating dashboards:
For limited size of data, dashboards can be made using Advanced excel. But, typically Organizations use more advanced tools for creation and dissemination of tools. Business Objects, Qlikview, Hyperion are names of some such softwares.
Skills required for creating dashboards:
Strong structured thinking: The person will need to create the entire architecture and data model
Business Understanding: If you don’t understand what you want to represent, God help you!
 Predictive Modeling - Answer to What is likely to happen?
This is where you take all your historical trends and information and apply it to predict the future. You try and predict customer behaviour based on past information. Please note that there is a fine difference in forecasting and predictive modeling. Forecasting is typically done at aggregate level, where as predictive modeling is typically done at a customer / instance level
Tools used for Predictive modeling:
SAS has the highest market share among tools used for predictive modeling followed by SPSS, R, Matlab.
Skills required for Predictive modeling:
Strong structured thinking
Business Understanding
Problem Solving
Big data - Answer to What can happen, given the behaviour of the community?
Imagine applying predictive modeling with a microscope in hand. What if you can store, analyze and make sense out of every information about the customer. What kind of social media community he is attached to? What kind of searches is he performing? Big data problems arise when data has grown on all three Vs (Volume, Velocity and Variety). You need data scientists to mine this data.
Tools used in Big data:
This is a very dynamic domain right now. A tool which used to be market leader 6 months back is no longer the best. Hence, it is difficult to pin down specific tools. These tools typically work on Hadoop to store the data.
Skills required for harnessing big data:
Strong structured thinking
Advanced Data Architecture knowledge
Ability to work with unstructured data
So, now that you understand the Analytics spectrum, if you come across a role which is not clear to you, please spend the necessary time understanding which domain does it refer to and does it fit right with what you want to achieve.
If you have come across this confusion on understanding “Business Analytics“, this article should have helped you. In case there are any further confusion, do let me know.
If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our facebook page
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Limitations of Pre vs. Post analysis and Importance of testing","Kunal Jain",2013-06-05 21:38:00,4,"
							
										
						During a recent interview with an analyst working for a big multi-national retail store chain, I asked:
How do you check if promotion of a particular product had cannibalization effect on future Sales of the product or on other similar products?
The response I got was
By simply comparing Pre promotion Sales to Post promotion Sales.
This answer might fetch a good score quite a few Organizations and you might be surprised to see the percentage of analysts who give this answer. However, in any analytically evolved Organization, it will be considered incomplete, if not wrong!
In case, you think that this answer is right, think again!
This simplistic analysis works in steady state when there is no impact due to changes is external factors or factors you can not control for. However, this is hardly the case in any practical application. In today’s world, external factors can change overnight and the cost of making wrong conclusions can be hefty.
In this particular example, imagine what would happen if one of your competitor launches a better promotion for same product which causes your Sales to shrink by 50% in case of no promotion and 25% in case you are also running promotion. If you just do a simple pre vs. post comparison, you may end up concluding that the promotion caused your Sales to drop by 25%, while the reality is that it actually increased it by 50% (25% absolute increase on a base of 50%).

Challenges with simple Pre vs. Post analysis?
There are bunch of practical challenges you face in doing a simple Pre vs. Post comparison. How do you account for regulatory changes happening year on year (making it difficult to compare against base of previous years)? Or for Seasonal products (e.g. Ice-cream), how do adjust for sudden heat waves or temperature drops? What do you do when the entire Industry is expanding or shrinking?
In each of these scenarios, a Pre vs. Post analysis might lead you to a wrong conclusion. So, how do you arrive at effectiveness of Sales promotion?
A simple answer to this is performing tests or experiments in order to size the impact more accurately.
While testing is a subject which deserves multiple posts, I plan to cover simplest form of testing in this post. The simplest form of testing is popularly known as A / B testing or Champion / Challenger testing. This is your solution to the kinds of problems mentioned above in this post.
So, what is A / B testing?
Conceptually, it is a simple solution, where you change only the parameter / factor you want to test and run two alternates in parallel to find out which one works better. For example, to answer my initial question, we will find 2 sets of stores similar to each other, run promotion in one set and don’t run promotion in other set of stores. Once we run the test for required duration (to achieve statistical significance), we compare Pre vs. Post for both the sets of stores and find out if promotion has actually benefited the store or not?
The reading you get now will adjust itself for all external changes as long as the 2 sets of stores are similar. Isn’t it elegant?

I understand A / B testing, where else can I use it?
Applications of testing are limited by your thinking. It can help you in situations where coming out with a neat analytical solution is difficult or impossible. Following are just a few such examples:
When you are in a situation where there is no right or wrong answer. How do you decide whether application form should be one page or multiple page? Should you suggest a particular credit card to your customers or let them select it themselves? Should you call a customer with 30 day default once a day or twice a day? All these can be run as tests to find out what works and what does not.
When you want to measure impact of making a change. How does your application conversion change with change in layout of website? How does a new credit card collection strategy change the collection rate or default rate?
It sounds simpler than it actually is
In case you have started thinking that this sounds easy, let me warn you. It sounds easier than it actually is. Following are some precautions you need to keep in mind while doing these tests / experiments:
The two scenarios you test should be different only on the parameter you want to test. For example, if you want to test Subject line of a communication to your customers, you need to make sure that only the subject line changes. Creative, content, delivery address, delivery timing across the two sets of population should remain same. If any of these change, then it might become difficult to understand which factor is responsible for what change. If you want to test multiple factors, i’ll discuss this in one of the future posts.
The two sets of population should be chosen randomly. There should be no bias in the samples chosen. Normally it is a good idea to check distributions on 2 – 3 key parameters before rolling out the tests. If the test is to check whether making a particular offer to customers increases their chances of charge-offs, it makes sense to check whether the distribution of credit scores and vintage of the customers in two sets is similar. Again, if you don’t check for these distributions, you might end up drawing wrong conclusions.
Make sure you have entire control on how are the two sets being treated. This might become a huge challenge especially if you are dealing with a cross-functional team. In one of the tests designed by a friend, he wanted to size the impact of a tele-calling intervention. So we designed a no-call test where a small group of customers were not called when their collection was due. The business unit responsible for collections took a decision to send additional communication to these customers with an intention to maximize collection without informing the analysts. Result - the test ended up under-estimating the impact of tele-calling and might have led us to believe that tele-calling has limited impact which was not the case!
Monitor / Follow the test intensely for first few days. A lot of Organization may not have capability / systems to run multiple options in parallel. These arrangements are typically made outside the system or require a lot of change. Once the test goes live, you need to make sure it is actually what you intended and not otherwise. A healthy practice here is to include people with in project group in various sets and see if things are as per expectation.
What next?
 Hopefully this post gives you a good overview of limitations of simple pre vs. post analysis and advantages of performing tests. Some time in future, I’ll cover multi-variate tests and Design of experiments. I’ll also share the best practices in designing a test.
For now, do let me know if you think you can apply this elegant concept in your respective areas and how can you benefit from it.
Image 2 credit: Maxymiser.com

If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our facebook page
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"How to apply web analytics for e-Commerce websites?","Kunal Jain",2013-05-30 19:49:00,5,"
							
										
						E-commerce is a dynamic and developing industry and so is web analytics. Mix the two and you can expect a world dynamic to its core. In such a dynamic and competitive industry, analytics is set to become a differentiator in how you get those extra customers and maximize their life time value.
Ironically, I get asked “How is web analytics applied in an e-Commerce website?” too frequently. At times, I come across people with in analytics industry ignorant about developments and applications of web analytics. This post aims to provide an overview of how web analytics can help an e-Commerce websites. Following are some benefits which you can realize by use of web analytics:
Deep understanding of prospect behavior resulting in higher customer conversion
Happier customers as they are served with what they would be looking for
Higher profit margins as you get higher share of wallet from your customers
So, how do we achieve these benefits? Simply, keep reading and you’ll get a good overview.
Following diagram shows a typical customer journey framework:

Applications of web analytics on e-Commerce sites can be divided in three parts:
Bringing more prospects through various acquisition channels
Converting more prospects to customers
Maximizing life time value of customers
Lets look at how web analytics can help in each of these areas:
Bringing more prospects to the website 
How do you get more prospects to your site in a cost efficient manner? Which channels are more efficient in bringing customers who are more likely to make a purchase? Which channels bring customers with higher life time values?
These are some questions which can be answered through web analytics. Following are some more examples  where web analytics can help:
Compare performance of various banner placements and creatives. Which ones have higher bounce rates? Which banner animation brings more relevant traffic?
Which search terms bring traffic more likely to convert? Are there any low competition niche keywords which can bring you high conversion traffic?
Which subject line in an Email campaign has higher open rate? Higher click through?
Converting more prospects to customers
Within this domain, the first area where web analytics helps is in understanding your prospects and customers. Do they like interactive site or they prefer to have a simple and directed journey? Do they expect multiple options or they want to  just go ahead with the most popular options? What kind of advertisements are non-intrusive? Understanding  these critical and specific questions ultimately help in converting more prospects into customers.
The second area where web analytics helps is understanding the journey of your customers. Are there particular pages or fields where a lot of people drop off? Following is a screenshot of how this journey page typically looks:

This particular screenshot tells that there is a huge drop which happens between home page and the next pages. It also tells which are the routes which customers take more often and which are the pages where customers end up going to and fro. Further, you can create different journeys for different traffic sources. Typically, prospects from display ads will have higher conversion chances if you ask bare minimum information from them. On the other hand, if the customer is coming from Search, they would expect more product information before purchasing.
Third area where web analytics helps is to understand overlap of various traffic sources. How many customers are influenced by your affiliates in order to convert prospects to customers? Do you give them due credit? Are you sure you don’t pay them for Sales where they had limited role to play?
Maximizing life time value of customers
Once a person has become your customer, you have some more information about him. Here are some examples:
What kind of city and geographical locality did he come from?
What mode of payment did he use?
Which Credit card did he use?
What product did the person buy?
Is he likely to be a young person? Professional middle aged Salaried person?
All this information can help you to pitch the right product for the customer, next time he visits your site. Here are some questions you can answer basis the information you have gathered till now:
What kind of products can you cross-sell or up-sell to these customers?
If a customer has not responded to a promotion in last ten visits, there is a little chance he would respond now. 
If a customer bought a printer 6 months back, can you offer him a ink cartridge now? Or may be a discount on purchasing multiple cartridges?
All these are ideas and areas where web analytics can help a e-Commerce website. Each of these areas deserve  more detailed posts in future. Hopefully this post would have helped with various applications of web analytics.
In case you know of some more interesting and exciting applications of web analytics, please feel free to share.

If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our facebook page
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"How to identify a good (and bad) Business Analyst?","Kunal Jain",2013-05-27 19:46:00,2,"
							
										
						Over last 6 years, I have come across more than hundreds of analysts and have conducted almost equal number of interviews. Over this time, I have developed a knack of differentiating best analysts from good and good analysts from bad. If you face this challenge regularly, this post might help you.
 
So how do you differentiate between good and bad analysts?

 
Thankfully, it’s not that difficult. I have put a framework around how to judge an analyst. You can use the same to make your life simpler. This framework has it’s genesis in hiring guidelines at Capital One. I have modified it to adopt it in Indian Analytics industry.
 
Structured thinking 
This is the most important attribute that distinguishes a good analyst from bad. This attribute is not only required to be a successful analyst, it becomes absolutely critical for a person managing Business Analysts.
So what is Structured thinking?
Structured thinking is a process of putting a framework to an unstructured problem. Having a structure not only helps an analyst understand the problem at a macro level, it also helps by identifying areas which require deeper understanding.
How do I test for structured thinking?
Typically I test this by throwing a open business problem at an analyst and then observing closely how he / she is solving it.
An example is asking a question like: “You have been appointed as CEO of a loss making restaurant at Delhi / London Airport and you are expected to join the company in a week. What would you want to do as a CEO of the company as soon as you join them?”
If the person lays out a nice structure about where the problems could be, he has already ticked one box. If he starts giving you answers out of his hat (e.g. I would be looking at what marketing are we doing?), you should consider it as a red flag. He will not be able to sail through the world of Analytics.
Business understanding and problem solving 
There is a reason why Business Analysts are called so and not just Analysts. Until a person understands what he is trying to solve and the business owners are confident that he can solve problems in meaningful manner, he is a dead analyst.
So, how do you test for business understanding?
For an experienced analyst, I typically start judging this by asking about business context for the projects he might have worked on. If he can explain that clearly, it’s a good start. If he can’t, you can almost make your hiring decision here. Next, you can look at the answers a person gives in response to question asked for judging structured thinking. If he gives answers based on numbers only, you need to probe him further. He needs to put a business thinking hat and provide some out of box suggestions.
Attention to details 
If a person is not detail oriented, he can never be a good analyst. Every analyst should have the ability to understand business at high level, but he should be able to get down to nuts and bolts of all the levers you might have.
So how do you judge for attention to details?
Start by looking at the CV of a person, has he spent time choosing words carefully? Has he mentioned impact of the projects he might have worked on?
For an experienced analyst, probe on the projects he might have worked on before. Did he consider all the aspects and possibilities? How much time does take to explain his previous projects?
Another way to judge it is by asking the candidate to  a guess estimate, something like “Estimate the number of smartphones used in India” and looking at how the candidate answers them. How many factors does he consider to come up with answers? How many segments does he consider to  arrive at sizing? These aspects should give you a good read on how detail oriented a person is.
Ability to triangulate numbers & do back of the envelope calculations
While the first three characteristics help you identify a better than average analyst, this characteristic and the next differentiates best of analysts from good analyst. This is an activity I love to do and something I know every good analyst loves. This is the ability to set up equations on page and then do back of the envelope calculations to answer 80% of questions without touching any excel / calculator or laptop. It is also the ability to arrive at a number through various sources and then validating them.
How to judge ability to triangulate numbers and do back of the envelope calculations?
Guess estimate comes to your rescue here. Just ask the candidate to perform the guess estimate on a paper and then ask him to validate the number through an alternate approach.
Communication skills – Ability to tell stories based on numbers
Any analyst is only as good as he can communicate. If a person can not take the complex world of numbers and create a meaningful story out of it, he will always be looked upon as a nerd. He can be a good analyst, but not the best one. Ability to create a story and present it almost has an equal, if not higher influence on your customers and hence increases the chances of success of any analytics project.
How do you judge communication skills?
You can get a sense on this through the entire interview. If this is very critical to the role you are evaluating for, you can provide datasets in excel and ask the person to present some open ended questions
Hopefully, this framework will help you for any analyst hiring in future. In case you have some suggestions, do let me know.
 
Image Source: 123rf.com
If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our facebook page
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Creating a simple and effective Sales dashboard (with Qlikview) – Part 2","Kunal Jain",2013-05-19 13:19:00,1,"
							
										
						In my last post, I discussed how a simple dashboard can provide information effectively. I will continue from where I left and go through the steps involved in creating the dashboard.
Step 1: Import data from source 
In case the data source is excel (which is the case here), go to File -> Edit Script or Ctrl + E. Once the script page opens, go to Table Files.

In case you want to import data through databases, you can connect through ODBC or OLE DB connectivity provided on left of “Table Files”.
Once you select the file from desired location, following code will be added on the script window:

LOAD City, 
     Region, 
     business_sourced, 
     rejections
FROM
C:\Users\Kunal\Desktop\temp_qlikview.xlsx
(ooxml, embedded labels, table is Sheet1);

Once the script is written with correct location, Save the script and Reload it. Once the script gets reloaded, Qlikview will open up a dialog box showing list of fields which have been loaded in Qlikview. Check that all the fields you wanted (City, Region, business_sourced and Rejections) are mentioned in this window. Close this window after checking the fields.
Step 2: Selecting the chart 
Next, right click on Main sheet and select New sheet object -> Chart….  In the next screen, select block chart from the list of charts, add any Windows title you want to add (Business Volume)  and click on Next. 
Step 3: Create a location based drilldown group
Select Edit Groups on the next screen. A window for Groups will open up. In this window, select New. On the next Window which opens, enter Group Name (Location Hierarchy), Used Fields (in order of hierarchy) and the type of group (Drill-down). 

Click on OK a couple of times to come out to Dimensions window. Add Location Hierarchy in Used Dimensions.
Step 3: Add expressions and background colour based on second dimension
When you click Next, window for Edit Expression opens up. This is the expression which determines sizes of box in your chart. Enter sum(business_sourced) here.
Once you press OK, Window for expressions opens up, with definition provided in last window. Add desired Label (Business Volume) and expand the parameters (as on next screen).

Next, you can select Background color and click on Definition. In this window, you can define the parameter for defining colors of the boxes in your chart. Insert the following code in the expression tab:

if(sum(rejections)/sum(business_sourced) < 0.06, green(), red())

This expression would color the box red when there are rejections more than 6%, otherwise they will be colored green.
If you want to add gradients of green and red color, the code can be changed to something like this:

if(sum(rejections)/sum(business_sourced) < 0.06, 
colormix1(sum(rejections)/sum(business_sourced)*10,green(), lightgreen()),
colormix1(sum(rejections)/sum(business_sourced)*5,lightred(), red())) 

This expression would color the boxes in gradient between green and light green or red and light red. Next, you can click Finish to see your chart.
This simple tutorial creates an effective dashboard with drill down and colors showing quality of business. Hope you would have found the tutorial helpful.
In case of any questions, please feel free to reach out to me. Also, let me know in case you apply these ideas to some other location.
 
If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our facebook page
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Creating a simple and effective Sales dashboard (with Qlikview)","Kunal Jain",2013-05-15 04:44:00,4,"
							
										
						Appropriate space allocation and compelling visuals which convey business insights in meaningful manner are key to creating a good dashboard.
The dashboard / chart which I plan to cover in today’s post is one example of how you can do this effectively. I am creating this dashboard for a Sales led business, but that does not stop you from applying in any other field. The core idea can be applied to any area where you want to analyze outcome in multi-dimension.
Let us start with an example, where a Sales team is divided into Regions (4), which are further divided in four Zones (16). Each of these zones will contain numerous cities.
In excel era, Sales MIS would typically look something like:

Next, you can either create multiple sheets or tables in same sheet to show comparison at a zonal or city level. Alternatively, we can also add Pivot tables to the mix or group rows. However, none of these features give you a natural drill down (barring Pivot tables to some extent). If you use pivots to create a drill down, you need your customer to be comfortable with pivot tables, which may or may not be the case for business users.
If you are slightly more evolved in Business Segmentation and Analytics thought process, we can summarize the performance on a grid like this (remember, Low rejection rate => Better quality):

However, even this visualization also suffers from same problems of having limited drill-downs. This is where Qlikview comes in handy. One of the ways which I have found simple and powerful is to use box charts in Qlikview.  Following are a few screenshots of how the dashboard might look like with natural capabilities of drill ups and drill downs.
Please note that the screenshots are for just this chart object only and not the entire dashboard.

This screenshot tells the summary at Regional level, with size of box showing the volume of business and Colour of box showing the quality of business. The chart below shows what happens as soon as you click on North.

A glance at this chart can tell you:
North sources 29.67% of business, but quality is below average.
Within North, 55.56% of business comes from Delhi, but is of very poor quality.
On the other hand, there are some relatively smaller markets in North (Chandigarh, Faridabad & Gurgaon), which source good quality of business.
This is a simple example where we have brought out some key information with help of simple dashboard. In the next post, I plan to post a step by step guide of creating this dashboard and enhancing the same.
Do you know of any other application of this simple concept? If yes, please feel free to add them below or to reach out to me.
 
If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our facebook page
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Joining / Merging in SAS – alternate approaches (including really efficient ones!)","Kunal Jain",2013-05-05 14:33:00,1,"
							
										
						One of the most common operation for any analyst is merging datasets. As per my estimate, an analyst spends at least 10 – 20% of his productive time joining and merging datasets. If you spend so much time doing joins / merges, it is extremely critical that you join datasets in most efficient manner. This is the thought behind this post.
Traditionally, databases have been designed in a manner where tables capture details of individual functional area.
Example below shows two tables, one capturing patient details in a clinic (from one time registrations) and second table showing their appointment details.
Example of data join
In order to analyze things like:
Which customer has walked in how many times in last month?
Which kind of customers have walked in more last month?
What are the common reasons for people walking in?
we need to join the two tables.
 
I’ll cover various ways in which you can do this in SAS:
1. Sort / sort / Merge:
This is the most common approach used in SAS. In order to use data step command, we need to sort the datasets first and then merge using the common key:

proc sort data=patient_details; by pat_id;
proc sort data=appointment_details; by pat_id;

data analysis_set;
 merge patient_details (in=a) appointment_details (in=b);
 by pat_id;
/* note by variables are in the same order as sort by */
if a and b; 

/* Control statement, other options: if a; if b; if not a; if not b;*/
The control statement defines the kind of merge. By specifying “if a and b”, values present in both the tables will be picked.
2. PROC SQL:
If you are used to writing SQL, PROC SQL might be the easiest way to learn joins in SAS

PROC SQL;
      CREATE TABLE analysis_set0 AS
                       SELECT a.*, b.*
                       FROM patient_details a
                       INNER JOIN /* control statement*/
                       /*other options LEFT JOIN, RIGHT JOIN, OUTER JOIN*/
                       appointment_details b
                       ON a.pat_id=b.pat_id;
      QUIT;
 RUN;

3. PROC FORMAT:
This is one of the latest ways I have learnt, but the most efficient one. Using this method, we convert the smaller file into a format.

DATA format1;
           SET patient_details (keep = PAT_ID);
           fmtname = '$pat_format';
           label = '*';
           RENAME pat_id=start;
RUN;
PROC SORT data=format1 nodupkey; 
           by pat_id; 
RUN;
PROC FORMAT CNTLIN=format1;
RUN;

The first step creates a dataset format1 from patient_details. PROC FORMAT then converts it into a format. Finally we use

DATA analysis_set;
     SET appointment_details;
     if PUT(pat_id,$pat_format.) = '*';
RUN;

This way to join datasets typically takes 30 – 40% lower computation time compared to the two approaches mentioned above.
Since this might look advanced SAS, I will devote one more post explaining formats in more details.
In the meanwhile, if you know of any other way to join tables, please let me know.
 
If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our facebook page
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Basics of Predictive modeling","Kunal Jain",2013-04-28 11:25:00,1,"
							
										
						 
Imagine how the world would change when any advertisement you receive is only about a product you are interested in. How beautiful it would be to receive information only about relevant products? How efficient would it be when you get all the required grocery items in first aisle? How much can mankind gain by being able to predict your diseases by looking at historical medical record and current symptoms?
All of this can be done by using power of predictive analytics. Many companies are already using this and becoming better and sharper with their targeting. They are able to get more than 100% response uplift from their marketing campaigns by predicting the need of customers and communicating with relevant products only.
So what is Predictive Analytics and how can it help?
According to Gartner:
Predictive modeling is a commonly used statistical technique to predict future behavior. Predictive modeling solutions are a form of data-mining technology that works by analyzing historical and current data and generating a model to help predict future outcomes
Simply put, predictive analytics uses past trends and applies them to future. For example, if a customer purchases a smart phone from a e-commerce website, he might be interested in it’s accessories immediately. He might be a potential customer for phone battery a few years down the line. Currently, chances of him buying accessory of a competitor smartphone are relatively bleak.
While the example might sound simple, imagine doing this for thousands of categories you might be selling. With in those thousands of categories, there might be multiple options (hundreds of covers, pouches, stylus…). Further, even if you have a thousand visitors every day (small number of many e-retailers), predicting the next purchase without data based decisioning for these customers might become impossible.
This is exactly where predictive analytics will come to your help (remember Amazon helping you out with, You might also like….).
I understand how predictive analytics can help, what do I do next?
If you are a business owner who wants to harness business analytics, you need to setup an Analytics team. I’ll cover details of setting this up sometime later. This post is for people wanting to learn the art of Predictive Analytics.
Following is a typical life cycle of building predictive models:
Steps to build a predictive model
The first step in any predictive model is to collate data from various sources. This can be data you own about your customer (like pages visited in past, products purchased in past), or data which the customer has provided (e.g. Address, Name, Age etc.).
This data needs to be cleaned and arranged in a structure so that it can be analyzed easily. This structure needs to be in sync with various business hypothesis. For example, if business hypothesis is that particular age / gender group may have higher likelihood to purchase certain set of products, Age and  Gender needs to be attributed at customer level.
Once these data sets are ready, we then use various predictive modeling techniques and business understanding to come out with various business insights (nuggets of gold). These insights can then be used in marketing / web site layout to increase efficiency.
In one of the future posts, we will go through these steps using a data set and case study to bring out these aspects.
In the meanwhile, if you have any examples where predictive modeling has helped or a business problem where predictive modeling can help, please let me know.

If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our facebook page
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Welcome to Analytics Vidhya!","Kunal Jain",2013-04-20 13:21:00,7,"
							
										
						
Welcome to Analytics Vidhya!
For those of you, who are wondering what is “Analytics Vidhya”, “Analytics” can be defined as the science of extracting insights from raw data. The spectrum of analytics starts from capturing data and evolves into using insights / trends from this data to make informed decisions. “Vidhya” on the other hand is a Sanskrit noun meaning “Knowledge” or “Clarity on a subject”. Knowledge, which has been gained through reading literature or through self practice / experimentation.
Through this blog, I want to create a passionate community, which dedicates itself in study of Analytics. I share my learning and tips on Analytics through this blog.
Why study Analytics? 
With growing data and complexity, data analytics will become a competitive advantage for companies doing it well.  Have you ever wondered how social networking sites like Facebook and Linkedin can suggest you friends in near time? How can Google use its search query data to identify areas under pandemic influence? How can a grocery store predict pregnancy of its customer even before it was known to her family?
All of these are examples of analytics at work in real time. And this is just the start. With increasing usage of mobile devices and digitization across the globe, advantages of investing in cutting edge analytics are unlimited.
According to Harvard Business Review (October 2012 edition), job of a data scientist is the sexiest job of 21st century. If you are still not convinced, look at some of the trends on Business Analytics related keywords (as per Google Trends):


How can this blog help? 
While the need for analytics experts is clear, the shortage is shocking!
According to the McKinsey Global Institute (In a May 2011 report): “By 2018, the United States alone could face a shortage of 140,000 to 190,000 people with deep analytical skills as well as 1.5 million managers and analysts with the know-how to use the analysis of big data to make effective decisions.”
Imagine what would be the number across the globe…
While this blog would not be able to cover shortfall of this magnitude, it can certainly provide resources for people willing to learn the subject.
What can you find on this blog?
I aim to provide as many resources as possible for learning analytics. These resources include:
Training and tutorials: Stuff to get you going
Tips and tricks related to Business Analytics and Business Intelligence tools
Case studies: Case studies of problems and their analytical solutions
Interviews of Business Analytics & Business Intelligence leaders
Who am I?
I am Kunal Jain, a post graduate from IIT Bomaby in Aerospace Engineering. I have spent more than 6 years in field of Business Analytics. My work experience ranges from mature markets like UK to a developing market like India. Over last 3 years, I have been instrumental in setting up a Business Analytics & Intelligence team in a multi national Insurance company. During these 6 years, I have worked with tools like SAS, SPSS, Qlikview, R and Matlab. For any queries ir questions, please feel free to reach out to me on kunal.jain@analyticsvidhya.com
Image credit: 123rf.com
If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our facebook page
Share this:PocketGoogle+Kunal Jain

	Related
			
						
						
		"
"Understanding and analyzing the hidden structures of unstructured dataset","Tavish Srivastava",2014-08-27 19:41:00,4,"
							
										
						The key to using unstructured data set is to identify the hidden structures in the data set.

This enables us to convert it to a structured and more usable format.In previous article (previous article on text mining ) we discussed the framework to use unstructured data set in predictive or descriptive modelling. In this article we will talk in more details to understand the data structure and clean unstructured text to make it usable for the modelling exercise. We will be using the same business problem as discussed in last article to understand these procedures.
Business Problem
You are the owner of Metrro cash n carry. Metrro has a tie up with Barcllays bank to launch co-branded cards. Metrro and Barcllay have recently entered into an agreement to share transactions data. Barcllays will share all transaction data done on their credit card on any retail store. Metrro will share all transaction done by any credit card on their stores. You wish to use this data to track where are your high value customers shopping other than Metrro.
To do this you need to fetch out information from the free transactions text available on Barcllays transaction data. For instance, a transaction with free text “Payment made to Messy” should be tagged as transaction made to the retail store “Messy”. Once we have the tags of retail store and the frequency of transactions at these stores for Metrro high value customers, you can analyze the reason of this customer outflow by comparing services between Metrro and the other retail store.
Understanding the dataset
Let us first look at the raw data to build a framework for data cleaning. Following are sample transactions on which we need to work on :
Paymt made to : Messy 230023929#21 Barcllay
Transactn made to : Big Bazaar 42323#2322 Barcllay
Pay to messy : 342343#2434 Barcllay

Messy bill pay 32344#24324 Barcllay

Let us observe the data carefully to understand what information can be derived out of this data set.
An Action word like “payment”, “Paymt” , “Transactn” is present in every transaction. It is possible that the word “pay” and “transact” refers to different modes of payments like Credit Card payment or cash card  payment.
The word in the end of every transaction is common. This should be the name of card used.
Every transactions has a name of the vendor. However, this name is both in small and capital letters.
There is number code in every transactions. We can comfortably ignore this code or derive out very meaningful information from this code. This code can possibly be the name of the area where the store is present, some kind of combination with the date of purchase or the customer code. If we are able to decode these numbers, we possibly will get to the next level of analysis. For instance, if we can find the area of transaction, we can do an area level analysis. Or say these codes caters to product family, and hence can be used to optimize our services.
Cleaning the dataset
Cleaning text data on R is extremely easy. For this analysis we will not be using the numbers at the end of the transactions. But in case you are to make a strong analysis, this is something you should definitely explore. In this dataset, we need to make following adjustments :
Remove the numbers
Remove the special character “#”
Remove common words like “to” , “is” etc.
Remove the common term “Barcllay” from the end of every sentence
Remove Punctuation marks
Given our understanding of data, step 2& 5  , 3 & 4 can be combined to avoid extra efforts. In step 2, we simply need to remove a single character “#”, which is automatically done in R while removing other punctuations . We will combine the words in step 3 and step 4, and remove them together. You can use the following code to clean the data set. Once we have the clean data set, we will convert it into a term document matrix. You can use the following codes for this exercise :

 
> library(tm) 
> myCorpus <- Corpus(VectorSource(a)) 
> inspect(myCorpus)
 

<<VCorpus (documents: 4, metadata (corpus/indexed): 0/0)>>
[[1]] <<PlainTextDocument (metadata: 7)>>
Paymt made to Messy 230023929#21 Barcllay
[[2]] <<PlainTextDocument (metadata: 7)>>
Transactn made to Big Bazaar 42323#2322 Barcllay
[[3]] <<PlainTextDocument (metadata: 7)>>
Pay to messy 342343#2434 Barcllay
[[4]] <<PlainTextDocument (metadata: 7)>>
messy bill pay 32344#24324 Barcllay

 


> # remove punctuation 
> myCorpus <- tm_map(myCorpus, removePunctuation) 
> inspect(myCorpus)
<<VCorpus (documents: 4, metadata (corpus/indexed): 0/0)>>
[[1]] <<PlainTextDocument (metadata: 7)>>
Paymt made to Messy 23002392921 Barcllay
[[2]] <<PlainTextDocument (metadata: 7)>>
Transactn made to Big Bazaar 423232322 Barcllay
[[3]] <<PlainTextDocument (metadata: 7)>>
Pay to messy 3423432434 Barcllay
[[4]] <<PlainTextDocument (metadata: 7)>>
messy bill pay 3234424324 Barcllay



> # remove numbers 
> myCorpus <- tm_map(myCorpus, removeNumbers) 
> inspect(myCorpus)
<<VCorpus (documents: 4, metadata (corpus/indexed): 0/0)>>
[[1]] <<PlainTextDocument (metadata: 7)>>
Paymt made to Messy Barcllay
[[2]] <<PlainTextDocument (metadata: 7)>>
Transactn made to Big Bazaar Barcllay
[[3]] <<PlainTextDocument (metadata: 7)>>
Pay to messy Barcllay
[[4]] <<PlainTextDocument (metadata: 7)>>
messy bill pay Barcllay



> # remove stopwords 
> # Add required words to the list 
> myStopwords <- c(stopwords(‘english’), “Barcllay”) 
> myCorpus <- tm_map(myCorpus, removeWords, myStopwords) 
> inspect(myCorpus)
<<VCorpus (documents: 4, metadata (corpus/indexed): 0/0)>>
[[1]] <<PlainTextDocument (metadata: 7)>>
Paymt made Messy
[[2]] <<PlainTextDocument (metadata: 7)>>
Transactn made Big Bazaar
[[3]] <<PlainTextDocument (metadata: 7)>>
Pay messy
[[4]] <<PlainTextDocument (metadata: 7)>>
messy bill pay



> myDtm <- TermDocumentMatrix(myCorpus, control = list(minWordLength = 1)) 
> inspect(myDtm)

<<TermDocumentMatrix (terms: 8, documents: 4)>>
Non-/sparse entries: 12/20
Sparsity : 62% Maximal term length: 9 Weighting : term frequency (tf)



           Docs
Terms     1 2 3 4
bazaar    0 1 0 0
big       0 1 0 0
bill      0 0 0 1
made      1 1 0 0
messy     1 0 1 1
pay       0 0 1 1
paymt     1 0 0 0
transactn 0 1 0 0
 

End Notes
Cleaning data sets is a very crucial step in any kind of data mining. However, it is many times more important while dealing with unstructured data sets. Understanding the data and cleaning the data consumes the maximum time of any text mining analysis. In the next article we will talk about creating a dictionary manually. This becomes important when we are doing a niche analysis for which ready made dictionary is either not available or very expensive.
Have you done text mining before? If you did, what other cleaning steps did you leverage? What tool do you think is most suitable for doing a niche kind of text mining like transactions analysis or behavioral analysis? Did you find the article useful? Did this article solve any of your existing dilemma?
If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.
 
Share this:Pocket

	Related
			
						
						
		"
"Step by step guide to extract inforation from unstructured data","Tavish Srivastava",2014-08-19 21:46:00,3,"
							
										
						Text Mining is one of the most complex analysis in the industry of analytics. The reason for this is that, while doing text mining, we deal with unstructured data. We do not have clearly defined observation and variables (rows and columns). Hence, for doing any kind of analytics, you need to first convert this unstructured data into a structured dataset and then proceed with normal modelling framework. The additional step of converting an unstructured data into a structured format is facilitated by a Word dictionary. You need a dictionary to do any kind of information extraction. Dictionary to do a sentiment analysis is easily available on web world. But, for some specific analysis you need to create a dictionary of your own.
This series of article starts from the very basic level to enable anyone, who might not have ever worked on text mining, be able to do one after a read.  We will consider a business case to explain this framework and the practical usage. In this article we will start with an overall broad steps, which we need to follow to do an unstructured text mining. In coming articles we will get into more specifics steps like building a dictionary and scoring the entire text.
Business Problem
You are the owner of Metrro cash n carry. Metrro has a tie up with Barcllays bank to launch co-branded cards. Metrro and Barcllay have recently entered into an agreement to share transactions data. Barcllays will share all transaction data done on their credit card on any retail store. Metrro will share all transaction done by any credit card on their stores. You wish to use this data to track where are your high value customers shopping other than Metrro.
To do this you need to fetch out information from the free transactions text available on Barcllays transaction data. For instance, a transaction with free text “Payment made to Messy” should be tagged as transaction made to the retail store “Messy”. Once we have the tags of retail store and the frequency of transactions at these stores for Metrro high value customers, you can analyze the reason of this customer outflow by comparing services between Metrro and the other retail store.
Unstructured data mining framework
The dictionary we need in this business problem looks like a very niche dictionary. We need to identify all retail store names from the transaction free text. Probability of such a dictionary being available and this dictionary being reliable is very low. Hence we need to create such a dictionary and then score our entire data set.
Following is a framework you can follow to create this dictionary :
 Step 1 : As analyzing the entire text manually is an impossible task, we take a random/stratified sample to build a dictionary.
Step 2 : We clean the data to make sure we capture the real essence of the text available. For instance, Maccy’s , maccy and Maccy should be counted as one word. Also, we need to remove stopwords of English dictionary.
Step 3 : Once you have the clean text, extract the most frequently occurring words. Just imagine how non conclusive results you will get if cleaning was not done. Manually identify the frequently occurring words as identifiers. This will form your dictionary.
Once you have the final dictionary, it is now time to score your entire dataset. Following is a framework you can follow to score your dataset :
Step 4: Now is the time to clean the entire data-set. This is done to make sure that the dictionary we have created in step 3 works on this entire dataset.
Step 5 : Using the dictionary, we can categorize each transaction statement.
Step 6: Once we have tags of category on each transaction statement, we can summarize the entire dataset to fetch business insights and frame business strategy.
End Notes 
This article gives you an overview on how to do text mining on real life problem statement. Starting from next article, we will give details on each step mentioned in this framework. I personally prefer using both R and SAS in conjunction to build a unstructured data model.R is very handy to create dictionary on smaller datasets. Whereas, SAS is capable of scoring this dictionary on the entire dataset. Once, we solve this business case completely, we will also look at some text mining visualization techniques.
Have you ever worked on unstructured datasets? If you did, what framework did you use? Did you find the article useful? Did this article solve any of your existing dilemma?
If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.
 
Share this:Pocket

	Related
			
						
						
		"
"Visualizing a market Basket analysis","Tavish Srivastava",2014-08-08 00:27:00,5,"
							
										
						
Last week had been very hectic. I had slogged more than 100 hours to come out with an awesome recommender based on market basket analysis.
“Now was the time to shine!” I thought, just before the meeting with stakeholders was about to start. I had prepared a good presentation and was feeling confident about the work. Thirty minutes into the presentation, I was trying my level best to explain lift, support and confidence in an imaginary 3d plane to the stakeholders.
Guess what – they were not impressed, they found the technique too complex. The meeting ended up with the key stakeholder saying “Can you create something simpler and more intuitive?”


This is when I went back to a drawing board and came out with this technique to visualize and explain market basket analysis in very simple visualization. This was the core thought behind this technique:
Algorithm used in Text mining can be leveraged to  create relationship plots in a Market basket analysis. 
Market basket is a widely used analytical tool in retail industry. However, retail industry use it extensively, this is no way an indication that the usage is limited to retail industry.  Various X-sell strategies in different industries can be made using a market basket analysis. There is a good amount of content available in the web world on the theory behind market basket analysis but I have hardly seen any articles on how to visualize market basket analysis . In this article, I will leverage some algorithm of text mining to get such visual plots.
Some basic Definitions
Support : Support is simply the probability of an event  to occur. If we have an event to buy a product A, Support(A) is simple the number of transactions which includes A divided by total number of transactions.
Confidence : Confidence is essentially the conditional probability of an event A happening given that B has happened.
For more detailed definition refer to our last article (last post).
 
Importing the dataset
The first part of any analysis is to bring in the dataset. I am using a dummy data to demonstrate this application. The data has details of 12k transactions. Each transaction has 3 products.  Following is the code to import the transaction data stored in a CSV file.

txn_data<-read.csv(""Retail_Data.csv"")
summary(txn_data)
transaction_id                                     Prod1                         Prod2                     Prod3
Min. :      100001                                 A:2983                        E:3962                   H:5907
1st Qu.: 103001                                 B:3024                        F:4053                    I:6093
Median :106001                                C:3047                        G:3985
Mean :   106001                                D:2946
3rd Qu.: 109000
Max. :     112000

As you can observe, each transaction has all 3 products. Product 1 takes only A,B,C and D. Product 2 takes E,F and G. Product 3 takes H and I. All the three products are mutually exclusive.

Creating an “item-transaction” Matrix
This is a concept, I learned in text mining. But it very well fits into this application as well.  We will first create a matrix with flags on each product. In total we have 9 products, hence we generate 9 vectors to capture these flags.  Here is the code to generate the 9 vectors and joining them to form item document matrix.

#Initializing vectors
 A <- numeric(0)
 B <- numeric(0)
 C <- numeric(0)
 D <- numeric(0)
 E <- numeric(0)
 F <- numeric(0)
 G <- numeric(0)
 H <- numeric(0)
 I <- numeric(0)
 #Preparing the flag metrics
 for ( i in 1:nrow(txn_data))
 {
 if (txn_data$Prod1[i] == ""A"") A[i] <- 1 else A[i]<-0
 if (txn_data$Prod1[i] == ""B"") B[i] <- 1 else B[i]<-0
 if (txn_data$Prod1[i] == ""C"") C[i] <- 1 else C[i]<-0
 if (txn_data$Prod1[i] == ""D"") D[i] <- 1 else D[i]<-0
 if (txn_data$Prod2[i] == ""E"") E[i] <- 1 else E[i]<-0
 if (txn_data$Prod2[i] == ""F"") F[i] <- 1 else F[i]<-0
 if (txn_data$Prod2[i] == ""G"") G[i] <- 1 else G[i]<-0
 if (txn_data$Prod3[i] == ""H"") H[i] <- 1 else H[i]<-0
 if (txn_data$Prod3[i] == ""I"") I[i] <- 1 else I[i]<-0
 }
 final.mat <- rbind(A,B,C,D,E,F,G,H,I)

 
Creating plots using igraph library
Once we have the transactions-item matrix, it is time to create an item-item correlation matrix. I have done this using a simple mathematical formulation.  We multiple the transaction-item matrix with its own transpose to get item-item correlation matrix. In this matrix, the number on diagonal gives an indication of Support whereas all other numbers give the confidence.  We use both these numbers to build a relationship plot. Following is the code to build the matrix and the plot.

#Creating the relationship matrix
 termMatrix <- final.mat %*% t(final.mat)
 #Creating the graphs
 library(igraph)
 # build a graph from the above matrix
 g <- graph.adjacency(termMatrix, weighted=T, mode = ""undirected"")
 # remove loops
 g <- simplify(g)
 # set labels and degrees of vertices
 V(g)$label <- V(g)$name
 V(g)$degree <- degree(g)
 # set seed to make the layout reproducible
 set.seed(3952)
 layout1 <- layout.fruchterman.reingold(g)
 plot(g, layout=layout1)
 plot(g, layout=layout.kamada.kawai)
 tkplot(g, layout=layout.kamada.kawai)


As of now we have not incorporated the strength of confidence or the support to plot this graph. Something to observe in this plot is that products like A and B are not connected. This is simply because they never co-exist together in any transaction. This plot can be use to visualize the negative lift items. Such items should  not be placed near each other. The next step is to incorporate the support of each product in the visual plot.

V(g)$label.cex <- 2.2 * V(g)$degree / max(V(g)$degree)+ .2
 V(g)$label.color <- rgb(0, 0, .2, .8)
 V(g)$frame.color <- NA
 egam <- (log(E(g)$weight)+0.2) / max(log(E(g)$weight)+0.2)


Here, we have incorporated the support of each product. As you can see H and I form the biggest letters and A,B,C and D the smallest. This is an indication of higher and lower support. You can validate these inferences from the initial frequency distribution. The next step is to incorporate the confidence as well in the relationship line width.

E(g)$color <- rgb(.5, .5, 0, egam)
 E(g)$width <- egam
 # plot the graph in layout1
 plot(g, layout=layout1)
 tkplot(g, layout=layout.kamada.kawai)


 
The final plot makes the entire story clear. We have already seen that H and I have the highest support. Now it is also clear that E-I , I-F and H-F have a high confidence as well. Hence, if a customer buys a product F there is a high propensity that he will also buy product H and I. Hence, following are the rules which we can infer from this analysis :
1. If a customer buys E, he has a high propensity to also buy I.
2.If a customer buys F, he has a high propensity to also buy I.
3. If a customer buys F, he has a high propensity to also buy H.
4. If a customer buys I, there is very small that he will also buy H.
The arrangement of items should flow from these rules in order to maximize the sales.
 
End Notes
Graphical representation of market basket analysis makes the interpretation of the entire puzzle of “probabilities/conditional probability/lift above random events” much simpler than a tabular format. This simplification can be more appreciated when we have a large number of transactions and product list. In case of large lists, we can simply find out using the dimension of product sign and width of the line connecting them to infer out simple rules which otherwise were buried in a matrix of complex probabilities.
Have you ever visualized relationships in a market basket analysis? If you did, what algorithm did you use? Did you find the article useful? Did this article solve any of your existing dilemma?
If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.
Share this:Pocket

	Related
			
						
						
		"
"How to interpret hidden state in Latent Markov Model","Tavish Srivastava",2014-07-30 13:43:00,4,"
							
										
						In some of my previous articles, I have illustrated how Markov model can be used in real life forecasting problems. As described in these articles, Simple Markov model cannot be used for customer level predictions, because it does not take into account any covariates for predictions. Latent Markov model is a modified version of the same Markov chain formulation, which can be leveraged for customer level predictions. “Latent” in this name is a representation of “Hidden states”. In this article, our focus will not be on how to formulate a Latent Markov model but simply on what do these hidden state actually mean. This is a concept which I have found quite ambiguous in the web world and too much statistics to understand this simple concept. In this article, I will try to illustrate physical interpretation of this concept “Hidden state” using a simple example.

 Case Background 
A prisoner was trying to escape from the prison. He was told that he will be sent a help from outside the prison, the first day when it rains. But, he was caught having a fight with his cellmate and sentenced for stay in a dark cell for a day. He is good with probabilities and will like to make inference about the weather outside. In case he gets a probability more than 50% of the day being rainy, he will make a move else will not attract attention unnecessarily. The only clue he gets in the dark cell is the accessories, which the policeman carries while coming to the cell. Given that the policeman carries Food plate wrapped in polythene 25% of times, Food plate in packed container 25% times and open food plate 50% of times; what is the probability that it will rain the same day when the prisoner is in the dark cell?
 Using case to build analogies 
In this case we have two key events. First event is “what accessories does the policeman carry” and second event is that “it will rain on the day when the prisoner is in the dark cell”.

What accessories does the policeman carry : Observation or Ownership
it will rain on the day when the prisoner is in the dark cell : Hidden state

Hidden state and Ownership are commonly used terms in LMM model. As you can see that the observation is something the prisoner can see and accurately determine at any point of time. But the event of raining the day when he is in dark cell is something which he can only infer and not state with 100% accuracy.
 Calculations 
Having understood the concept of hidden states, let’s crunch some numbers to come up with the final probability of it raining on the day prisoner is in the dark cell. Prisoner being anxious for last few days about the weather was noting the weather for last few months. Based on these sequence, he has make a Markov chain for the weather next day given the weather of that day. Following is how the chain looks like :

The prisoner knows that it didn’t rain yesterday (Obviously, otherwise he would not have been in jail anymore). If he uses the Markov chain directly, he can conclude with some accuracy whether it will rain today or not. Following is the formulation for such a calculation :

P(Rain today/No Rain yesterday)= 5%

Hence, the chances seem really low that it is raining out today. Now, let’s bring in some amount of information on the observation or ownership. Using some good judgement, the prisoner already knows the following conditional probability Matrix :

Let’s take one cell to clarify the grid. The chances are 90% that it is raining today if we already know that the policeman is carrying the food plate with a polythene without taking into account the weather of last day. The prisoner is keenly waiting for the policeman to come and give the final clue to determine the final set of probability. The policeman actually brings in food with a polythene. Before making calculations, let’s first decide the set of events.

A : It will rain today
B: It did not rain yesterday
C: The  policeman brings in food with a polythene

What we want to calculate is P(A/B,C)? Now let’s look at the set of probabilities we know :

P(A/B) = 5%         P(C/A) = 90%      P(C) = 25%

We now will convert the expression P(A/B,C) into these know 3 parameters.

P(A/B,C) = P(A,B/C)/P(B/C) = P(A,B/C)/P(B) {Using Markov first order principle} …………………………1
P(A,B/C) = P(A,B,C)/P(C) = P(C/A,B)*P(A,B)/P(C) = P(C/A)*P(A,B)/P(C) {Using Markov first order principle}
=> P(A,B/C) = P(C/A) * P(A/B)*P(B)/P(C)
Substituting this in equation 1,
P(A/B,C) = P(C/A) * P(A/B) / P(C) = 90%*5%/25% = 18%

 Final inferences 
P(It will rain today/no rain yesterday,policeman brings in food with a polythene) = 18%
As you can see, this probability is between 5% and 90% as estimated  separately by the two clues we have for prediction. Combination of both the clues reveals a more accurate prediction of the event in focus. Because this probability is less than 50%, the prisoner will not take a chance expecting a rain today.
 End Notes 
Using Markov chain simplifications , observations and Markov chain transition probability we were able to find out the hidden state for the day when prisoner was in the dark cell. The scope of this article was restricted to understanding hidden states and not framework of Latent Markov model. In some of the future article we will also touch up on formulation of Latent Markov model and its applications.
Did you find the article useful? Did this article solve any of your existing dilemmas? If you did, share with us your thoughts on the topic.
If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.
Share this:Pocket

	Related
			
						
						
		"
"Solve a business case using simple Markov Chain","Tavish Srivastava",2014-07-23 22:24:00,4,"
							
										
						Markov process fits into many real life scenarios. Any sequence of event that can be approximated by Markov chain assumption, can be predicted using Markov chain algorithm. In the last article, we explained What is a Markov chain and how can we represent it graphically or using Matrices. In this article, we will go a step further and leverage this technique to draw useful business inferences.
 Business Case 
“Krazy Bank”, deals with both asset and liability products in retail bank industry. A big portfolio of the bank is based on loans. These loans make the majority of the total revenue earned by the bank. Hence, it is very essential for the bank to find the proportion of loans which have a high propensity to be paid in full and those which will finally become Bad loans. “Krazy Bank” has hired you as a consultant to come up with these scoping numbers.
All the loans, which have been issued by “Krazy Bank” can be classified into four categories :
Good Loans : These are the loans which are in progress but are given to low risk customers. We expect most of these loans will be paid up in full with time.
Risky loans : These are also the loans which are in progress but are given to medium or high risk customers. We expect a good number of these customers will default.
Bad loans : The customer to whom these loans were given have already defaulted.
Paid up loans : These loans have already been paid in full.
 
 Short Note on Absorbing nodes 
Absorbing nodes in a Markov chain are the possible end states. All nodes in Markov chain have an array of transitional probability to all other nodes and themselves. But, absorbing nodes have no transitional probability to any other node. Hence, if any individual lands up to this state, he will stick to this node for ever. Let’s take a simple example. We are making a Markov chain for a bill which is being passed in parliament house. It has a sequence of steps to follow, but the end states are always either it becomes a law or it is scrapped. These two are said to be absorbing nodes. For the loans example, bad loans and paid up loans are end states and hence absorbing nodes.
 
 Transition diagram 
You have done a thorough research on past trends of loan cycle and based on past trends here is the Markov chain you observed (for a period of 1 year):

Explaining absorbing nodes becomes simple from this diagram. As you can see, Paid up and bad loans transition only to themselves. Hence, whatever path a process takes, if it lands up to one of these two states, it will stay there for ever. In other words, a bad loan cannot become paid up, risky or good loan ever after. Same is true with paid up loans.
 Transition calculations 
Once, we have  1 year transition probability, we can convert prediction algorithm to simple matrix multiplication. Currently the portfolio has 60% Good and 40% Risky loans. We look forward to calculate, how many of these loans will be finally paid up in full? Using 1 year transition probability, we can estimate the number of loans falling into each of the four bucket 1 year down the line.

Here are some interesting insights from this calculation. We can expect 15% of the loans to be paid up in this year and 16% being ending up as bad loans. As the % of bad loans seem to be on a higher side, it will be beneficial to identify these loans and make adequate interventions. Pin pointing to these 15% is not possible using simple Markov chain, but same is possible using a Latent Markov model.
Now, to make a prediction for 2 years, we can use the same transition matrix. This time the initial proportions will the final proportions of last calculation. Transition probability generally do not change much. This is because, it is based on several time points in past.

 If we keep on repeating this exercise, we see the proportion matrix converges. Following is the converged matrix. Note that, multiplying it with transition matrix makes no change to proportions.

These are the proportion numbers we were looking for. 54% of the current loans will be paid up in full but 46% will default. Hence doing this simple exercise lead us to such an important conclusion that the current portfolio exposes bank to a very high risk.
 Loan transition deep dive  
We have already seen the stationary point proportions for the portfolio. Something which will be of interest to us next is that what proportion of Good loans land up being paid up in full. For this we can start with an initial proportion split of Good – 100% and rest – 0%. The final converged matrix is as follows:

Here are some interesting insights. If the entire portfolio was built of good loans, only 23% of loans would have defaulted against 46% for current portfolio. Hence, we will expect a very high proportion of risky loans will show default. We can find this using a simple transition calculation using Risky – 100% and rest – 0%. Following is the final converged matrix :

 
80% of such loans will default. Hence, our classification of Risky and Good separates out propensity to default pretty nicely.
 End Notes  
In this article, we saw how Markov chain can be used to find out multiple insights and make good predictions on an overall level. There are many other processes which can be explained using Markov chain. In such cases, Markov chain algorithm will give you number of insights and will serve as a very handy forecasting tool. However, Markov chain can only make forecast on segment level and not make prediction on customer level. In case you need to make customer level forecast, you need a Latent Markov model and not a simple Markov model.
Did you find the article useful? Are you aware of any other real life Markov process? Can Markov chain be used in that process to bring out interesting insights? Did this article solve any of your existing problems? Have you used simple Markov chain in any other context ?
If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.
 
Share this:Pocket

	Related
			
						
						
		"
"Introduction to Markov chain : simplified!","Tavish Srivastava",2014-07-17 10:38:00,5,"
							
										
						Markov chain is a simple concept which can explain most complicated real time processes.Speech recognition, Text identifiers, Path recognition and many other Artificial intelligence tools use this simple principle called Markov chain in some form. In this article we will illustrate how easy it is to understand this concept.
 
Markov chain is based on a principle of “memorylessness”. In other words the next state of the process only depends on the previous state and not the sequence of states. This simple assumption makes the calculation of conditional probability easy and enables this algorithm to be applied in number of scenarios. In this article we will restrict ourself to simple Markov chain. In real life problems we generally use Latent Markov model, which is a much evolved version of Markov chain. We will also talk about a simple application of Markov chain in the next article.
 A simple business case 
Coke and Pepsi are the only companies in country X. A soda company wants to tie up with one of these competitor. They hire a market research company to find which of the brand will have a higher market share after 1 month. Currently, Pepsi owns 55% and Coke owns 45% of market share. Following are the conclusions drawn out by the market research company:

P(P->P) : Probability of a customer staying with the brand Pepsi over a month = 0.7
P(P->C) : Probability of a customer switching from Pepsi to Coke over a month = 0.3
P(C->C) : Probability of a customer staying with the brand Coke over a month = 0.9
P(C->P) : Probability of a customer switching from Coke to Pepsi over a month = 0.1

We can clearly see customer tend to stick with Coke but Coke currently has a lower wallet share. Hence, we cannot be sure on the recommendation without making some transition calculations.
 Transition diagram 
The four statements made by the research company can be structured in a simple transition diagram.

The diagram simply shows the transitions and the current market share. Now, if we want to calculate the market share after a month, we need to do following calculations :
Market share (t+1) of Pepsi = Current market Share of Pepsi * P(P->P) + Current market Share of Coke * P(C->P)
Market share (t+1) of Coke = Current market Share of Coke * P(C->C) + Current market Share of Pepsi * P(P->C)
These calculations can be simply done by looking at the following matrix multiplication :
Current State X Transition Matrix = Final State

As we can see clearly see that Pepsi, although has a higher market share now, will have a lower market share after one month. This simple calculation is called Markov chain. If the transition matrix does not change with time, we can predict the market share at any future time point. Let’s make the same calculation for 2 months later.

 Steady state Calculations 
Furthermore to the business case in hand, the soda company wants to size the gap in market share of the company Coke and Pepsi in a long run. This will help them frame the right costing strategy while pitching to Coke.The share of Pepsi will keep on going down till a point the number of customer leaving Pepsi and number of customers adapting Pepsi is same. Hence, we need to satisfy following conditions to find the steady state proportions:
Pepsi MS * 30% = Coke MS * 10%  ……………………………………………..1
Pepsi MS + Coke MS = 100% ……………………………………………………2
4 * Pepsi MS = 100% => Pepsi MS = 25% and Coke MS = 75%
Let’s formulate an algorithm to find the steady state. After steady state, multiplication of Initial state with transition matrix will give initial state itself. Hence, the matrix which can satisfy following condition will be the final proportions:
Initial state X Transition Matrix = Initial state
By solving for above equation, we can find the steady state matrix. The solution will be same as [25%,75%].
 End Notes  
In this article we introduced you to Markov chain equations and terminology. We also looked at how simple equations can be scaled using Matrix multiplication. We will use these terminologies and framework to solve a real life example in the next article. We will also introduce you to concepts like absorbing node and Regular Markov Chain to solve the example.
Did you find the article useful? Did this article solve any of your existing problems? Have you used simple Markov chain before? If you did, share with us your thoughts on the topic.
If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.
Share this:Pocket

	Related
			
						
						
		"
"Who is the world cheering for? 2014 FIFA WC winner predicted using Twitter feed (in R)","Tavish Srivastava",2014-07-08 15:30:00,3,"
							
										
						Sports are filled with emotions! Cheering of audience, reactions to events on various media channels are some of the factors, which make a huge impact on the mind of the players.

If people support you, your chances to win are greatly enhanced. Live example of this fact, are the statistics of Indian cricket team playing in India and abroad. The win rate of Indian cricket team in India is approximately twice the win rate abroad.
Football is again a game driven largely by emotions. Cards (Yellow/Red), have been kept in the game to limit these emotions. If you think about places, where people express their emotions, Facebook and Twitter come out on top of the list. In this article we will make a prediction using a simplistic algorithm on the winner of 2014 FIFA world cup. This prediction will be based on the emotions expressed by people on Twitter. The entire code has been shared on github. We will just keep bits and pieces of this code as a reference in this article.

 Fetching the tweets on the 4 shortlisted teams  
The first step is to fetch the tweets, which have a reference to both FIFA and a team (out of 4). We have done this using hashtags on twitter. In this case we have put an upper threshold of 1000 tweets because of constrained hardware resources. You can use the following code to fetch the same :

ARG.list <- searchTwitter(‘#ARG #FIFA’, n=1000, cainfo=”cacert.pem”)ARG.df = twListToDF(ARG.list)
BRA.list <- searchTwitter('#BRA #FIFA', n=1000, cainfo=""cacert.pem"")
BRA.df = twListToDF(BRA.list)
GER.list <- searchTwitter('#GER #FIFA', n=1000, cainfo=""cacert.pem"")
GER.df = twListToDF(GER.list)
NED.list <- searchTwitter('#NED #FIFA', n=1000, cainfo=""cacert.pem"")
NED.df = twListToDF(NED.list)

 Sentiment analysis 
Once we have all the tweets, we need to clean the tweets and then check the sentiment of these tweets. Following is the code, I have used to pull out cleaned words and map them to a positive and negative strings.

score.sentiment = function(sentences, pos.words, neg.words,.progress=’none’){require(plyr)require(stringr)
scores = laply(sentences, function(sentence, pos.words, neg.words) {
# clean up sentences with R's regex-driven global substitute, gsub():
sentence = gsub("":)"", 'awsum', sentence)
sentence = gsub('[[:punct:]]', '', sentence)
sentence = gsub('[[:cntrl:]]', '', sentence)
sentence = gsub('\\d+', '', sentence)
# and convert to lower case:
sentence = tolower(sentence)
# split into words. str_split is in the stringr package
word.list = str_split(sentence, '\\s+')
# sometimes a list() is one level of hierarchy too much
words = unlist(word.list)
# compare our words to the dictionaries of positive & negative terms
pos.matches = match(words, pos.words)
neg.matches = match(words, neg.words)
# match() returns the position of the matched term or NA
# we just want a TRUE/FALSE:
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
# and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():
score = sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words, .progress=.progress )
scores.df = data.frame(score=scores, text=sentences)
return(scores.df)
}
#Load sentiment word lists
hu.liu.pos = scan('C:/temp/positive-words.txt', what='character', comment.char=';')
hu.liu.neg = scan('C:/temp/negative-words.txt', what='character', comment.char=';')
#Add words to list
pos.words = c(hu.liu.pos, 'upgrade', 'awsum')
neg.words = c(hu.liu.neg, 'wtf', 'wait','waiting', 'epicfail', 'mechanical',""suspension"",""no"")

 Scoring each tweet 
Once we have a well defined function which can score the tweets individually, we now score out tweets after converting them to factors (Refer to the github code).

ARG.scores = score.sentiment(ARG.df$text, pos.words,neg.words, .progress=’text’)BRA.scores = score.sentiment(BRA.df$text, pos.words,neg.words, .progress=’text’)
NED.scores = score.sentiment(NED.df$text,pos.words,neg.words, .progress='text')
GER.scores = score.sentiment(GER.df$text,pos.words,neg.words, .progress='text')
ARG.scores$Team = 'Argentina'
BRA.scores$Team = 'Brazil'
NED.scores$Team = 'Netherland'
GER.scores$Team = 'Germany'
head(all.scores)
all.scores = rbind(ARG.scores, NED.scores, GER.scores,BRA.scores)

 Summarizing the processed score 
Once we have a sentiment score against each tweet, we now try to summarize the score and fetch useful information from the same. You can use the following code to do the summarization
 
table(all.scores$score,all.scores$Team)
ggplot(data=all.scores) + # ggplot works on data.frames, always geom_bar(mapping=aes(x=score, fill=Team), binwidth=1) + facet_grid(Team~.) + # make a separate plot for each hashtag theme_bw() + scale_fill_brewer() # plain display, nicer colors

 

 Final Results 
As you can clearly see, we have a clear winner from the graphs i.e. Argentina. Let’s summarize this dataset into a cross tab.
#Tweets with a +ve/-ve score for each team 

%Tweets with a +ve/-ve score for each team

Final Summary

 
We can use different parameters to come up with the rank ordering. I have considered following to rank order teams :
Criterion 1 :  %positive tweet – %negative tweets
Criterion 2 :  weighted score
Criterion 3 :  Fixture of matches
Using all three, we see our clear winner is Argentina and Brazil is clearly on rank 4. But Germany and Netherland are really close. But using the 3rd criterion, we see Germany is the one competing against Brazil. Hence, we have a clear rank order. The prediction can be seen in the 1st picture of the article.
 End Notes 
I am not a follower of the sport: football, but this analysis has excited me enough to compare my prediction to the actuals. I see myself as an unbiased analyst to make this prediction. The technique used in this article, is an over simplistic model to make such a strong prediction, but a good point to start one.
An actual model should have all kinds of input like past performance of each team versus each other, venue, players injured and finally the sentiment feed. I will like to hear more inputs to make this model more accurate. These recommendation can be used to either enhance the sentiment analysis algorithm or to include new types of input variables. People who follow the sport will be the best ones to make recommendations on this.
Here is another application – if you feel strongly against this prediction, or have your own algorithm to predict a winner – you can use the difference in the two models to create a nice betting strategy!
Did you find the article useful? Have you worked on similar objective before? How can we enhance this code to make more accurate predictions? Share with us similar kinds of post to enable us make a even stronger model.
If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.
 
Share this:Pocket

	Related
			
						
						
		"
"Using Facebook as an analyst (Hint - using R)","Tavish Srivastava",2014-07-03 05:26:00,5,"
							
										
						Facebook has huge data bank and it allows us to make use of it to some extent.
October is a month of celebration in India. We have festivals like Diwali and Dushehra in October, which makes the entire month a time to celebrate and reunion. Every time we meet our friends and relatives at different places, to make it easier for everyone to reunite.  Every time before going to the city, I update my FB/Twitter status to “Going to city xyz” and get a bunch of replies from people who are traveling to the same place. But almost every time I miss some of my friends just because of lack of information. This time I went a step ahead and connected my FB account to R and looked for people who have their current city as my target location. Surprisingly, I got 10 more friends who were in the city, whom I might have missed if this exercise was never done. In this process, I had a lot of fun with other user profile variables FB permits us to look at. In this article, I will help readers to understand this process of connecting FB to R and demonstrate the simplicity of the process. This type of analysis is not restricted only to the case in hand but a much broader set of daily life problems.
We will use library Rfacebook for this analysis.

  How to connect R to Facebook  
Facebook provides you two simple ways to import data from the website. I will demonstrate the simpler one in this article.
Step 1 : Goto the link ” https://developers.facebook.com/tools/explorer/” . This will open thee FB developer page.

Step 2: Change the API Version(Red box in the picture) to “unversioned”
Step 3: Click the “Get Access Token” (Green box in the picture).

Step 4: Check all the boxes in all three tabs. These are the permissions you are asking from yourself to access. Assuming you do not wish to hide anything from yourself, you can safely check all boxes.
Step 5 : Click on the button “Get Access Token”. This token is valid for 2 hours.
Step 6 : Store your token as variable in R studio. You can use the following code for the same :

> token <- ""XXXXX12333YYY""
> me <- getUsers(""me"", token=token)
> me$name
[1] “Tavish Srivastava”

Now, you have facebook connected to your R session for the next 2 hours.
  Search people in a particular city among your friend list  
I and all my relatives decided to meet in Pune (Maharashtra) this year and hence “Pune” is the location I am looking for in the current location field of all my friends profile. Imagine doing the same thing manually on facebook. Let’s take a smarter route and check out the frequency distribution of current location among the user IDs in my friend list. To accomplish this task you can execute a simple code on R.
Step1 : Pull out the list of all friends and their ID.
Step 2 : Pull all the user details corresponding to this table of IDs.
Step 3 : Check the frequency distribution of all current location. This is done to make sure the same name “Pune” is not appearing in different formats.
This frequency distribution is a reason why this method adds power over traditional search on Facebook. For example, if we were meeting in Delhi, I would want to search Delhi, Gurgaon, Noida and possibly Faridabad for my friends. However, through this method, I can write one single query to get it.
You can use following code to do the same :

> my_friends <- getFriends(token, simplify=TRUE)
> my_friends_info <- getUsers(my_friends$id, token=token, private_info=TRUE)
> table(my_friends_info$location)


We see that, I have 16 friends with their current location as Pune. I also get the exact string I should search for to complete my task. Following is the code you can use to find these 16 friends.

> Pune_resident <- subset(my_friends_info$first_name,my_friends_info$location == ""Pune, Maharashtra"")
> Pune_resident

Finally I get the list of names of my friends, who have their current location as Pune. While doing this exercise, I found some other interesting facts about my friend list. It is very easy to tabulate the relationship_status of all your friends. Because the possible values are very few, it becomes interesting to analyze the same. Following is a code I used to tabulate the relationship_status of my friends.

> table(my_friends_info$relationship_status)
 Engaged : 3
 In a relationship : 6
 It’s complicated    : 5
 Married  : 126
 Single : 434

As I have been lately busy in my work, I completely lost track of people getting engaged. Here is an easy method to find the same :

> engaged.friends <- subset(my_friends_info$first_name,my_friends_info$relationship_status == ""Engaged"")
> engaged.friends

I did a tabulation on each of the user information Facebook shared and discovered new things about my friends every single time.
  End Notes 
I found this small piece of analysis both interesting and insightful. It just helps you get a summary of everything. You can go through the user information of your entire friend list in less than 5 minutes. You can use this data to visualize your friends on a graph and see various clusters of population (Hint – you will need to use igraph library for this). You can do some cool things like define the distance between nodes basis interactions on Facebook and see which are the closest people to you as per Facebook.
How would you play around with your social media data? Have you done such small experiments on your Facebook or twitter profile? Did you go beyond the scope of this article in your analysis? Please share with us your thoughts on the topic.
If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.
 
Share this:Pocket

	Related
			
						
						
		"
"Part 2 : Random forest vs. CART model","Tavish Srivastava",2014-06-27 05:35:00,6,"
							
										
						Random forest is one of the most commonly used algorithm in Kaggle competitions. Along with a good predictive power, Random forest model are pretty simple to build. We have previously explained the algorithm of a random forest ( Introduction to Random Forest ). This article is the second part of the series on comparison of a random forest with a CART model. In the first article, we took an example of an inbuilt R-dataset to predict the classification of an specie. In this article we will build a random forest model on the same dataset to compare the performance with previously built CART model. I did this experiment a week back and found the results very insightful. I recommend the reader to read the first part of this article (Last article) before reading this one.
  Background on Dataset “Iris”  
Data set “iris” gives the measurements in centimeters of the variables : sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of Iris. The dataset has 150 cases (rows) and 5 variables (columns) named Sepal.Length, Sepal.Width, Petal.Length, Petal.Width, Species. We intend to predict the Specie based on the 4 flower characteristic variables.
We will first load the dataset into R and then look at some of the key statistics. You can use the following codes to do so.

 
data(iris)
# look at the dataset
summary(iris)
# visually look at the dataset
qplot(Petal.Length,Petal.Width,colour=Species,data=iris)


  Results using CART Model  
The first step we follow in any modeling exercise is to split the data into training and validation. You can use the following code for the split. (We will use the same split for random forest as well)

 
train.flag <- createDataPartition(y=iris$Species,p=0.5,list=FALSE)
training <- iris[train.flag,]
Validation <- iris[-train.flag,]

CART model gave following result in the training and validation :
Misclassification rate in training data = 3/75
Misclassification rate in validation data = 4/75
As you can see, CART model gave decent result in terms of accuracy and stability. We will now model the random forest algorithm on the same training dataset and validate it using same validation dataset.
  Building  a Random forest model  
We have used “caret” , “randomForest” and “randomForestSRC” package to build this model. You can use the following code to generate a random forest model on the training dataset.

 
> library(randomForest)
> library(randomForestSRC)
> library(caret)
> modfit <- train(Species~ .,method=""rf"",data=training)
 
> pred <- predict(modfit,training)
> table(pred,training$Species)
  
  pred       setosa versicolor virginica
 setosa        25       0          0
 versicolor     0      25          0
 virginica      0       0         25



Misclassification rate in training data = 0/75                                        [This is simply awesome!]
  Validating the model  
Having built such an accurate model, we will like to make sure that we are not over fitting the model on the training data. This is done by validating the same model on an independent data set. We use the following code to do the same :

 
> train.cart<-predict(modfit,newdata=training)
> table(train.cart,training$Species)
> train.cart   setosa versicolor virginica
  pred       setosa versicolor virginica
 setosa        25       0          0
 versicolor     0      22          0
 virginica      0       3         25
 # Misclassification rate = 3/75

Only 3 misclassified observations out of 75, signifies good predictive power. However, we see a significant drop in predictive power of this model when we compare it to training misclassification.
  Comparison between the two models  
Till this point, everything was as per books. Here comes the tricky part. Once you have all performance metrics, you need to select the best model as per your business requirement. We will make this judgement based on 3 criterion in this case apart from business requirements:
.1. Stability : The model should have similar performance metrics across both training and validation. This is very essential because business can live with a lower accuracy but not with a lower stability. We will give the highest weight to stability. For this case let’s take it as 5.
2. Performance on Training data : This is one of the important metric but nothing conclusive can be said just based on this metric. This is because an over fit model is unacceptable but will get a very high score at this parameter. Hence, we will give a low weight to this parameter (say 2).
3. Performance on Validation data : This metric catch holds of overfit model and hence is an important metric. We will score it higher than performance and lower than stability. For this case let’s take it as 3.
Note that the weights and scores entirely depends on the business case. Following is a score table as per my judgement in this case.
As you can see from the table that however Random forest gives me a better performance, I still will go ahead and use CART model because of the stability factor. Other factor in favor of CART model is the easy business justification. Random forest is very difficult to explain to people working on field. CART models are simple cuts which can be justified by simple business justification/reasons. But the choice of model selection is entirely dependent on business requirement.
  End Notes  
Every model has its own strength. Random forest, as seen from this case study, has a very high accuracy on the training population, because it uses many different characteristics to make a prediction. But, because of the same reason, it sometimes over fits the model on the data. CART model on the other side is simplistic criterion cut model. This might be over simplification in some case but works pretty well in most business scenarios. However, the choice of model might be business requirement dependent, it is always good to compare performance of different model before taking this call.
Did you find the article useful? Did this article solve any of your existing dilemmas? Have you compared the two models in any of your projects? If you did, share with us your thoughts on the topic.
If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.
 
Share this:Pocket

	Related
			
						
						
		"
"Comparing a CART model to Random Forest","Tavish Srivastava",2014-06-20 04:16:00,6,"
							
										
						I created my first simple regression model with my father in 8th standard (year: 2002) on MS Excel. Obviously, my contribution in that model was minimal, but I really enjoyed the graphical representation of the data. We tried validating all the assumptions etc. for this model. By the end of the exercise, we had 5 sheets of the simple regression model on 700 data points. The entire exercise was complex enough to confuse any person with average IQ level. When I look at my models today, which are built on millions of observations and utilize complex statistics behind the scene, I realize how machine learning with sophisticated tools (like SAS, SPSS, R)  has made our life easy.
Having said that, many people in the industry do not bother about the complex statistics, which goes behind the scene. It becomes very important to realize the predictive power of each technique. No model is perfect in all scenarios. Hence, we need to understand the data and the surrounding eco-system before coming up with a model recommendation.
In this article, we will compare two widely used techniques i.e. CART vs. Random forest. Basics of Random forest were covered in my last article. We will take a case study to build a strong foundation of this concept and use R to do the comparison. The dataset used in this article is an inbuilt dataset of R.
As the concept is pretty lengthy, we have broken down this article into two parts
 
 Background on Dataset “Iris” 
Data set “iris” gives the measurements in centimeters of the variables : sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of Iris. The dataset has 150 cases (rows) and 5 variables (columns) named Sepal.Length, Sepal.Width, Petal.Length, Petal.Width, Species. We intend to predict the Specie based on the 4 flower characteristic variables.
We will first load the dataset into R and then look at some of the key statistics. You can use the following codes to do so.
 

 
data(iris)
# look at the dataset
summary(iris)
# visually look at the dataset
qplot(Petal.Length,Petal.Width,colour=Species,data=iris)

 

The three species seem to be well segregated from each other. The accuracy in prediction of borderline cases determines the predictive power of the model. In this case, we will install two useful packages for making a CART model.

 
library(rpart)
library(caret)

After loading the library, we will divide the population in two sets: Training and validation. We do this to make sure that we do not overfit the model. In this case, we use a split of 50-50 for training and validation. Generally, we keep training heavier to make sure that we capture the key characteristics. You can use the following code to make this split.

 
train.flag <- createDataPartition(y=iris$Species,p=0.5,list=FALSE)
training <- iris[train.flag,]
Validation <- iris[-train.flag,]

 
 Building a CART model 
Once we have the two data sets and have got a basic understanding of data, we now build a CART model. We have used “caret” and “rpart” package to build this model. However, the traditional representation of the CART model is not graphically appealing on R. Hence, we have used a package called “rattle” to make this decision tree. “Rattle” builds a more fancy and clean trees, which can be easily interpreted. Use the following code to build a tree and graphically check this tree:

 
modfit <- train(Species~.,method=""rpart"",data=training) 

library(rattle)
fancyRpartPlot(modfit$finalModel)

 

 
 Validating the model 
Now, we need to check the predictive power of the CART model, we just built. Here, we are looking at a discordance rate (which is the number of misclassifications in the tree) as the decision criteria. We use the following code to do the same :

 
train.cart<-predict(modfit,newdata=training)
table(train.cart,training$Species)
train.cart   setosa versicolor virginica

setosa         25         0         0
versicolor     0         22         0
virginica       0         3       25

# Misclassification rate = 3/75
 

Only 3 misclassified observations out of 75, signifies good predictive power. In general, a model with misclassification rate less than 30% is considered to be a good model. But, the range of a good model depends on the industry and the nature of the problem. Once we have built the model, we will validate the same on a separate data set. This is done to make sure that we are not over fitting the model. In case we do over fit the model, validation will show a sharp decline in the predictive power. It is also recommended to do an out of time validation of the model. This will make sure that our model is not time dependent. For instance, a model built in festive time, might not hold in regular time. For simplicity, we will only do an in-time validation of the model. We use the following code to do an in-time validation:

 
pred.cart<-predict(modfit,newdata=Validation)
table(pred.cart,Validation$Species)
pred.cart   setosa versicolor virginica

setosa         25         0         0
versicolor     0         22         1
virginica       0         3       24

# Misclassification rate = 4/75

As we see from the above calculations that the predictive power decreased in validation as compared to training. This is generally true in most cases. The reason being, the model is trained on the training data set, and just overlaid on validation training set. But, it hardly matters, if the predictive power of validation is lesser or better than training. What we need to check is that they are close enough. In this case, we do see the misclassification rate to be really close to each other. Hence, we see a stable CART model in this case study.
Let’s now try to visualize the cases for which the prediction went wrong. Following is the code we use to find the same :

 
correct <- pred.cart == Validation$Species
qplot(Petal.Length,Petal.Width,colour=correct,data=Validation)


As you see from the graph, the predictions which went wrong were actually those borderline cases. We have already discussed before that these are the cases which make or break the comparison for the model. Most of the models will be able to categorize observation far away from each other. It takes a model to be sharp to distinguish these borderline cases.
End Notes : 
In the next article, we will solve the same problem using a random forest algorithm. We hope that random forest will be able to make even better prediction for these borderline cases. But, we can never generalize the order of predictive power among a CART and a random forest, or rather any predictive algorithm. The reason being every model has its own strength. Random forest generally tends to have a very high accuracy on the training population, because it uses many different characteristics to make a prediction. But, because of the same reason, it sometimes over fits the model on the data. We will see these observations graphically in the next article and talk in more details on scenarios where random forest or CART comes out to be a better predictive model.
Did you find the article useful? Did this article solve any of your existing dilemmas? Have you compared the two models in any of your projects? If you did, share with us your thoughts on the topic.
If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.
 
Share this:Pocket

	Related
			
						
						
		"
"Introduction to Random forest - Simplified","Tavish Srivastava",2014-06-10 02:16:00,3,"
							
										
						
With increase in computational power, we can now choose algorithms which perform very intensive calculations. One such algorithm is “Random Forest”, which we will discuss in this article. While the algorithm is very popular in various competitions (e.g. like the ones running on Kaggle), the end output of the model is like a black box and hence should be used judiciously.
Before going any further, here is an example on the importance of choosing the best algorithm.
Importance of choosing the right algorithm
Yesterday, I saw a movie called ” Edge of tomorrow“.  I loved the concept and the thought process which went behind the plot of this movie. Let me summarize the plot (without commenting on the climax, of course). Unlike other sci-fi movies, this movie revolves around one single power which is given to both the sides (hero and villain). The power being the ability to reset the day.
Human race is at war with an alien specie called “Mimics”.  Mimic is described as a far more evolved civilization of an alien specie. Entire Mimic civilization is like a single complete organism. It has a central brain called “Omega” which commands all other organisms in the civilization. It stays in contact with all other species of the civilization every single second. “Alpha” is the main warrior specie (like the nervous system) of this civilization and takes command from “Omega”. “Omega” has the power to reset the day at any point of time.
Now, let’s wear the hat of a predictive analyst to analyze this plot. If a system has the ability to reset the day at any point of time, it will use this power, whenever any of its warrior specie die. And, hence there will be no single war ,when any of the warrior specie (alpha) will actually die, and the brain “Omega” will repeatedly test the best case scenario to maximize the death of human race and put a constraint on number of deaths of alpha (warrior specie) to be zero every single day. You can imagine this as “THE BEST” predictive algorithm ever made. It is literally impossible to defeat such an algorithm.
Let’s now get back to “Random Forests” using a case study.
  Case Study  
Following is a distribution of Annual income Gini Coefficients across different countries :

Mexico has the second highest Gini coefficient and hence has a very high segregation in annual income of rich and poor. Our task is to come up with an accurate predictive algorithm to estimate annual income bracket of each individual in Mexico. The brackets of income are as follows :
1. Below $40,000
2. $40,000 - 150,000
3. More than $150,000
Following are the information available for each individual :
1. Age , 2. Gender,  3. Highest educational qualification, 4. Working in Industry, 5. Residence in Metro/Non-metro
We need to come up with an algorithm to give an accurate prediction for an individual who has following traits:
1. Age : 35 years , 2, Gender : Male , 3. Highest Educational Qualification : Diploma holder, 4. Industry : Manufacturing, 5. Residence : Metro
We will only talk about random forest to make this prediction in this article.
 The algorithm of Random Forest 
Random forest is like a bootstrapping algorithm with Decision tree (CART) model. Say, we have 1000 observation in the complete population with 10 variables. Random forest tries to build multiple CART model with different sample and different initial variables. For instance, it will take a random sample of 100 observation and 5 randomly chosen initial variables to build a CART model. It will repeat the process (say) 10 times and then make a final prediction on each observation. Final prediction is a function of each prediction. This final prediction can simply be the mean of each prediction.
 Back to Case study 
Disclaimer : The numbers in this article are illustrative
Mexico has a population of 118 MM. Say, the algorithm Random forest picks up 10k observation with only one variable (for simplicity) to build each CART model. In total, we are looking at 5 CART model being built with different variables. In a real life problem, you will have more number of population sample and different combinations of  input variables.
Salary bands :
Band 1 : Below $40,000
Band 2: $40,000 - 150,000
Band 3: More than $150,000
Following are the outputs of the 5 different CART model.
CART 1 : Variable Age

CART 2 : Variable Gender

CART 3 : Variable Education

CART 4 : Variable Residence

CART 5 : Variable Industry

Using these 5 CART models, we need to come up with singe set of probability to belong to each of the salary classes. For simplicity, we will just take a mean of probabilities in this case study. Other than simple mean, we also consider vote method to come up with the final prediction. To come up with the final prediction let’s locate the following profile in each CART model :
1. Age : 35 years , 2, Gender : Male , 3. Highest Educational Qualification : Diploma holder, 4. Industry : Manufacturing, 5. Residence : Metro
For each of these CART model, following is the distribution across salary bands :

The final probability is simply the average of the probability in the same salary bands in different CART models. As you can see from this analysis, that there is 70% chance of this individual falling in class 1 (less than $40,000) and around 24% chance of the individual falling in class 2.
 End Notes 
Random forest gives much more accurate predictions when compared to simple CART/CHAID or regression models in many scenarios. These cases generally have high number of predictive variables and huge sample size. This is because it captures the variance of several input variables at the same time and enables high number of observations to participate in the prediction. In some of the coming articles, we will talk more about the algorithm in more detail and talk about how to build a simple random forest on R.
If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.
Share this:Pocket

	Related
			
						
						
		"
"Data manipulations in Hadoop - using MapReduce","Tavish Srivastava",2014-06-01 22:51:00,1,"
							
										
						Magic is a performing art that entertains audiences by staging tricks or creating illusions of seemingly impossible or supernatural feats using natural means (Source : Wikipedia) . If you understand the last sentence, you will have no challenge leveraging MapReduce routine for solving any kind of data manipulation challenge. In the last two articles, we gave an Introduction to Hadoop and MapReduce. We saw that Map-Reduce can handle big data but has a stringent format. It can only structure data using Mapper and summarize data using the reducer. This article will help you understand how to “trick” Hadoop to see different kind of data manipulation functionality as simple MapReduce function. 
Note that this article talks only about MapReduce used along with HDFS, it does not talk about packages such as PIG, HIVE, etc. Using these packages, you might not have to use these “tricks” because these packages will enable you to build simple SQL queries. But as of now, such packages are not widely used and most industries are using the basic Hadoop tool (MapReduce with HDFS). 
 The analogy 
Let’s analyze a situation to better understand this MapReduce functionality.
“Magician prepares for a new trick. Now, he shows a trick  to the audience. Audience only looks at what Magician wants to show them. “
Let’s analyze this situation with reference to MapReduce.
Following are the role-play :
Magician : User
Audience : Hadoop
Trick : Data manipulation functionality

Now let’s replace the words used in the last sentence with mapped Hadoop jargons.
“User prepares for a new data manipulation functionality. Now, he (user) executes the data manipulation functionality on Hadoop. Hadoop only looks at  what user wants to show to the Hadoop (MapReduce type function).”
Let us demonstrate the concept by taking a few examples – Show time!
 Example 1 : Word Count 
You have a bunch of text files in your system and would like to count the overall number of occurrences of each word. This example has already been covered in the last article.
Map: For each distinct word, generate a key value pair of (word,1)
Shuffle: For each key (word) w, produced by any of the Map tasks, there will be one or more key-value pairs (w,1). Then, for key w, the shuffle will produce (w, [1, 1,..., 1]) and present this as input to a reducer.
Reduce: the reducer associated with key (word) w turns (w, [1, 1, ..., 1]) into (w, n), where n is the sum of the values. Note that the reduce phase produces exactly one output pair for each key w.
 Example 2 : Removing Duplicates 
We basically would like to implement the distinct operator: the desired output should contain all of the input values, but values that appear multiple times in input, should appear only once in output. 

The solution is simpler than that of word count problem: we use the value itself as the map output key; the reducer associated with a specific key, can then return a single output.
Map: For each input value id and marks , output the key-value pair (key = id , value = (id,marks))
Shuffle: For each key id produced by any of the Map tasks, there will be one or more key-value pairs . Then, for key id, the shuffle will produce (id, [(id,marks) , (id,marks)...]) and present this as input to a reducer.
Reduce: the reducer for key id turns (id, [(id,marks) , (id,marks)...]) into single (id, (id,marks), so it produces exactly one output (id,marks) for this key id.
Note that Reducer can select the (id,marks) pair based on any logic. For instance, we might want to know the highest marks scored by any student. In this case, we will take the maximum of all marks on every id.
 Example 3 : Matrix transpose 
Here we just want to transpose a matrix. Matrix transposition simply means that every element (row, column) comes to the position (column , row).
Let us denote each of these value with a row,column index. For instance 1 will (1,1), 2 will be (1,2) etc.
Map: For each input (x,y) , generate key as (y,x) and values as the actual value in matrix. For instance 4 will be taken as (key=(1,2),value=4)
Shuffle: As there is a single value for every key, shuffle phase doesn’t have any significant role. However, it will sort all the values as per the key.
Reduce: The reducer will directly return the values by accepting all the keys as (row,column) for the matrix.
 Example 4 : Merging tables 
Merging tables is the most frequent data manipulation used in analytics industry. We have two tables : table 1 & table 2. They both have a common variable (id variable) called Name. We need to merge the two tables and find marks corresponding to each names.
First step in this process is to append the two tables together. Then follow the algorithm given below :
Map: For each input of table 1  , generate key as ‘id’ and values as ,(“tab1<U+2033>,id,marks) & for each input of table 2  , generate key as ‘id’ and values as ,(“tab2<U+2033>,id,marks)  . For instance (2,34) will be taken as (key=2,value=(“tab1<U+2033>,2,34))
Shuffle: The shuffle phase will bring all the same id together. You can identify the ownership of each row by the first argument in value.
Reduce: The reducer has to be deigned in such a way that for each id with first argument as “tab1<U+2033>, find all the matches on id with first argument as “tab2<U+2033> and combine them to single row.
Note that this algorithm will create a Cartesian product of the input matrix.
 End Notes 
The list of examples listed in this article is not exhaustive. We need to change the way we look at data manipulation task on Hadoop. We need to start thinking in terms of (key,value) pairs. We saw in this article that Hadoop does restrict the way of coding for the user, but even this restricted style of coding can be exploited to execute several data manipulation processes.
Before I end this article, here is something to scratch your brains: How can you implement matrix multiplication in Hadoop? Do let me know your thoughts / answers / questions / challenges on usage of MapReduce for data manipulation through the comments below.
If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.
Share this:Pocket

	Related
			
						
						
		"
"Introduction to MapReduce","Tavish Srivastava",2014-05-28 09:41:00,4,"
							
										
						MapReduce is a programming model for processing large data sets with a parallel , distributed algorithm on a cluster (source: Wikipedia). Map Reduce when coupled with HDFS can be used to handle big data. The fundamentals of this HDFS-MapReduce system, which is commonly referred to as Hadoop was discussed in our previous article. 
The basic unit of information, used in MapReduce is a (Key,value) pair. All types of structured and unstructured data need to be translated to this basic unit, before feeding the data to MapReduce model.  As the name suggests, MapReduce model consist of two separate routines, namely Map-function and Reduce-function. This article will help you understand the step by step functionality of Map-Reduce model.The computation on an input (i.e. on a set of pairs) in MapReduce model occurs in three stages:

Step 1 :  The map stage
Step 2 : The shuffle stage
Step 3 :  The reduce stage.
Semantically, the map and shuffle phases distribute the data, and the reduce phase performs the computation. In this article we will discuss about each of these stages in detail.
 The Map stage 
MapReduce logic, unlike other data frameworks, is not restricted to just structured datasets. It has an extensive capability to handle unstructured data as well. Map stage is the critical step which makes this possible. Mapper brings a structure to unstructured data. For instance, if I want to count the number of photographs on my laptop by the location (city), where the photo was taken, I need to analyze unstructured data. The mapper makes (key, value) pairs from this data set. In this case, key will be the location and value will be the photograph. After mapper is done with its task, we have a structure to the entire data-set.
In the map stage, the mapper takes a single (key, value) pair as input and produces any number of (key, value) pairs as output . It is important to think of the map operation as stateless, that is, its logic operates on a single pair at a time (even if in practice several input pairs are delivered to the same mapper).  To summarize, for the map phase, the user simply designs a map function that maps an input (key, value) pair to any number (even none) of output pairs. Most of the time, the map phase is simply used to specify the desired location of the input value by changing its key.
 The shuffle stage 
The shuffle stage is automatically handled by the MapReduce framework, i.e. the engineer has nothing to do for this stage. The underlying system implementing MapReduce routes all of the values that are associated with an individual key to the same reducer.
 The Reduce stage 
In the reduce stage, the reducer takes all of the values associated with a single key k and outputs any number of (key, value) pairs. This highlights one of the sequential aspects of MapReduce computation: all of the maps need to finish before the reduce stage can begin. Since the reducer has access to all the values with the same key, it can perform sequential computations on these values. In the reduce step, the parallelism is exploited by observing that reducers operating on different keys can be executed simultaneously. To summarize, for the reduce phase, the user designs a function that takes in input a list of values associated with a single key and outputs any number of pairs. Often the output keys of a reducer equal the input key (in fact, in the original MapReduce paper the output key must equal to the input key, but Hadoop relaxed this constraint). 
Overall, a program in the MapReduce paradigm can consist of many rounds (usually called jobs) of different map and reduce functions, performed sequentially one after another. 
 An example 
Let’s consider an example to understand Map-Reduce in depth. We have the following 3 sentences :
1. The quick brown fox
2. The fox ate the mouse
3. How now brown cow
Our objective is to count the frequency of each word in all the sentences. Imagine that each of these sentences acquire huge memory and hence are allotted to different data nodes.  Mapper takes over this unstructured data and creates key value pairs. In this case key is the word and value is the count of this word in the text available at this data node.  For instance, the 1st Map node generates 4 key-value pairs  : (the,1), (brown,1),(fox,1), (quick,1). The first 3 key-value pairs go to the first Reducer and the last key-value go to the second Reducer.

Similarly, the 2nd and 3rd map functions do the mapping for the other two sentences. Through shuffling, all the similar words come to the same end. Once, the key value pairs are sorted, the reducer function operates on this structured data to come up with a summary.
  End Notes :  
Let’s take some example of Map-Reduce function usage in the industry :
• At Google:
–<U+202F>Index building for Google Search
–<U+202F>Article clustering for Google News
–<U+202F>Statistical machine translation
•<U+202F> At Yahoo!:
–<U+202F>Index building for Yahoo! Search
–<U+202F>Spam detection for Yahoo! Mail
•<U+202F> At Facebook:
–<U+202F>Data mining
–<U+202F>Ad optimization
–<U+202F>Spam detection Example
•<U+202F> At Amazon:
–<U+202F>Product clustering
–<U+202F>Statistical machine translation
The constraint of using Map-reduce function is that user has to follow a  logic format. This logic is to generate key-value pairs using Map function and then summarize using Reduce function. But luckily most of the data manipulation operations can be tricked into this format. In the next article we will take some example like how to do data-set merging, matrix multiplication, matrix transpose, etc. using Map-Reduce.
Did you find the article useful? Share with us any other practical examples of Map-Reduce function. Do let us know your thoughts about this article in the box below.
If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.
Share this:Pocket

	Related
			
						
						
		"
"What is Hadoop? - Simplified!","Tavish Srivastava",2014-05-21 19:19:00,4,"
							
										
						
Scenario 1: Any global bank today has more than 100 Million customers doing billions of transactions every month
Scenario 2: Social network websites or eCommerce websites track customer behaviour on the website and then serve relevant information / product.
Traditional systems find it difficult to cope up with this scale at required pace in cost-efficient manner.
This is where Big data platforms come to help. In this article, we introduce you to the mesmerizing world of Hadoop. Hadoop comes handy when we deal with enormous data. It may not make the process faster, but gives us the capability to use parallel processing capability to handle big data. In short, Hadoop gives us capability to deal with the complexities of high volume, velocity and variety of data (popularly known as 3Vs).
Please note that apart from Hadoop, there are other big data platforms e.g. NoSQL (MongoDB being the most popular), we will take a look at them at a later point.
 Introduction to Hadoop  
Hadoop is a complete eco-system of open source projects that provide us the framework to deal with big data. Let’s start by brainstorming the possible challenges of dealing with big data (on traditional systems) and then look at the capability of Hadoop solution.
Following are the challenges I can think of in dealing with big data :
1. High capital investment in procuring a server with high processing capacity.
2. Enormous time taken
3. In case of long query, imagine an error happens on the last step. You will waste so much time making these iterations.
4. Difficulty in program query building
Here is how Hadoop solves all of these issues :
1. High capital investment in procuring a server with high processing capacity: Hadoop clusters work on normal commodity hardware and keep multiple copies to ensure reliability of data. A maximum of 4500 machines can be connected together using Hadoop.
2. Enormous time taken : The process is broken down into pieces and executed in parallel, hence saving time. A maximum of 25 Petabyte (1 PB = 1000 TB) data can be processed using Hadoop.
3. In case of long query, imagine an error happens on the last step. You will waste so much time making these iterations : Hadoop builds back up data-sets at every level. It also executes query on duplicate datasets to avoid process loss in case of individual failure. These steps makes Hadoop processing more precise and accurate.
4. Difficulty in program query building  : Queries in Hadoop are as simple as coding in any language. You just need to change the way of thinking around building a query to enable parallel processing.
 Little background of Hadoop  
With an increase in the penetration of internet and the usage of the internet, the data captured by Google increased exponentially year on year. Just to give you an estimate of this number, in 2007 Google collected on an average 270 PB of data every month. The same number increased to 20000 PB everyday in 2009. Obviously, Google needed a better platform to process such an enormous data. Google implemented a programming model called MapReduce, which could process this 20000 PB per day. Google ran these MapReduce operations on a special file system called Google File System (GFS). Sadly, GFS is not an open source.
Doug cutting and Yahoo! reverse engineered the model GFS and built a parallel Hadoop Distributed File System (HDFS). The software or framework that supports HDFS and MapReduce is known as Hadoop. Hadoop is an open source and distributed by Apache.
 Framework of Hadoop processing  
Let’s draw an analogy from our daily life to understand the working of Hadoop. The bottom of the pyramid of any firm are the people who are individual contributors.  They can be analyst, programmers, manual labors, chefs, etc. Managing their work is the project manager. The project manager is responsible for a successful completion of the task. He needs to distribute labor, smoothen the coordination among them etc.  Also, most of these firms have a people manager, who is more concerned about retaining the head count.

Hadoop works in a similar format. On the bottom we have machines arranged in parallel. These machines are analogous to individual contributor in our analogy. Every machine has a data node and a task tracker. Data node is also known as HDFS (Hadoop Distributed File System) and Task tracker is also known as map-reducers.
Data node contains the entire set of data and Task tracker does all the operations. You can imagine task tracker as your arms and leg, which enables you to do a task and data node as your brain, which contains all the information which you want to process. These machines are working in silos and it is very essential to coordinate them. The Task trackers (Project manager in our analogy) in different machines are coordinated by a Job Tracker. Job Tracker makes sure that each operation is completed and if there is a process failure at any node, it needs to assign a duplicate task to some task tracker. Job tracker also distributes the entire task to all the machines.
A name node on the other hand coordinates all the data nodes. It governs the distribution of data going to each machine. It also checks for any kind of purging which have happened on any machine. If such purging happens, it finds the duplicate data which was sent to other data node and duplicates it again.  You can think of this name node as the people manager in our analogy which is concerned more about the retention of the entire dataset.

  Where not to use Hadoop ? 
Till now, we have seen how Hadoop has made handling big data possible. But in some scenarios Hadoop implementation is not recommended. Following are some of those scenarios :
Low Latency data access : Quick access to small parts of data
Multiple data modification : Hadoop is a better fit only if we are primarily concerned about reading data and not writing data.
Lots of small files : Hadoop is a better fit in scenarios, where we have few but large files.
  End Notes :  
This article gives you a view on how Hadoop comes to the rescue when we deal with enormous data. Understanding of the working of Hadoop is very essential before starting to code for the same. This is because you need to change the way of thinking of a code. Now you need to start thinking of enabling parallel processing. You can do many different types of processes on Hadoop, but you need to convert all these codes into a map-reduce function. In the next few articles we will explain how you can convert your simple logic to Hadoop based Map-Reduce logic. We will also take R-language specific case studies to build a solid understanding of the application of Hadoop.
Did you find the article useful? Share with us any practical application of Hadoop you encountered in your work . Do let us know your thoughts about this article in the box below.
You might be interested in: Introduction to MapReduce
If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.
Share this:Pocket

	Related
			
						
						
		"
"4 Tricky R interview questions","Tavish Srivastava",2014-05-14 14:52:00,4,"
							
										
						Analytics industry in India is dominated by SAS currently. But, it will be too optimistic to hope that this remains to in years to come. R, on the other hand is open source, and can be implemented in any environment. SAS grows by efforts of smart people employed by SAS but  R grows by the effort of anyone who works on the language. Anyone can contribute to the language R. Hence, I feel that every analyst should develop expertise in both the languages.
There are some key differences in coding on R vs. coding on SAS. This makes some of the interview questions on R tricky and handling them becomes overwhelming for some candidates. I strongly feel a need of a common thread which has all the tricky R questions asked in interviews. This article will give a kick-start to such a thread.  We have a similar series of articles published on SAS (Part 1 and Part 2). Please note that the content of this article is based on the information I gathered from various R sources.
 Question 1 : Rotational multiplication 
You have two vector defined as follows :

> a <- c(2,3,4) 
> b <- c(1,2)

What is the value of the vector d, which is defined as follows :

> d <- a*b

Answer : 2 , 6 , 4
R language does vectorized operations. ‘a’ and ‘b’ are two vectors with different length. By process, R multiplies the first element of a with 1st element of b, than second element of a with that of b, and so on. But in this case, after the second multiplication R hits the end of vector “b”. In such cases R, starts with the first element of smaller vector till each element of longer vector is exhausted. The vectorized operation always leads to a vector of length equal to that of longer vector.
 Question 2 : Scoping Rules 
You need to understand the following code and answer a question based on this understanding.

> y <- 3
> f <- function(x) {
+                            y <- 2
+                            y ^ 2 + g(x)
+                            }
> g <- function(x) {
+                             x * y
+                             }

What is the value of f(6)?
Answer : 22
If you answered anything other than 22, you probably need to refresh the lexical scoping in R.  The function f(x) returns a value y^2 + g(x). y in this environment has been defined as 2 and g(x) from inside this function. The value of x is passed of function g as 6. Now comes the catch, what is the value of free variable y here? Unlike dynamic environment where the value is assumed from the parent environment, lexical scoping assumes the value of a variable from the environment where the function is defined. The function g(x) is defined in the global environment here, and hence the value of y is assumed to be 3. Therefore a value of 18 is returned from the function g(x).  f(6) is finally returning as 22.
 Question 3 : Summarizing at each factor 
You have been assigned to check two race tracks. To complete this task you are expected to find the means of the total time taken by cars to cross the track. In the following data assignment, “b” is the vector of total time taken by different cars and “a” is the vector of track on which this time is taken. The first element of the vector “b” corresponds to the first element of vector “a” (and so on).

> a <- c(1,1,1,1,2,2,2,2,2)
> b <- c(10,12,15,12,NA,30,42,38,40)

How do you find the mean time of each track using split function?
Answer : Code is as follows 

> s <- split(b,a)
> lapply(s,mean)

 Question 4 : Treating missing values 
Following is the output of the last section :

$`1` [1] 12.25
$`2` [1] NA

How do you modify the code, to treat the missing value in the second track record?
Answer : The modified code is as follows :

> lapply(s,mean,na.rm=TRUE)
$`1` [1] 12.25
$`2` [1] 37.5

End Notes : 
Coders are lazy! and R language is built for coders. Codes in R are much more compact as compared to SAS. But it makes the language more difficult to retain all the syntax. You will probably need a lot of practice to get a hang of it (if you have been using SAS extensively). In one of our coming articles, we will compare coding in SAS and R. Have you faced any other R problem in analytics interview? Are you facing any specific problem with R codes?  Do you think this provides a solution to any problem you face? Do you think there are other methods to solve the problems discussed in a more optimized way? Do let us know your thoughts in the comments below.
If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.
Share this:Pocket

	Related
			
						
						
		"
"Building a word cloud using R","Tavish Srivastava",2014-05-07 06:04:00,4,"
							
										
						
 This is how a word cloud of our entire website looks like!
A word cloud is a graphical representation of frequently used words in a collection of text files. The height of each word in this picture is an indication of frequency of occurrence of the word in the entire text. By the end of this article, you will be able to make a word cloud using R on any given set of text files. Such diagrams are very useful when doing text analytics.
 Why do we need text analytics? 
Analytics is the science of processing raw information to bring out meaningful insights. This raw information can come from variety of sources. For instance, let’s consider a modern multinational bank, who wants to use all the available information to drive the best strategy. What are the sources of information available to the bank?
1. Bank issues different kinds of products to different customers. This information is fed to the system and can be used for targeting new customers, servicing existing customers and forming customer level strategies.
2. Customers of bank would be doing millions of transactions everyday. The information about where these transactions are done, when they are done and what amount of transactions where they helps bank to understand their customer.
There can be other behavioral variables (e.g. cash withdrawal patterns) which can provide the bank with valuable data, which helps the bank build optimal strategy. This analysis gives the bank, a competitive edge over other market players by targeting the right customer, with the right product at the right time. But, given that, at present every competitor is using similar kind of tools and data, analytics have become more of a hygiene factor rather than competitive edge. To gain the edge back, the bank has to find more sources of data and more sophisticated tools to handle this data.  All the data, we have discussed till this point is the structured data. There are two other types of data, the bank can use to drive insightful information.
1. System data : Consider a teller carrying out a transaction at one of the counter. Every time he completes a transaction, a log is created in the system. This type of data is called system data. It is obviously enormous in volumes, but still not utilized to a considerable extent in a lot of banks. If we do analyze this data, we can optimize the number of tellers in a branch or scale the efficiency of each branch.
2. Unstructured data : Feedback forms with free text comments, comments on Bank’s Facebook Page, twitter page, etc. are all examples of unstructured data.  This data has unique information about customer sentiment. Say, the bank launches a product and found that this product is very profitable in first 3 months. But customers who bought the product found that this product was really a bad choice and started spreading bad words about the product on all social networks and through feedback channels. If the bank has no way to decode this information, this will lead to a huge loss because the bank will never make a proactive effort to stop the negative wave against its image. Imagine, the kind of power analyzing such data hands over to the bank.
 Installing required packages on R  
Text Mining needs some special packages, which might not be pre-installed on your R software.  You need to install Natural Language Processing package to load a library called tm and SnowballC.  Follow the instructions written in the box to install the required packages.

> install.packages(“ctv”)
> library(“ctv”)
> install.views(“NaturalLanguageProcessing”)

 Step by step coding on R 
Following is the step by step algorithm of creating a word cloud on a bunch of text files. For simplicity, we are using files in .txt format.
Step 1 : Identiy & create text files to turn into a cloud
The first step is to identify & create text files on which you want to create the word cloud.  Store these files in the location “./corpus/target”. Make sure that you do not have any other file in this location. You can use any location to do this exercise, but for simplicity, try it with this location for the first time.
Step 2 : Create a corpus from the collection of text files
The second step is to transform these text files into a R – readable format. The package TM and other text mining packages operate on a format called corpus. Corpus is just a way to store a collection of documents in a R software readable format.

 
> cname <- file.path(""."",""corpus"",""target"")
> library (tm)
> docs <- Corpus(DirSource(cname))


Step 3 : Data processing on the text files
This is the most critical step in the entire process. Here, we will decode the text file by selecting some keywords, which builds up the meaning of the sentence or the sentiments of the author. R makes this step really easy. Here we will make 3 important transformations.
i. Replace symbols like “/” or “@” with a blank space
ii. Remove words like “a”, “an”, “the”, “I”, “He” and numbers. This is done to remove any skewness caused by these commonly occurring words.
iii.  Remove punctuation and finally whitespaces. Note that we are not replacing these with blanks because grammatically they will have an additional blank.

 
> library (SnowballC)
> for (j in seq(docs))
+ {docs[[j]] <- gsub(""/"","" "",docs[[j]])
+  docs[[j]] <- gsub(""@"","" "",docs[[j]])}
> docs <- tm_map(docs,tolower)
> docs <- tm_map(docs,removeWords, stopwords(""english""))
> docs <- tm_map(docs,removeNumbers)
> docs <- tm_map(docs,removePunctuation)
> docs <- tm_map(docs,stripWhitespace)

Step 4 : Create structured data from the text file
Now is the time to convert this entire corpus into a structured dataset. Note that we have removed all filler words. This is done by a command “DocumentTermMatrix” in R. Execute the following line in your R session to make this conversion.

 
> dtm <- DocumentTermMatrix(docs)

 
Step 5 : Making the word cloud using the structured form of the data
Once, we have the structured format of the text file content, we now make a matrix of word and their frequencies. This matrix will be finally put into the function to build wordcloud.
 
 
> library(wordcloud)

> m <- as.matrix(dtm)

> v <- sort(colSums(m),decreasing=TRUE)

> head(v,14)
> words <- names(v)

> d <- data.frame(word=words, freq=v)

> wordcloud(d$word,d$freq,min.freq=50)

Running this code will give you the required output. The order of words is completely random but the length of the words are directly proportional to the frequency of occurrence of the word in text files.Our website has the world “Analytics Vidhya” repeated many times and hence this word has the maximum length. This diagram directly helps us identify the most frequently used words in the text files.
 End Notes 
Text mining deals with relationships between words and analyzing the sentiment made by the combination of these words and their relationship. Structured data have defined number of variables and all the analysis is done by finding out correlation between these variables. But in text mining the relationship is found between all the words present in the text.  This is the reason text mining is rarely used in the industry today. But R offers such tools which make this analysis much simpler. This article covers only the tip of the iceberg. In one of the coming articles, we will cover a framework of analyzing unstructured data.
Did you find the article interesting? Have you worked on text mining before? Did you use R to do text mining or some other softwares? Let us know any other interesting feature of R used for text mining.
If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.
 
Share this:Pocket

	Related
			
						
						
		"
"Framework to build survival analysis model on R","Tavish Srivastava",2014-04-27 22:25:00,1,"
							
										
						
In the last article, we introduced you to a technique often used in the analytics industry called Survival analysis. We also talked about some of the application of this technique across the industries. In this article we will layout a simple framework to use survival analysis on the tool R.
 Case Study (Background)  
You are the head of the analytics team with an online Retail chain Mazon. You have received a limited number of offers which costs you $200/customer targeted . The offer breaks even if a customer makes a purchase of minimum $20,000 in his entire lifetime. You want to target customers who are likely to make a purchase of $20,000 as early as possible. You have their relationship card segment (Platinum cards are expensive and Gold card is cheaper) and their response rate on last campaign they were targeted with. You have been targeting customers with this offer for past 1 year. Now you want to learn from the past response data and target accordingly. You need to find which customer base should you target for this offer. You will have to use survival analysis in this case because the dependent variable is the time to respond the campaign. This again contains censored data which are people who did not respond till date.
 Data Structure 
 The raw data includes a unique ID, 2 input variables and 2 target variables. The first target variable is months, which indicates the number of months customer has not completed a total purchase of $20,000.  The second target variable is Purchase_2k, which indicates if the customer finally makes a purchase of $20,000. Any customer who have not yet made the purchase is not a non-responder but a censored data.
Censored data is an observation for whom the actual response tag is unknown. Say, customer id “213<U+2033> made a purchase of $19,999 till month 8 when the data was collected. His month variable will have a value of 8 and Purchase_2k will have a value of 0. But the next day, customer 213 completed $20,000. Hence, on the date of collection this data should not be treated as a non-responder but a censored observation. Other direct techniques like logistic regression cannot take censored data into the model. However, survival analysis has a capability to take them into account.

 Theory behind Survival analysis (Optional read) 
Survival and hazard functions : Survival analysis is modelling of the time to death. But survival analysis has a much broader use in statistics. Any event can be defined as death. For example, age for marriage, time for the customer to buy his first product after visiting the website for the first time, time to attrition of an employee etc. all can be modeled as survival analysis.
Let’s say T is a random variable which signifies the variable time. We define the function f(t) as the probability distribution function (pdf) on this random variable. F(t) is the cumulative distribution function with F(t) = Pr { T < t }.  Now the survival function S(t) is mathematically written as follows :

Finally, we define a Hazard function (instantaneous rate of occurrence of the event) mathematically as follows :


From the above relationships, it is clear that if one of the Survival function, pdf or Hazard function is known, others can be calculated easily. To estimate these functions we have three kinds of solutions. Following are the three ways of estimation :
1. Non-parametric solution : Simplest solution which can be used for descriptive analysis, but cannot be extrapolated to find out the survival of censored data with high time span.
2. Semi – Parametric solution : Widely used in the industry.  Will be discussed in detail in this article.
3.  Parametric solution : We will not touch up on this route of estimation. The reason being the parameters found by different software have different signs. We will cover this in one of the coming articles.
 Solving survival analysis on R ( Initializing)  
To model survival analysis in R, we need to load some additional packages. Following are the initial steps you need to start the analysis.
Step 1 : Load Survival package
Step 2 : Set working directory
Step 3 : Load the data set to the temporary memory of R

 
> library(survival)
> setwd (“D:/”)
> mydata <- read.csv (“D:/worksheet.csv”)
> attach(mydata)

Once we have data in our temporary memory, you need to create an array of input variable. Note that, till this point both non-parametric and semi-parametric have the same standard codes. We will use different codes for the two later in the process.

 
> Cust <- Customer_id
> l_resp <- Last_Response_tag
> Plat <- Platinum_flag
> span <- Months
> Resp <- Purchase_2k
> X <- cbind(l_resp,Plat)

Here, we have clubbed the input or defining functions. Also note that span and Resp are both target functions.
 Non-parametric solution 
Non-parametric solution to survival analysis problem gives a directional view on which profile has a better survival rate. It cannot be extrapolated to higher time span predictions. Use following steps to create a survival curve and get insights to the overall portfolio survival view.

 
> kmsurv <- survfit(Surv(span,Resp) ~ 1)
> summary(kmsurv)
> plot(kmsurv,xlab = “span”,ylab=”Survival Proabability”)

 

Above is the graph we get after executing the codes. Let’s try to understand this curve. This is a survival curve, which shows following facts about the population:
1. The curve starts from a point below 1, which means some of the observation/customer made an immediate purchase of $20,000 just after receiving the offer (in month 0)
2. After 6 months, around 62% of the population have survived. In other words, 38% of the population has made a purchase of more than $20,000
3. Around 38% of the population survives even after 12 months. This does not mean they will never make a purchase. But from a non-parametric solution we cannot extrapolate the solution for more than 12 months.
To make a deep-dive into this population, lets look at the survival curve of individual strata. Strata are different levels of input variable for the population.In this case study, we have 2 levels of 2 input variables.
1st variable (Response of last time) : Any individual customer will either have responded in the last offer or not. Hence, this variable has two levels.
2nd variable (Membership) : There are two types of membership which we offer. Either a customer can be Gold or Platinum. Again, this variable has two levels.
Execute following code to get survival curves on these individual levels.

 
> group1 <- l_resp
> group2<- Plat
> kmsurv1 <- survfit(Surv(span,Resp) ~ group1)
>  summary(kmsurv1)
>  plot(kmsurv1,xlab = “span”,ylab=”Survival Proabbility”)

Above is the curve you get on execution of the code. Note that curve with higher values of probability is for group1 =0 or non-responders of last campaign. Hence we can infer from this graph that non-responders of last campaign have a higher probability to not respond for this campaign as well compared to rest at any point of time till 12 months. We can do a similar exercise for package. Here again we find customer with Platinum package has a higher probability to make a purchase of $20,000 in any number of months till 12 months.
 Semi-parametric solution 
Cox(1972) introduced an approach which directly focuses on estimating the hazard function directly using both time and profile variables. Following is the equation for hazard which we want to solve :

Here is the code you need to write to execute Cox hazard model.

 
coxph <- coxph(Surv(span,Resp)~X,method = “breslow”)
summary(coxph)

Let’s try to understand the output step by step.
1. Data summary :  The first line of the output summarizes the entire data. In total we have 199 observation out of which for 84 observations, the event has already occurred.

2. Coefficient estimates: These estimates help us understand the impact of profile on survival rate. As you can see, for both the variables p value is low and, hence, both the variables are significant. Also both of them have a positive sign which implies two facts about the population :
1. Platinum customers have a higher probability of making a purchase of more than $20,000
2. Last campaign responders have a higher probability of making a purchase of more than $20,000

3. Marginal effect table : This table does not have anything new compared to table 2. But, it teller you the % increase in the risk of event happening (which in this case is a good thing) by unit increase in input variable (which in this case are only two level variables).  Hence, following are the insights coming out of this table :
1.  Platinum customers have 230% higher chance to make a purchase of more than $20,000.
2.  Last campaign responders have 290% higher chance to make a purchase of more than $20,000.

 
End Notes 
Survival analysis provides a solution to a set of problems which are almost impossible to solve precisely in analytics. These solutions are not that common at present in the industry, but there is no reason to suspect its high utility in the future. In this article we covered a framework to get a survival analysis solution on R. In one of our future articles, we will also cover doing survival analysis in SAS.
Did you find any opportunity in your line of business where you can implement survival analysis? Did you find this article helpful? Have you worked on other cutting edge modelling techniques in the recent past? Were the results encouraging? Do let us know your thoughts in the comments below.
If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.
 
Share this:Pocket

	Related
			
						
						
		"
"Is survival analysis the right model for you?","Tavish Srivastava",2014-04-21 04:56:00,2,"
							
										
						
I was a post-graduate in Mechanical Engineering when I joined the analytics industry as a fresher. 
The only background I had in analytics industry was based on a few courses in Operations Research. I was scared to join the industry, where I knew lesser than a student of statistics in 12th standard. In less than a month, I realized that Analytics industry does not require you to have a masters in statistics or economics, but requires structured thinking with sharp mathematical reflexes. It took me no more than 3 months to build and implement my first logistic regression model. Most of the people in India still use basic analytics tools such as CART, regressions and time series. We still are scared of using complex statistical techniques, such as neural network and survival analysis. Last year, I used survival analysis in one of the analytics projects and realized the power of the tool without getting into the Limbo of statistics behind the tool. This article will help you find if the survival analysis is the right tool for your next project. The article will end with a case study, which we will solve using survival analysis in the next article.
How is survival analysis different from regression models?
Regression models have a single output function. In case of a logistic regression, the output is the response function which can only take two values. In any other model, we can define the output function in a single objective function. For instance, if we are building a customer attrition model, which predicts whether a customer will attrite in next 3 months, following is the objective of the logistic model :
f(x) =  0    if no attrition in next 3 months
=  1    if customer attrited in next 3 months
Say, we want to profile the customers, who are likely to attrite early and restrict the acquisition of such profile customers. Let’s assume for simplicity that there are only 2 variables : Gender and Tenure. We build a logistic model in Jan ’13 and found that out of 100 Males 30 attrite till Jan’13, whereas out of 100 females only 10 attrite till Jan’13. My model profiles females as better profile, but because of some reason we did not implement this factor into our acquisition strategy. Now, we stand in Jul’13 and if we look at the same population as considered in the logistic model,  out of 100 Males 35 attrite and out of 100 Females 55 attrite.
The results seem to have swapped. In last 6 months, females saw high attrition, whereas male population seems to be very stable over this period. Now you observe that females had a lower tenure in Jan’13 as compared to the male population. The possible solution in this case is to take same month tranches/acquisitions to build a model. You now take only customer who was acquired before Jan’12. You get a population of 50 Males and 10 Females. However, you have reduced the noise coming from new tranches, you have also reduced the population you are building a model on. How do you address this issue?
Such data for whom the results (attrition in this case) are unknown, are called censored data. We can include this data without compromising on the model accuracy in a survival analysis model. This is because the output or target variable of a survival analysis is a combination of death (attrition in this case) and time on books (tenure of customer).
Applications of survival analysis
There are four major applications of survival analysis into analytics:
1. Business Planning : Profiling customers who has a higher survival rate and make strategy accordingly.
2. Lifetime Value Prediction : Engage with customers according to their lifetime value
3. Active customers : Predict when the customer will be active for the next time and take interventions accordingly.
4. Campaign evaluation : Monitor effect of campaign on the survival rate of customers.
  Following are some industrial specific applications of survival analysis :
• Banking – customer lifetime and LTV
• Insurance – time to lapsing on policy
• Mortgages – time to mortgage redemption
• Mail Order Catalogue – time to next purchase
• Retail – time till food customer starts purchasing non-food
• Manufacturing – lifetime of a machine component
• Public Sector – time intervals to critical events
Case study 
You are the head of the analytics team with an online Retail chain Mazon. You have received a limited number of offers which costs you $200/customer targeted . The offer breaks even if a customer makes a purchase of minimum $20,000 in his entire lifetime. You want to target customers who are likely to make a purchase of $20,000 as early as possible. You have their relationship card segment (Platinum cards are expensive and Gold card is cheaper) and their response rate on last campaign they were targeted with. You have been targeting customers with this offer for past 1 year. Now you want to learn from the past response data and target accordingly. You need to find which customer base should you target for this offer. You have data for a similar test campaign in the past, based on which you will have to build the strategy.
You will have to use survival analysis in this case because the dependent variable is the time to respond the campaign. This again contains censored data which are people who did not respond till date. We will solve this case study in the next article where we will lay out a step by step process doing a survival analysis to find profile of customers who respond early.
End notes 
In the last three years, I have realized that analytics project are not always created by the business. The business itself is constrained by what value they have seen in the past coming from the analytics team. Learning new modelling techniques and learning from nextgen analytics project make us capable of seeing the value beyond what business can see. We will like to know the new techniques you have learnt in recent past and their applications.  The objective to start a discussion on survival analysis here is not restricted to only this technique.
Did you find any opportunity in your line of business where you can implement survival analysis? Did you find this article helpful? Have you worked on other cutting edge modelling techniques in the recent past? Were the results encouraging? Do let us know your thoughts in the comments below.
If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.
Share this:Pocket

	Related
			
						
						
		"
"Tricky Base SAS interview questions : Part-II","Tavish Srivastava",2014-04-13 13:22:00,1,"
							
										
						
SAS is the largest market-share holder for advanced analytics.
If you are going to work in analytics industry, it is impossible to escape from the language SAS. Different softwares of SAS are used in the industry for data handling. Most common of them used in data handling are SAS Enterprise guide and Base SAS. Both the tools have a similar format and usage. The only difference being SAS Enterprise guide is the graphical user interface of Base SAS. SAS Enterprise guide is much easier to use and is modular in nature. Because of the modularity, SAS  Enterprise guide is being widely used in the industry. With time Base SAS is losing its importance in the industry, and SAS Enterprise guide is filling in for Base SAS. However, having used both, I now appreciate and prefer using Base SAS to handle data of billions of customers and trillions of transactions.
One of our previous articles (http://www.analyticsvidhya.com/blog/2013/11/4-sas-tricky-analytics-interview/) covers four tricky questions asked in SAS interviews. In this article, I will cover some tricky scenarios in which using base SAS will become far easier than using the SAS Enterprise guide. These questions are tougher and lengthier than those covered in the first part of this article series. These questions are asked widely in companies who have a broad base of analytics and deal with big data (Millions of customers, Billions of transactions, Trillions of dollar value transactions).
Base SAS vs. SAS Enterprise guide
Let’s first look into the pros of using both Base SAS and SAS Enterprise guide. This will make us appreciate those tricky scenarios, even more.
SAS Enterprise guide
1. EG is more common in firms with a smaller team of analytics. This is because programs on EG are far more understandable by a person who is new to the firm. Given the high attrition rate in analytics industry, it becomes very essential for such firm to hedge their risk by using the SAS Enterprise guide instead of Base SAS.
2. You develop a certain traditional routine much faster on EG than on Base SAS.
3. It is much easier to comprehend the flow using EG.
 Base SAS
1. It is much easier to code on base SAS in case the logic of code is very complex.
2. Base SAS is much faster as compare to EG.
3. It is much easier to modify a code on Base SAS than on EG.
4. Using Macros makes coding in Base SAS much easier than SAS EG
Background to case 1
You work in a retail industry. You have recently started a loyalty program for your customers. A study conducted on retail bank, says that the customers with a total purchase of $1,000 in 3rd month (T+ 2th months) are the customers who will finally purchase more than $30,000. You want to focus your loyalty campaign on these customers.
You have 2 data-sets. First has the entire list of customer IDs with their date of first purchase. Second data has the customer ID with their monthly purchases for each year-month. First purchase can possibly be a non-financial transaction which might not be a part of table 2.
You need to identify customers who make more than $1000 purchase in the 3rd month from the first purchase.
Table 1 : 

Table 2 :

Solution to Case 1
This question is a classic case when Base SAS clearly beats SAS EG. In this section you will see a simple solution for this case study.

* Creating a macro for each month
%macro fetch_data (next_mon = , third_mon = );
data create_list;
set table_1;
if first_pur < next_mon;
run;
 
proc sort data =  create_list out=list; by customer_id; run;
 
proc sort data =  table_2 out=purchase; by customer_id; run;
 
data fetch_purchase;
merge list(in=a) purchase(in=b);
if yearmonth = third_mon;
by customer_id;
if a;
run;
 
proc datasets;
append base=final_dataset data =  fetch_purchase foce;
run;
%mend fetch_data;
 
%fetch_data (next_mon = ’01Feb2012'd  , third_mon = 201204);
%fetch_data (next_mon = ’01Feb2013'd  , third_mon = 201304);
%fetch_data (next_mon = ’01Mar2012'd  , third_mon = 201205);
%fetch_data (next_mon = ’01Mar2013'd  , third_mon = 201305);
 
*Identifying the customers with purchase above $1000 in 3rd month
data shortlisted;
set   final_dataset;
if sales ge 1000;
run;

Background to case 2 
You work for a banking industry. You want to analyze the transaction dataset and want to find the median transaction amount for each customer. This is the amount over which we will want to pay to the customer for stretching. More the dollar value of transactions, the cheaper is the total cost of transactions.You need to make a list of all customer with their floored median transaction amount (if there are 5 transactions, we want the 2nd lowest transaction and not 3rd and if transactions are only 1 then remove the customer from the list).
The only dataset you have is unique on transaction ID. It also has the customer ID and amount of the transaction.
 Table 1

Solution to Case 2 
The solution to this problem is tiresome on SAS EG because there is no median function on SQL routines after grouping data. SQL routines are the foundation of data handling in SAS EG.But this becomes quite easy on Base SAS. Let’s see how this can be done easily on Base SAS.

proc sql;
create table work.summarize as
select count(*) as trans_nos, customer_id
from work.table1
group by customer_id;
quit;
 
proc sort data = tables1; by customer_id;run;
 
proc sort data = summarize; by customer_id;run;
 
data add_total_trans;
merge table1 (in=a) summarize (in=b);
median_no = floor(trans_nos/2);
by customer_id;
drop trans_nos;
run;
 
proc sort data = add_total_trans; by customer_id amount;run;
 
data final_list;
set add_total_trans;
by customer_id amount;
if first.customer_id then n =1;
if n = median_no;
n + 1;
run;

The solution in base SAS for this question is not only effective but also time efficient.
End Notes
Both Base SAS and EG have their own pros and cons. The best recommended strategy is to use both. If you want to make a traditional query, use SAS EG to generate automated code. Now copy this code to make it macronized and generalized using Base SAS. The macro adds a new dimension to the codes which helps you generalize the code and avoid hard entered data.
Have you faced any other SAS problem in analytics interview? Are you facing any specific problem with SAS codes?  Do you think this provides a solution to any problem you face? Do you think there are other methods to solve the problems discussed in a more optimized way? Do let us know your thoughts in the comments below.
If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.
Share this:Pocket

	Related
			
						
						
		"
"Probability in action - Could Monty Hall have made more money?","Tavish Srivastava",2014-04-07 21:25:00,2,"
							
										
						Monty Hall could have lost 66.7% of times in the show, if contestant consistently took the best strategy. Could he reduce these losses and have made more money for the show? 
Apparently, it boils down to how good he was with probablities!
Monty Halparin, also known as Monty Hall, hosted a game show in 1960's called “Lets make a deal”. I have no doubt that the show minted billions of dollars over the years. The game has two levels – In the first level the maximum probability you can win is 33% which becomes 66% in the second round. Just imagine, that if audience knew the best strategy to approach this problem, host will lose 66% of times.
In this article, we will not touch up on the derivation of the Monty Hall’s problem as this is something thousands of people have studied and published. This article will serve as food for thought of digging even deeper into the same problem.
What happens if we add a door (making a total of 4 doors) and open 2 of them? This certainly makes the audience believe that their chances of winning increases further making the show even more interesting. But does the host lose more than 66% of times, if audience takes up the best strategy consistently. If not, probably the director should add a door in their show making it more interesting, making the deals longer and losing less number of times even against the best strategies.
 Actual Monty Hall’s problem :  
Suppose you’re on a game show, and you’re given the choice of three doors: Behind one door is a car (Ferrari, let’s say); behind the others – goats. You pick a door, say 1, and the host, who knows what’s behind the doors, opens another door, say 3, which has a goat. He then says to you, “Do you want to pick door 2?” Is it to your advantage to switch your choice?

 Simplistic solution  : 
Following are the events :

A : door 1 has car                         B : door 2 has car                          C : door 3 has car
Co : Event that host opens door 3 and shows a goat
P(Co/C) = 0  (as host will never show the door with a car)
P(Co/A) = 0.5 ( as the contestant choose a door with car, host can show any of the doors 1 or 2 )
P(Co/B) = 1 ( host is left with no choice but open door 3)
P(A) = P(B) = P(C) =1/3 (all the events are equally likely)
Using Bayes theorem
P(Co) = 1/3*(0+1+0.5) = 0.5
P(A/Co) = (0.5*1/3) /0.5 = 1/3 = 33.34%
P(B/Co) = (1*1/3) / 0.5 = 2/3 = 66.67%

Hence, the contestant should switch the door raise his chances of winning from 33.34% to 66.67%. But notice, if contestant applies the best strategy the host would have lost 66.67% of times.
 Modified Monty Hall’s problem Part 1 :
You’re given the choice of four doors: Behind one door is a car; behind the others, goats. You pick a door, say 1, and the host, who knows what’s behind the doors, opens another door, say 2, which has a goat. He then says to you, “Do you want to pick door 3 or 4?” What is your best strategy (lets say this choice 1)?

 Finding the best strategy to choice 1:
 Following are the events :

A : door A has car                         B : door B has car                          C : door C has car               D : door D has car
Bo : Event that host opens door B and shows a goat
P(Bo/C) = P(Bo/D) = 1/2  (as host can show any of the door which does not have the car)
P(Bo/A) = 1/3 ( as the contestant choose a door with car, host can show any of the doors 1, 2 or 3 )
P(Bo/B) = 0 ( host cannot show the door with car behind it)
P(A) = P(B) = P(C) = P(D) = 1/4 (all the events are equally likely)
Using Bayes theorem
P(Bo) = 1/4*(0+0.5+0.5 + 0.333) = 1/3
P(A/Bo) = P(Bo/A)*P(A) / P(Bo) = (1/3 * 1/4) / (1/3) = 1/4 = 25%
P(C/Bo) = P(D/Bo) = 1/2(1-1/4) = 3/8 = 37.5% (By symmetry the two probabilities will be equal)

Hence, the best strategy as of now will be to shift to either door C or door D.
 Modified Monty Hall’s problem Part 2 :
The host further opens up door D and shows a goat again. You are again asked if you want to make a shift back to door A or choose to stay at door C.

Finding the best strategy to choice 1:

Do : Event that host opens door D and shows a goat
P(Do/  A  /Bo) = Probability of host opening door D given that B is already open and A has the car
= 1 (Host has no other choice)
P(Do/ C /Bo) = 0.5 (Host has a choice between A and D)
P(Do/ D /Bo) = 0
P(Do/Bo) = P(Do/ A / Bo)*P(A/Bo) + P(Do/C/Bo)*P(C/Bo) =1*1/4 + 3/8*1/2 = 7/16
P(C/Do/Bo) = (3/16) / (7/16) = 3/7 = 42.8%
P(A/Do/Bo) = 57.2%

 Again, we see that the chances of winning a car is significantly increased if we make a switch from door C to door A.  Note that we still have not explored the best strategy if host would have opened door A. If you do similar type of calculations in this revised scenario, you will see that the best strategy exist if you switch to door D. The chances of winning in this scenario becomes 66.67%.
On comparison of these two scenarios to original Monty Hall problem we see that the maximum of the probability of host losing in 4 door scenario is equal to 3 door scenario.Let’s think of the scenario in which the host is forced to open door A (which is a loss making proposal). The only case when host is bound to open door A is when participant originally chooses door A.
P(anchor losing in 4 door-2 open) = 0.25 * 66.67% + 0.75*57.2% =  59.6%
P(anchor losing in 3 door-1 open) = 66.67%
Note that we have left a scenario in which the participant sticks to his original door in the first scenario. This time the probability to win will come out higher than before but we will not touch up on this case. This is because this case assumes that the candidate is already aware that the host will open the second door after the participant chooses to stay with his first choice. In original Monty Hall’s problem the participant does not make his choice based on the knowledge that the host will open on of the other two doors. Imagine that you choose door A and host shows you a goat in door B. Now hosts asks you if you want to switch, and you choose to stay with the door A. Next moment, the host says “Looks like you really love door A, so let’s show you what is behind door A.” You obviously have only 25% of probability to win in this case.
 Summary of findings :
 We see that the probability of anchor losing is much lower in a 4 door scenario as compared to 3 door scenario. We have no intention to give recommendation to this legacy game show but will want to spark a discussion on the following : “If 4 door – 2 open Monty hall scenario gives a lesser probability of anchor losing compared to 3 door – 1 open scenario, certainly will excite the audience even more than the traditional Monty hall as it intensifies the illusion of winning on the wrong door and obviously consumes more time per contestant, then why did the show director restricted themselves to only 3 doors?” If the answer lies outside the world of probability, then its not worth initiating a debate on the same but if you have any inputs within the scope, do let us know your thoughts in the box below.
If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.
Share this:Pocket

	Related
			
						
						
		"
"Solving Accuracy vs. Cost using probabilities (with case study)","Tavish Srivastava",2014-04-02 08:09:00,4,"
							
										
						
As a manager, you face cost vs. quality / accuracy trade-offs on a regular basis. This can be in the form of any of these questions:
Should we invest more time and resources to gather incremental data and take more informed decision?
What is the value gained by asking the analyst in your team to spend one more day on data cleansing?
What is the incremental gain by trying out some more hypothesis and gain slightly higher lift curve in your predictive models?
What is the value of extra time / resources spent on gathering additional information?
If you are a decision making authority, these dilemma arise quite often. Sadly, not many people have a framework to answer these questions. They end up taking these decision based on their gut or change their stand multiple times in the process.
If you do not put a framework around these problems, your decision making is dependent on how the problem is narrated to you or how you think about it at that instance. This also leads to change in stand / decisions multiple times. In this article, I have laid out a simple framework to answer these questions and have illustrated its usage through a case study.
 Framework: 
The framework for these situations is fairly simple. As a stand alone decision, you invest resources until the value created by these investments is higher than the cost incurred.
On the other hand, If there are multiple opportunities with limited resources, you invest in the projects, which give you the highest ROI.
 Case study - Background : 
You are the general manager of FUORD, an automobile company in India. FUORD has recently launched a model in India and China called Bistra. The engine of this model has been outsourced to two companies, namely X and Y (referred as vendor after this). Both X and Y make an identical design of the engines. However, 10 out of every 100 engines from Vendor X are faulty in working whereas 1 out of 100 engines of Y is faulty in working. India and China import exact the same number of engines. FUORD has a policy of not revealing the vendor name while sending the engines to any country. Hence, in any month India has an equal probability of receiving either engines manufactured by X or by Y.
 Faulty engine found :: 
After 10 months of launch of Bistra, you found one of the engines is of wrong design. A fault in design is against the code of conduct of FUORD and the contract with this supplier needs to be terminated immediately. But the fix is that neither you nor anyone in the firm is sure whether this engine was supplied by X or Y. Here are various costs involved in the process:
Test for working of each engine costs the company $40 k   .
Cost of a decision of terminating a wrong vendor contract is $1 MM.
Should you test an engine for its working, if it is faulty or not before terminating the supply from a vendor?
Note: Fault in design and fault in working of the engine are two independent events.
The play of probabilities: 
What is the probability that the lot in India in the 10th month of the launch came from X? Obviously 0.5, as there are only two options of choosing a vendors and both equally likely. Can you take a decision to terminate any of the two vendors based on your intuition? Probably not. What do you do in such situations? Collect more information.
But is the collection of information worth the cost which you will incur to test an engine. Let’s try to find out the expected costs involved.

Event X : The lot in the 10th month is from X
Event Y : The lot in the 10th month is from Y
Event F: The chosen engine is faulty
Event P: The chosen engine is perfect
P(X) = P(Y) = 0.5
P(F/X ) = Probability of the engine being faulty given that the lot is from X = 0.1 (Given)
P(F/Y ) = Probability of the engine being faulty given that the lot is from Y = 0.01 (Given)

We already know that

 


P(F) = Probability of the engine being faulty
= P(FnX) + P(FnY)
= P(F/X)*F(X) + P(F/Y)*P(Y)
= 0.5*(0.1 + 0.01) = 0.055
P(X/F) = Probability of lot being from X given that the first random engine chosen is faulty
= P(XnF) /P(F)
= 0.5*0.1/0.055 = 0.909
P(P) = Probability of the engine being perfect  = P(PnX) + P(PnY)
= P(P/X)*P(X) + P(P/Y)*P(Y)
= 0.5*(0.9 + 0.99) = 0.945
P(X/P) = Probability of lot being from X given that the first random engine chosen is perfect
= P(XnP) /P(P)
= 0.5*0.9/0.945 = 0.47
P(Y/P) = 0.53

K : Event of choosing the correct vendor after the first engine assessment.


P(K) = P(KnF) + P(KnP)
= P(F) * P(K/F) + P(P)*P(K/P)
= 0.055*0.909 + 0.945*0.53
= 0.05 +  0.5 = 0.55
Now let’s make some cost estimations.
Expected value of cost if engine assessment is not done= A = $1MM * 0.5 = $500 k
Expected value of cost if the engine assessment is done = B = $40 k + 0.45 *$1MM = $490k

Clearly A > B and, hence, assessment of first engine is totally justified. Also, on a stand alone basis, we were able to reduce cost of wrong assessment by $10 k, by investing $40 k
  End Notes :  
In this part of the case study, we took a very simple case with a single step of processing. Say, you completed the first test and found the engine to be perfect. Now you wish to check if the second test is cost effective or not. Make cases and find the expected cost of test and cost saved by the test. Make the comparison and write in the box below your recommendation to do the second test or not?
Did you find the article useful? Share with us how you would have approached making strategies mentioned in the article. Do let us know your thoughts about this article in the box below.
If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.
Share this:Pocket

	Related
			
						
						
		"
"How analytics help organizations becoming customer centric","Tavish Srivastava",2014-03-23 18:24:00,1,"
							
										
						
Whatever industry you work in, you will hear following line in almost all the top management presentation :
“Our target for this is year is to become more customer-centric.” 
What is customer centricity? Why does everyone want to move to a more customer-centric platform? How is it better than product centric approach? How will analytics play a key role in this transition of industries from product centric to customer centric offerings? This article will provide you answers to all these complex question.
Let’s begin with a delightful futuristic scenario. You wake up in the morning. With your eye lashes opening, your curtain opens, opening a beautiful scenery. The glass window adjusts the brightness of the surrounding and your bed gradually inclines to help you get up. All these solution are so synchronized that it will be impossible for different companies to give different solutions to the same customer. It has to be a single company providing a complete solution to the customer.

Now imagine a second scenario, you get out of home for office and bunch of sales man encircle you trying to force-fit their solutions on to you. Some one is trying to sell you a  market linked  insurance even though the sales man himself is not convinced that the product is fit for anyone, someone trying to sell you a credit card even though you already have purse full of them etc. Which one do you want to be your future ? If you say first one, you already know what customer centricity means.
  The contrast :  
There are only two ways a company can operate. First, it launches a new product basket and finds the best fit customers for this product. It modifies the target customers time to time to suit the product basket. Second, the company launches a product and find the best fit customers for this product. Having identified the most profitable customers, it tailors the product, offering and services as per the customer need. The former is called the product centric approach and the later is called the customer centric approach. Following are the definition of the two which I find best suitable to explain the two approaches :
A product-centric competitor focuses on one product at a time and tries to sell that product to as many customers as possible.
A customer-centric competitor focuses on one customer at a time and tries to sell that customer as many products as possible.
Let’s consider a business case to understand the two approaches.
  Business Case : 
There are two competitors of Laptop retail market in India with same revenue in the beginning of 2013. Retailer X has many stores in India and, hence, sells Laptop with a very aggressive marketing to capture the highest market share.  Retailer Y, on the other hand, is a new retailer with limited outlet. If Retailer Y does not do something exceptional retailer X can easily crush Y’s business given the high amount of its visibility. This is what Retailer Y does :
It analyzes the daily laptop accessories/services requirement of each existing customer. Finally it is able to establish the life-cycle of each customer. For example, any customer needs an additional speaker for the laptop in 3 rd month of purchase, an additional hard disk after 6-9 months of purchase etc. It further drills down to customer level identifying the exact need of each customer with time. Retailer Y thus decides to venture into some of these laptop accessory supply as well. The overall strategy helped retailer Y to get higher wallet share from their existing customers and new acquisitions.
By the end of 2013, both the retailers experienced the exactly same growth. Following is the mathematical workout with some illustrative figures to bring out this picture :

New Revenue = Old Revenue * ( 1 + Growth in wallet share ) * ( 1 + Growth in market share ) 
Company X : Growth in wallet share = 10% , Growth in market share = 50%
Company Y : Growth in wallet share = 50% , Growth in market share = 10%

 As you see from the illustrative figure, both the retailer grow with the same growth rate of 65%. The difference is that Retailer X took a product centric approach to grow revenue and penetration of the same product and Retailer Y tailored the product offering as per the choice of the customer and increased the total wallet share.
Every company tries to find the right balance between the wallet share and the market share growth. Companies who is able to achieve both becomes the market leaders in the industry. Following graph shows the positioning of brands with difference in approaches.

  Which approach is better in a longer run? 
Given a choice between product centricity and customer centricity approach, today companies prefer latter. There are multiple reasons for the same. Following are few of them :
1. Product-centric companies have the life span of their products. They are vulnerable to fluctuations in the market. In case the taste of the customer changes, the market of such companies completely dies off. Also, if the cost of the product increases and the customer is not ready to pay for this increase, the company has to cut down on their profit margins.
2. The main objective for a product-centric company is to maximize the value created by each product, while the financial objective for a customer-centric competitor is to maximize the value created by each customer. But unlike products, customers have memories. This means that the business a customer generates for you tomorrow, either as a repeat customer or as a reference for other customers, is based largely on their memory of how well they were treated today.Hence, making the approach more customer centric not only ensures higher wallet share but also incremental wallet share by referrals and good faith generated. Referral or word of mouth is the most effective and responsive way of marketing .
  How does analytics help the organization drive the customer centric approach : 
As a marketing analyst we build marketing strategy day in day out. Here is a case which will demonstrate how customer centric approach gives an edge over competitors in this dynamic market. There is a garment shop which has monopoly in the market. It currently has 5 customers and the owner has built a predictive model to find the propensity of a customer to buy Men or Women garment for each customer. We will focus only on one customer i.e. A to make the discussion simpler.  Following are the probabilities for each customer to buy the two products.

The owner found that for the Men apparels A has the highest propensity to buy  whereas for Women  apparels E has the highest propensity to buy. Given the cost constraint he sends only a Men apparel discount voucher to customer A.
Shop 1

Customer A is happy to have got the voucher and increases his spend on Men apparel, leading to an increase in the overall spend of A in this shop.  This is a product centric approach which works perfectly fine till there is a monopoly. But such monopoly hardly exists in real world. One of the fundamental principle of economics says
” There will an entry of a new player in the market till the point that marginal gain of the new player becomes negative on his entry.”
Hence monopoly in a free market is impossible. Let’s analyze the same problem with a new competitor in market. Assume that the probability to buy apparels and the customer for shop 2 (the competitor) is same. But this time shop 2 takes a customer centric approach. It targets A with a product offering he has higher propensity to buy i.e. women apparel.
Shop 2

Let’s assume that the customer A  does not change his entire purchase pattern because of these offers, but changes his preference of shop for different product. This will happen very often in the real world except in the case of established loyalty. Following was his spend on the two shops before the offer was made :
Given that Shop 1 makes a better offer in Men section and shop 2 makes a better offer in Women section, the distribution changes as follows :
As you see in the table above, even though shop 1 increased the share for Men apparel product but because of competition in market decreased its overall wallet share of customer A by making the wrong offering.
Additionally, customer A becomes more loyal to shop 2 because majority of his purchases now happen at shop 2. This further will lean customer A’s behavior towards shop 2.
  End Notes : 
The two approaches, namely product centric and customer centric, are mutually exclusive and can be implemented along with each other. However, customer centricity automatically helps company to grow its market share by the virtue of growing loyalty base who become promoters of the company. Apple is a great example of such behavior. Apple provides a wide variety of solution and has updated its offering keeping customer always in the center.  We will like to hear your thoughts of how companies are driving customer centricity in their strategies.
If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.
Share this:Pocket

	Related
			
						
						
		"
"Learn Analytics using a business case study : Part III","Tavish Srivastava",2014-03-09 23:32:00,1,"
							
										
						
Data based analytics and intelligence practices typically continue to grow complex over time. This is because we get more data over time, computational power is becoming cheaper by the day and businesses are facing situations where they can either innovate or perish.
Same applies to our case study. There are various levels of complex analysis which can be applied to this business once we start getting more and more customer information / data. In this concluding part of the case study, we will talk about framing targeted marketing strategies through propensity modelling or simple segmentation.

A quick Recap  :  
You have recently started a Video CD rent shop. After 2 months you realize that there is tough competition in the market and you need to make a more customer-centric strategy to stand out in the market. You have already built all the datasets and collected data for first 2 months (Read part 1 here). After 2 months, you used insights from portfolio data to define your mass market strategy and acquisition target localities (Read Part 2 here).
Now, you are in 5th month and business has grown to 750 customers. You have 4 months of data now and are wondering how to make your strategies even more customer-centric?
 Month 5 :  
Here are some high level metrics for your business :

Till this point, all your analysis has been on portfolio level. You, want to create a more granular optimized marketing strategy. Here, we will use one the best industry approach to classify our customer portfolio into meaningful segments and then define targeted strategies for each of them.
Predictive modelling is still out of reach because of relatively small size of portfolio and data points.

Targeted engagement Strategy :
Step 1 : Form group using RFM approach :
This is one of the best and most commonly used technique. It is very quick to implement and its flexibility makes it easy to apply it across industries. This technique can be used in multiple ways. Following is a demonstrative example:
THE R : R stands for measuring Recency. In this case, we will take into account the number of days since the customer took the last CD. We will score each customer based on the band he qualifies.

THE F : F stands for frequency. In this case, we will want to know the total number of CDs a customer has rented in his entire lifetime with us. Again we will band the frequency and score accordingly

The M : M stands for Monetary. Because, in our case each product costs the same, monetary parameter will behave very similar to frequency, there is no use of this metric.
Let us consider a case, customer X has bought 8 CDs in total and bought his last CD 6 days before. His total score becomes 8/10. Similarly, we score each of the customers. First thing you need to check is the distribution and the stability of the score.  Following is the distribution of scores as on month 4 and month 5:


Both the curves are approximately normally distributed and have similar proportion of population in each score band.
Step 2 : Find the profile of each customer using scores of 2 months :
For example, a customer with high score in both the months fall into loyal bucket (1st quadrant).

Step 3 :  Find the right strategy for each of the groups
Loyal customers : These are the customer who were highly valuable and are still highly valuable. This segment is our loyal base and it is important to delight these customers because they give us a constant stream of revenue. All our customer delight / engagement strategies need to focus on these customers. Let’s list down a few of these possible engagement for “Up-selling” :
1. Based on the preferred genre, send updates of new releases or newly acquired CDs.
2. Send weekly updates of star rating on new movies of preferred genre.
3. Send festive promotional offers to up-sell.
4. Give free delivery services.
Grow customers : These customers are showing a growing pattern and are becoming more and more engaged with us.  The right strategy with these customer is to delight them and grow their engagement further. We again add these customer to the up-sell group. A similar strategy as of Loyal customer might be helpful in making them more engaged.
Win back customers : These customers are showing a declining pattern in terms of engagement/revenue. Here we need some different set of strategies to arrest their likely attrition and bring them back. If we think about it, there are possibly 2 reasons for their possible attrition:
a) They are now shopping from other shops
b) They have declined their overall requirement
Let’s try to list down some strategies to increase their wallet share or transfer their demand from other vendors to our store :
1. Win back offers : Give promotional offers to get them back on books. Offers like ” Buy 2 CDs a month and get third 50% off” will differentiate us from other vendors.
2. Customer specific engagements : Customer all wants to be treated deferentially. A simple promo offer like ” Get 25% off on next CD” might not work as good as ” We know you like watching Romantic movies. Here is “If Only” one of the best in genre 15% off only for you”.
Other strategies :
There can be many other data driven strategies, which can be implemented at this point in time. Some of these strategies are as follows :
1. Inventory optimization model based on past demand trends
2. X-sell of other rental services to loyal customers like renting home theatre.
3. Mutually share data with Home theater store and bring off-us customers with profile similar to loyal customer on board.
  End Notes :  
Strategies discussed in this article are meant to be food for thought for the viewers. These are by no means comprehensive. If you have examples of techniques, which you implemented in your industry and can be useful for this case study, please share them for benefit of wider community.
At some point in future, I will also cover how can this business use predictive modelling once it starts operating on a larger scale.
If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.
Share this:Pocket

	Related
			
						
						
		"
"Learn Analytics using a business case study : Part II","Tavish Srivastava",2014-03-02 21:34:00,1,"
							
										
						
The sequel episode, of most trilogies, is often the most interesting. This is because the first episode builds a foundation, of the overall plot, and includes many mandatory but non-interesting events, but by the second episode, the director has to build sufficient interest to ensure that the viewer is hooked to the entire season. We assume that you have gone through the first part of this case study (http://bit.ly/CDcasePart1). Let’s make this article more interesting by creating a near-life simulation of the newly built business.
 
A quick Recap  :  
You have recently started a Video CD rental shop. After 2 months you realize that there is tough competition in the market and you need to create a more customer centric strategy to stand out. You have already built all the datasets and collected data for the first 2 months. You stand in the third month and want to maximize your profit, using data analytics.
 
 Month 3 :  
Here are some high level metrics indicating health of your business :

Your shop is just like all other shops and your product is no different either. Why will your customer see you differently? As of now, you have burnt around INR 32k and earned only 6.75k. This is what happens in almost all new businesses. You have no loyal customers yet; and customer acquisitions are always costly. But starting month 2, you need to stop following the old-school way of doing business.
Let’s think of all possible changes you might want to bring to your business.  Following are some of them which are on top of my head :
Customise the theme basis what customers may like
Customise the banners basis the localities you are going to place them in.
Identify the most popular genre and stock more CDs pertaining to the same.
Map and profile localities which get you more business & profits. Identify more such localities for more customer acquisitions.
Now, you might have noticed that all the solutions above general in nature, and do not require any predictive modeling. This is because the business just 3 months old, and probably doesn’t have much data to build predictive models. But there is still a lot you can do to build the business further.
 
1. Make theme more customer-centric :
The Current Situation: As of now you have put up a banner of Hindi Comedy movies and have stacked Hindi Action movies in the front counter. Is this the right display for your target audience?

Assumption : The profile spread of your current portfolio and the future acquisition will be similar.
Idea: Imagine a new customer looking for an English Action movie looks at two competing CD shops. He will make a perception that the probability of finding a CD of his choice is more in the shop with banner of English movies.

Analysis : Our target should be, to display more genres that people will want to rent. People, who would like to rent a CD, may not really be interested in Hindi Comedy and Hindi Action movies. Since the movies available in each genre are similar, and the choice of movie genre is not restricted by availability, the two categories, which stand out are Hindi Romantic and English Action. Hence, both, the banner and the display should correspond to only these two categories.
2. Make each advertising banner focused to local taste :
The second pointer is very similar to first analysis. But here you break the audience basis the locality or area they belong to, and decide on the most popular genre in the area. This thought process should govern your marketing banners strategy.
3. Genre wise Inventory/Stock optimization :

The third pointer is derived from the same analysis, as described in point 2. Our analysis showed that people prefer Hindi Romantic and English Action movies. Now, after revising the display and banners, the client base would expect a larger variety corresponding to these two genres, even more. Hence, stocking the same number of CDs, in each genre, may not be required and wouldn’t be justified.
4. Target most profitable societies for acquisition :
The fourth pointer however needs a slightly different analysis. This pointer would need us to identify areas with the highest profitability. The tricky part is, that you would need to find out the profit at an individual customer level. In the table below, the cost incurred on a society is the cost of installing the customised banners. The revenue generated, is calculated by multiplying INR 13, with the CDs rented in total by the society. In the data below, we have assumed that the same amount of money was spent in marketing, in 5 different societies.

If we had not done this calculation, we may have assumed that targeting a high standard society, would be most profitable because the number of CDs bought by each customer was higher. But from this analysis it is clear that the conversion rate in such a society is also less. Hence, it is best to target the Medium standard society.
Note that this rank order might change with time. Given the trends, with time the number of repeat customers in high standard society might raise significantly higher, but the same cannot be concluded as of now. Hence you will want to market more and acquire more customers from medium class societies.

Here is a summary of strategies we recommend for the month 3 :
1. Change the banner and front desk of movie genre to English Action and Hindi Romantic
2. Put customized banners in different kinds of societies, according to the society’s taste
3. Stock more titles in the English Action and Hindi Romantic genre.
4. Market the business, more in Medium standard societies similar to “HEWO society”.

 Month 5 :  
Here are some of the high level metrics for your business :

You see a good improvement in the total sales and the net loss you make month on month. If the trend goes, you would start making a profit over the month on month, running costs in the next 1 or 2 months. This itself is an insight, and the first step towards prediction. Since you are ambitious, you would want to introduce some cutting edge marketing strategy. Here is what you can do:
Identify most valuable customers, send movie reviews & other value added services to these customers
Innovate, and come up with new services like home delivery, weekly packages, promotional packages etc. to targeted customers
Create targeted marketing campaigns for each type of new product.
Create a website to promote your shop and collect information of trailers viewed by the customers.
Create an optimized inventory management system to make sure you are never stocked out
Create proper retention/win-back processes
As you might have observed, all the pointers are customer level marketing tools. Now we are not talking at the portfolio level, but have drilled down to the customer level. This is because now you have about 750 customers’ data to play with. The thumb rule here is, that customer analytics should be done on a statistically significant number, which in most of the industries is taken to be at least 500. We will talk about customer level analysis in the next article and take each of the pointers one by one to find some key insights and frame strategies from the 5th month onwards.
  End Notes :  
In this part of the case study, we looked at framing initial portfolio level strategies, and how data guides us to take some key initial decisions in a new business. Next, we will look at some interesting customer level strategies which can be derived using data sources mentioned in part I of the case study.
The reason we have segregated customer level analytics from porfolio level analytics is that the two analysis are done with very different mind sets. In the portfolio level, we focused more on mass media and themes. Whereas, in the customer level, you will focus more on targeted marketing, engagement and other key strategies.
 
Did you find the article useful? Share with us how you would have approached making strategies mentioned in the article. Do let us know your thoughts about this article in the box below.
If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.
Next part of case study: Learn Analytics using a business case study: Part III
Share this:Pocket

	Related
			
						
						
		"
"Learn Analytics using a business case study : Part I","Tavish Srivastava",2014-02-24 04:06:00,2,"
							
										
						
Best way to learn analytics is through experience and solving case studies. Here, I will present you a complete business model and take you through a step by step process of how analytics is set up in a new business, how is it used in daily processes and some of the advanced analytics techniques which a business can use to make meaningful segmentation and prediction to optimize its marketing & sales campaign.
   Background :  
You have recently started a Video CD rent shop. After 2 months you realize that there is tough competition in the market and you need to make a more customer centric strategy to stand out in the market. Hence, you want to collect the most granular details of your customer behavior and build strategy accordingly.

    Business case layout :  
This business case have been broken down into 3 articles. Following is a plot of the articles and each article will be strongly dependent on findings in the previous articles:
1. How do you collect data so as to capture all the important information?
2. Deep dive into customer behavior and using basic data analysis with business knowledge to optimize daily operations : Click here to directly move to this part http://www.analyticsvidhya.com/blog/2014/03/learn-analytics-business-case-study-part-ii/
3. How do you use data with advanced analytics to make your marketing/sales startegies more targeted?
    Case study part I :  
Did you ever wonder why do you deal with so many datamarts in your company. Let’s try to understand as the owner of the busienss what all data sources do you need.
1. Transactions Table :
You rent out Video CDs and the most important data for you will be transactional data. Transactional data is by far the richest data throughout all industries. Each row in transactional data corresponds to one transaction made. This transaction mostly are monetary transaction. To identify each transaction, you need a distinct transaction code associated with each transaction. What other fields can you think of to be captured along with each transaction. Following is a small list of such variables :

1. Transaction ID
2. Customer ID : Identifying the customer to whom you have rented out the CD
3. Rent due : How much does the customer need to pay as rent
4. Issue date : When was the movie rented out
5. Recieved date : When was the movie recieved. Blank if CD is still due
6. Movie ID : Identifying the movie

Following is a  sample transactional data set :

2. Product Table :
If you have transaction table, you basically have the linkage between the customers and the products. But why does transaction table not have the discription of products? The simplest reason for the same is that total number of products are limited in any industry, and the same product is repeated throughout the transactions table. If we add description in every single line, it adds enormously to the overall size of transaction table, which anyway is huge. Hence, we keep the products table seperate and merge it with required transactions for specific analysis.
Product table is unique on product id, which maps to transactions table. What other parameters can you think of that make sense for you to include? Following is a list of possible variables :

1. Movie ID : Unique ID of movie
2. Origin yearmonth : When was the CD bought?
3. Genre of movie
4. Language of movie
5. Star Cast of movie
6. Movie name
 

Note : Product ID generally can be decoded to know product details. For example, here H denotes “Hindi” and E denotes “English”. This coding makes the analysis simpler.
3. Customer Table :
The other hand of transaction table is the customer table. Using the above two tables, you almost have everything except the details of the customer. While making any kind of customer centric strategy, its very essential to consider the customer profile.This table helps you find the customer profile. This table is unique on customer id. What other parameters can you think of that make sense for you to include? Following is a list of possible variables :

1. Customer ID : Unique ID of customer
2. Age
3. Gender
4. Area : Area where the customer lives
5. Package : Package customer has enrolled to
6. Enrol_date : When did the customer make his first transaction
7. Name


Note : Similar to Product ID, Customer ID also generally can be decoded to know customer details.
4. Engagement Tracker :
All the three tables together can be used to create any kind of analysis to build marketing and sales strategy. What they do not cover is the engagement you had with your customers till date. Say, I called Kunal 1 week back to tell him about a movie X. Now, it might not be the best idea to call Kunal again this week to tell about the same movie. Hence, we need to keep a track on all kinds of engagement we have with our customer on daily basis. This is similar to transactions table but this include all the non-monetary interactions we have with out customers till date. These interactions can be inbound or outbound. This table is unique on engagement_id. What other parameters can you think of that make sense for you to include? Following is a list of possible variables :

1. Engagement id : Unique engagement identification
2. Inbound flag : 1 if inbound, 0 if outbound
3. Type of engagement : Code for the type of engagement
4. Product ID : ID of product in question

5. Derived tables :
Because the data sizes become huge with time, it is always recommended to keep some monthly snapshots handy. One of such table can be transactions data rolled up at customer level. Following is a list of such possible variables :

1. Customer ID
2. # lifetime transactions : No. of transactions made by customer till date
3. Enrol date
4. # English Movies : No. of English movies taken by customer till date
5. Last engement date
6. Last transaction date

Such derived tables come very handy to make quick analysis. Say, you have acquired 10 new english movies and want to market them. You might want to market these movies to customers who watch english movies, who responded to recent engagements and who have done recent transactions. For such a targeting list, imagine the process you might need to follow. Following is a possible way to achieve the same :

Imagine how easy this analysis gets if you have the derived monthly snapshot handy.
    Graph schemas:  
The article till now focuses on use of traditional relational databases. Graph based databases (e.g. Neo4j) are a strong alternate to these traditional databases. They add a lot of flexibility to your database, where you can change the schema very easily.
This kind of flexibility is required in case your data formats can change and you can not have much control on it. Also, you can add new structures and relationships very quickly. Before we go in these details, a typical graph schema in this case would look something like:

Blue nodes represent customers, Red represent movies and Green represents various package available. Every edge is a relationship in between nodes. For example, if a customer rents out a movie, we can draw an edge between the 2.
Now by calculating things like number of edges from a node, you can look at things like most active customer, most rented and least rented movies. You can also start looking at what kind of customers are renting what kind of movies.
P.S. Like all data model designs, there are various alternates to this design and you should choose the best depending on your usage.
    End Notes :  
We discussed relational database and graph database for representing a typical business problem. The data tables we discussed in this article is almost parallel to datamarts in any industry. We will look at some interesting strategies which can be derived using these data sources for the CD rental business case. Some of these strategies which are very basic in nature and needs more of business sense than modelling will be discussed in the next article. This will make you understand how effective strategies can be built if you mix business knowledge with simple data analysis.Knowledge of data is very essential regardless of the industry you work for. To view the next part of this case study click http://www.analyticsvidhya.com/blog/2014/03/learn-analytics-business-case-study-part-ii/
Did you find the article useful? Share with us any other problem statements you can think of. Do let us know your thoughts about this article in the box below.
If you like what you just read & want to continue your analytics learning, subscribe to our emails, follow us on twitter or like our facebook page.
 
Next part of case study: Learn Analytics using a business case study: Part II
Share this:Pocket

	Related
			
						
						
		"
"Demystifying LinkedIn using probabilities","Tavish Srivastava",2014-02-09 21:19:00,1,"
							
										
						You will be able to view only 1-2 Million of profiles out of 18 Million profile on LinkedIn,
if you are a non premium customer 
Since last 3 years, I spend more than half an hour on LinkedIn everyday. This is what corporate life does to us. LinkedIn is a one stop solution for almost anything you want to know in this corporate environment. I use LinkedIn for reading interesting article, knowing interesting people, marketing etc. Here is a restriction LinkedIn puts on any non-premium users :
 You are not allowed to see full profile of some of the restricted 3rd level friends 
(neither friends nor friend of friends) 
This article will give you a flavor of how you can use probability to think beyond what is visible to an average user in any social networking website. Here is simple tricks using probabilities on LinkedIn to overrule the restriction put by LinkedIn on its non premium users.
 Sizing of 3rd/3rd+ level locked Linkedin friend : 
Let’s start with estimating some numbers.
Total number of friends I have on my LinkedIn network = 1,700
Total number of 2nd degree friends on my LinkedIn network = 853,430
Total number of group users (share a group) = 973,323
The total of these numbers is the total number of profiles open to me to visit. I am likely to get profile restriction message for all other users. Let’s do a sizing of this number.
Say total number of users on LinkedIn = N
Number of friends of Mr.X = 10482
Number of common friends between Mr. X & Tavish Srivastava = 1
Say,
Probability of a person being a friend of Tavish = a = 1700/N
Probability of a person being a friend of Mr.X = b = 10482/N
Probability of a person being a friend of Mr.X & Tavish = c = 1/N
For independent  events,
a * b = c
N = 1700 * 10482 / 1 = 17.8 Million
Number of profiles not open to me = 17.8 MM – 1.8 MM = 16 MM

~16MM profiles on LinkedIn are not open to me because LinkedIn thinks I do not know the person enough

  2 steps to unlock details of 3rd/3rd+ level Linkedin friend : 
LinkedIn restrict users to view a profile if it thinks you don’t know the person enough. This is the screen you get
We will like to open the full profile of this person without upgrading our account to premium. Let’s try to understand what factors linkedin takes into account before deciding whether a person X knows Y.

Here are the behavioral attributes I can think of :
1. Number of common friends between X and Y
2. Number of common communities between X and Y
3. Profile matches between X and Y (University, company)
4. Any specific information X knows about Y ( like Email ID)
5. Last page viewed by X before landing to page of Y

4 out of the 5 attributes of this list are very difficult to trigger unless until X actually knows Y. Last criterion is something one can easily manipulated. Here’s a simple way to do the same.
X reaches a Y’s profile but Y’s profile is locked. LinkedIn has a tab on left hand side called “people also viewed”. Consider following event :
P(A) : Random person views Y’s profile
P(B) : Random person views B’s profile.
B’s name comes in Y’s  “people also viewed” tab 1st name. Hence, P(B/A) is extremely high.
We all know that P(B/A) ~ P(A/B) or both move in the same direction. If P(B/A) is high P(A/B) will also be high.

Step 1 : X clicks on B's icon in the Y's ""people also viewed tab 
(As illustrated in the following figure) 


Hence, if X goes into B’s profile from this page, it is highly likely that X will be on B’s “people also viewed” tab. Also because X reaches B through Y’s profile and not a random search, he triggers the 5th criterion (among the 5 attributes identified above) and will be able to see complete B’s profile.  
X finds Y on 4th number in the B’s “people also viewed” tab.

Step 2 : X clicks on Y's icon in the B's ""people also viewed tab
(As illustrated in the following figure) 


This time however X triggers 5th criterion for Y as well and hence linkedin shows X Y’s complete profile.

 End Notes : 
The algorithm works in most of the cases, however, in some cases you might not find the target in the list of  “people also viewed” of the intermediate person. This is probably because you chose an intermediate person who might be much more popular than target person. In such cases try choosing people who have a closer relation to the target person. Please note that the article is based on my experience with analytics and frequent usage of LinkedIn, there is a possibility that we might have missed some of the variables LinkedIn takes into account. Do let us know of any attribute you think might be one of the variable LinkedIn takes into account and we missed to mention the same in this article.
Did you find the article useful? Share with us any other techniques which you know to make use of social networks in a better way. Also share with us any live examples of using probabilities in real world scenarios. Do let us know your thoughts about this article in the box below.
If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our facebook page.
Share this:Pocket

	Related
			
						
						
		"
"An analytics interview case study ","Tavish Srivastava",2014-02-05 12:22:00,4,"
							
										
						
Case study is the most important round for any analytics hiring. However, a lot of people feel nervous with the mention of undergoing a case interview. There are multiple reasons for this, but the popular ones are:
You need to think on your feet in a situation where there is already enough pressure
Limited resources available to prepare for analytical case studies. Even with the amount of content available on web, there aren’t many analytical case studies which are available freely.
From an interviewer perspective, he is judging the candidate on structured thinking, problem solving and comfort level with numbers using these case studies. This article will take you through a case study. Answer to each question takes you deeper into the same problem.
Background: 
 I moved to Bangalore 10 months back. Bangalore is a big city with number of roads tagged as one-way. You take a wrong turn and you are late by more than 20 minutes.  Every single day I compare the time taken on different routes and choose the best among all possible combinations. This article takes you through an interesting road puzzle which took me considerable time to crack.
Process to solve: 
I have structured this in a fashion very similar to an analytics interview. You will be provided with background at start of the interview, which will be followed by questions. After you have brainstormed / solved a question, you will be presented with additional information which will progress the case further.
If you want to undergo this case in true spirit, just ask one of your friends to take the questions and information (provided in next section) and present them to you at the right time. After all the questions, I have provided asnwers which I expect. You can compare your answers to mine.
Please note that there is no right or wrong answer in many situation and a case evolves in the way the interviewer wants. If you have a different answer / approach, please feel free to post in comments and I would love to discuss them.
Problem statement : 
Background : There are two alternate roads I take to hit the main road from my home. Average speed on each of the road comes out around 30 km/hr. Let’s call the two roads as road A and road B. Total distance one needs to travel on road A and road B is 1 km and 1.3 km respectively to hit the same point on the main road . Note that, before the two roads split, I see a signal (say Z)  which is common to both the roads and hence does not come in this calculation. See figure for clarifications.

Q1 : What are the possible factors, I should consider to come up with the total time taken on each road?
Q2 : Which road should one take to reach  the main road so as to minimize the time taken? And what is the difference in total time taken by the two alternate routes?
Additional information (to be provided after question 2): Recently, one of the junction (say, X) on road A got too crowded and a traffic signal was installed on the same. The traffic signal was configured for 80 seconds red and 20 seconds green. Let’s denote the seconds of signal as R1 R2 R3 … G1 G2 G3 . Here, R1 denotes 1 sec after signal switched to red.
Q3 : Does it still makes sense to take road A, or to switch to road B provided the average speed on the road A is still the same except the halt at signal?
Additional information (to be provided after question 3):  If I reach the signal at R1, I will be in the front rows to be released once the signal turns green. Whereas, if I reach the signal at R80, I might have to wait for some time even after signal turns green because the vehicles in the front rows will block me for some seconds before I start. Let’s take some realistic guesses for the wait time after signal turns green.
R1 – R 10 : 0 sec , R11-R20 : 3 sec , R21 – R60 : 10 sec, R61 – R80 : 15 sec, G1-G15 : 5 sec, G15-G20 : 0 sec
Q4 : Does it still makes sense to take road A, or to switch to road B provided the average speed on the road A is still the same except the halt at signal?
Q5: Can you think of a reason, why road A can still be a better choice for reaching junction X in minimum time?
Additional information (to be provided after question 5): The signal Z (before the two roads split) has the exact same cycle as the signal at point X i.e. 90 sec red and 20 sec green. Average speed of any vehicle vary on road A from 25km/hr (heavy traffic) to 30km/hr (light traffic). The signal X is offset from signal Z by 25 seconds. Hence, when it turns green at Z, it is R55 at signal X.
Q6 : Does it still makes sense to take road A, or to switch to road B provided the average speed on the road A is still the same except the halt at signal?
Solution  : 
Background : There are two alternate roads I take to hit the main road from my home. Average speed on each of the road comes out around 30 km/hr. Let’s call the two roads as road A and road B. Total distance one needs to travel on road A and road B is 1 km and 1.3 km respectively to hit the same point on the main road . Note that, before the two roads split, I see a signal (say Z)  which is common to both the roads and hence does not come in this calculation.
Question : Which road should one take to reach  the main road so as to minimize the time taken? And what is the difference in total time taken by the two alternate routes?
Solution : 

Time taken on road A = 1/30 * 60 min = 2 minutes
Time taken on road B = 1.3/30 * 60 min = 2.6 minutes = 2 min 36 sec
Hence, the clear choice is road A. Road B would have taken 36 sec more than road A.
Interviewer tests your comfort with numbers and your confidence with the answer in this step.

Background : Recently, one of the junction (say, X) on road A got too crowded and a traffic signal was installed on the same. The traffic signal was configured for 80 seconds red and 20 seconds green. Let’s denote the seconds of signal as R1 R2 R3 … G1 G2 G3 . Here, R1 denotes 1sec after signal switched to red.
Question : Does it still makes sense to take road A, or to switch to road B provided the average speed on the road A is still the same except the halt at signal?
Solution : Let’s assume I come to the signal at a random time. Hence, probability of getting to the signal at R1 R2 R3 …or G1 G2 G3 are all equal. Hence, the expected time taken at the signal is :

E(halt time) = (1+2+3+4+…….80)/(80+20) = (80*81)/(100*2) = 32.4 seconds.
Still we see 32.4 sec < 36 sec. Hence, it still made sense to take road A.
Interviewer tests your knowledge of statistics (Calculation of expected value) , approach to the problem and the interpretation of the final results in this step.

Background : Till this point, the solution will look good in books. Lets spice the problem up by ground realities. If I reach the signal at R1, I will be in the front rows to be released once the signal turns green. Whereas, if I reach the signal at R80, I might have to wait for some time even after signal turns green because the vehicles in the front rows will block me for some seconds before I start. Let’s take some realistic guesses for the wait time after signal turns green.
R1 – R 10 : 0 sec , R11-R20 : 3 sec , R21 – R60 : 10 sec, R61 – R80 : 15 sec, G1-G15 : 5 sec, G15-G20 : 0 sec
Question : Does it still makes sense to take road A, or to switch to road B provided the average speed on the road A is still the same except the halt at signal?
Solution :.

E(halt time) = {(1+2+3+4+…….80) + 3*10 + 10*40 + 15*20 + 5*15}/(80+20) = 40.15 seconds.
This time the game changes and as 40.15 sec > 36 sec, I will prefer road B over road A.
Interviewer tests how well swiftly you change some of the assumption so as to minimize the added calculations.

Background : Even after making such logical calculation, I noted that in 30 different events, I was commuting more than 25 sec faster on road A compared to road B every single time. I did not change my average velocity on either of the roads. It could have been acceptable in case I found x number of event where A wins and 30 – x where B wins. But A winning every single time was fishy. I was struggling for last 10 days to figure out a valid cause. It struck me today and following is what I figured out:
The signal Z ( before the two roads split), which I initially though had nothing to do with the calculation was actually the game changer. Here is how it played a role.  This signal had the exact same cycle as the signal at point X i.e. 90 sec red and 20 sec green. Whenever, the two lights have the same cycle, the incidence on signal X is no longer random.
Question : Does it still makes sense to take road A, or to switch to road B provided the average speed on the road A is still the same except the halt at signal?
Solution : 

Say, my average speed vary on road A from 25km/hr to 30km/hr. The signal X is offset from signal Z by 25 seconds. Hence, when it turns green at Z, it is R55 at signal X.
Case 1 : (Light traffic) Time taken to cover road A = 2 mins = 120 sec
Reading at X when I reach the signal = R55 + 120 = R75.
Case 2 : (Heavy traffic) Time taken to cover road A = 2 mins 24 sec = 144 sec
Reading at X when I reach the signal = R55 + 144 = G19
Hence, the probability of R1- R74 is zero. And the revised equation of expected time is :
E(halt time) = (5 + 4+ 3+ 2+ 1 + 15*5 + 5*15)/25 = 6.6 sec
Therefore, as 6.6 sec < 36 sec road A always wins on road B.
Thus, the assumption of random events is not always true. Try to figure out all possible factors that can possibly influence the happening of event before making random events assumption.
Interviewer tests your out of the box thinking, questioning your assumption skill and interpretation of results skill in this step.

End Notes:  
Did you find the article useful? Share with us any other problem statements you can think of. Do let us know your thoughts about this article in the box below.
In one of the upcoming articles, we will share how an interviewer judges an analyst during a case study.
If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our facebook page.
Share this:Pocket

	Related
			
						
						
		"
"Tips to train your brain on Analytical thinking","Tavish Srivastava",2014-01-26 19:11:00,1,"
							
										
						
I recently started going to the GYM. Quite a big achievement for me to be going to the gym regularly for more than a month. What always made me irregular was a lack of motivation to go to the gym everyday. However, I made some key changes this time. For starters, I paid a personal trainer and tightly followed a diet schedule. In around 20 days I realized I was able to lift 3 times the weight and 1.5 times the repetitions of the same exercise. Did my muscles become thrice as strong as before? No, what my trainer played on was “the muscle memory”. Say, I lifted 10kg on a particular exercise yesterday and struggled to make 10 repetition, but somehow did it. Today, my muscle already knows I was able to complete 10 repetition on 10 kg. This time I struggle far less, because I am already prepared for what is coming.
I realized that same logic applies with the power to think. If you make calculations on daily basis, your calculations become more reflexive and accurate. An average working person in weekday spends 25-30% of his time sleeping, 40-60% of his time working , 10% of time eating and 15-25% idle. In this busy world more than 50% of our idle time is spent on road. You can use this particular time to develop sharper reflexes on numbers. This article will illustrates some engaging methods that I use in this idle time to sharpen my brain reflexes.
Some examples :  
 Driving alone to office, sitting in a cab to airport ,and travelling in trains, metro or bus are boring . I, however, engage myself in small puzzle solving which not only engages me, but also sharpens my brain reflexes. Here I will take some puzzles which I solve everyday while on the way to the office:
1. Escape cops :
By far “the most interesting one” . Everyday, I am just in time to leave for office. If the traffic is heavy, it becomes sometimes inevitable to cross signal just after it turns red. But on some blind turns, you find the traffic police waiting for the next meat. Here’s what I did to predict the number of police standing on the blind turns. I took two attributes to predict whether I will find cops or not. These attributes were :
a. Day of the week
b. Pattern of cops on previous junctions

Using the above decision tree, I find a particular node, where I found almost zero probability to find a cop on blind junction. Till date, the algorithm works fantastically, but I am still figuring out better attributes to follow.
2. Time to office :
Here’s an interesting one again. It takes me 35 mins to reach office. But in case I get late for 2 mins at any particular road, I am almost able to calculate the exact time I can expect to reach the office. It’s simple but accurate. I have calculated the time it takes to cover each segment of the route and a factor in different scenarios of traffic at each segment. In total I have 7 check points at a difference of 5 mins. each in case of light traffic. Looking at the traffic in first segment gives me reasonable information to find the right multiplier for each of the segments.

Till date I have been able to predict the time to office within first 5 mins of drive in a confidence interval of +/- 3 mins.
3. How fast is the other vehicles :
This is the most addictive one. I always know my own vehicle’s speed and can judge the distance of approach of other vehicles in 10 seconds. Hence, I am able to calculate the relative velocity of other vehicle and, finally, the absolute velocity of the other vehicle.

4. Sizing of services we use :
Whenever I take an auto-rickshaw, taxi or any other services, I try to calculate the total sizing of that business model and the profit individual players make in the process. I have had the most interesting conversation with the drivers, who always had some new insight on ground realities which I missed to incorporate while thinking of the business model. You can read my article on sizing problems here (http://www.analyticsvidhya.com/blog/2014/01/tips-crack-guess-estimate-case-study/). Even though the article focuses more on interview approach, you can leverage same framework to do the sizing of services on daily basis. Not only will you find it interesting but also you will improve your analytical skills.
Potential benefits of implementing such practices :  
 Three basic benefits which I have realized by implementing such practices are as follows:
1.  Power to innovate in problem statement and its solution:
To bring an out of box solution, you always need an out of the box problem statement. As an analyst, I continuously feel the need to find fact based problems which can create significant impact. We are surrounded by facts, and to search for the right facts to build up implementable solution is what it takes to be a successful analyst.
When we do a regular search of such interesting problems, our reflexes to look at imperfections sharpens. We are more capable of  to think of new business cases which can become impact full projects.
2. Sharpen the reflexes to calculate faster:
Practice makes man perfect. It does so in two ways. First, your brain tends to retain some frequent calculations. Say, 1 million * 1000 = 1 billion. You don’t need to calculate the number of zeros because it gradually becomes very intuitive. Imagine thousands of such combinations right on the tip of your head. Engaging free time to make meaningful calculations for sure makes your calculative reflexes sharper.
3. Think about the same problem in many angles and choose the most effective one:
The puzzles can be very simple, but thinking the same puzzle with different methods and then comparison of different answers not only is interesting in nature but also helps you build on your evaluative skills. We gradually start to implement the same on complex scenarios.
End Notes:  
 Most of my experiences which I shared in this article were implemented while I was not driving.Do try this practice and let us know of your exciting routine problems. Try to be innovative while defining a new problem. The more challenging is the problem more interesting will be your after thoughts.
Did you find the article useful? Share with us any other problem statements you can think of.Also share with us other techniques you use to keep your brain in its front foot.  Do let us know your thoughts about this article in the box below.
If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our facebook page.
 
Share this:Pocket

	Related
			
						
						
		"
"Framework to build logistic regression in a rare event population","Tavish Srivastava",2014-01-18 01:23:00,7,"
							
										
						Only 531 out of a population of 50,431 customer closed their saving account in a year, but the dollar value lost because of such closures was more than $ 5 Million. 
The best way to arrest these attrition was by predicting the propensity of attrition for individual customer and then pitch retention offers to these identified customers. This was  a typical case of modeling in a rare event population. This kind of problems are also very common in Health care analytics.
In such analysis, there are two challenges :
Accurate prediction is difficult because of small sample bias.
The accuracy of prediction need to be extremely high to make an implementable strategy. This is because high number of false positive, unnecessarily burdens the retention budgets.

We can find number of statistical papers on this specific problem. This article will collect the best practices and layout the step by step process to make a logistic regression model in a rare event population.
Why not simply make a logistic regression model on the population? 
The problem basically is that maximum likelihood estimation of the logistic model is well-known to suffer from small-sample bias. And the degree of bias is strongly dependent on the number of cases in the less frequent of the two categories. Try estimating the degree of bias in each of the following samples:
A. 20 events in a sample size of 1000 (Response rate : 2%)
B. 180 events in a sample size of 10000 (Response rate : 1.8%)
C.  990 events in a sample size of 1000 (Response rate : 99%)
Try not to see the answer below before you have your answer ready.
The correct answer here is C > A > B . C will suffer with the problem of small-sample bias most. Confused? Did we not say this problem exist in cases where events are too low? The problem is not specifically the rarity of events, but rather the possibility of a small number of cases on the rarer of the two outcomes. Why “A>B”? Its simply because of the population size. Even though the response rate in B is lesser than A, A struggles with the problem more than B. Hence, smaller the sample size ,higher is the risk of small sample bias.
 What is the solution in such problems? 
The solution in such problems is slightly longer than a normal logistic regression model. In such cases, we make a biased sample to increase the proportion of events. Now, we run logistic regression on the sample created. Once we have the final Logit equation, we transform the equation to fit the entire population.

Case study:  
Let’s consider the case in hand and walk through the step by step process. We have a population of 50,431 customers out of which 531 attrite in 12 months. We need to predict the probability of attrition, minimizing the false positives.
Step 1 :Select a biased sample
Total number of non attritors in the population is 49,900. We plan to take a sample of 1000 customers.  As a thumb rule, we select 25% of the sample size as the responders. Hence, we select 250 customers out of the 531 attriting customers. And rest 750 come from the 49,900 base. This sample of 1000 customers is a biased sample we will consider for our analysis.
Step 2 : Develop the regression model
We now build a logistic regression model on the biased sample selected. We make sure that all the assumptions of the logistic regression are met and we get a reasonable lift because the lift tends to decrease after the transformations.
Step 3 : Overlay equation on the population:
Using the equation found in step 2, get the number of attritors in each decile of the overall population. In the table below, -Log odds (Predicted) directly comes from the regression equation. Using this function, one can find the Predicted attrition for each decile.

 Step 4: Solve for intercept and slope transformation
Using the actual and the predicted decile value of the log odds, we find the slope and the intercept required to transform the equation of the sample to the equation of the population. This equation is given by,
{- Log odds (actual)} = slope * {-Log odds(predicted)} + Intercept
Find the slope and intercept using the 10 data-points, each corresponding to each decile.

In this case slope is 0.63 and the intercept is 1.66 .

As seen from the above figure, the actual and the transformed logit curve for each decile is much closer compared to the predicted curve.
Step 5: Validate the equation on out of time sample :
Once we reach a final equation of the logit function, we now validate the same on out of time samples. For the case in hand, we take a different cohort and compile the lift chart. If the model holds on out of time as well, we are good to go.
End Notes:  
Did you find the article useful? Share with us any other techniques you incorporate while solving a rare event problem.  Do let us know your thoughts about this article in the box below.
If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our facebook page.
Share this:Pocket

	Related
			
						
						
		"
"Tips to crack a guess estimate (Analytics case study)","Tavish Srivastava",2014-01-06 22:17:00,2,"
							
										
						
After a wait for 3 long hours, it was my turn to enter the interview room. The first question asked to me by the interviewer was “Can you estimate the total number of cigarettes consumed per month in India?” Having worked on a project for ITC in one of the core courses, I was able to crack the problem. I started with the total number of factories of ITC in India and calculated the number of cigarettes manufactured by ITC in a year with the help of average turnover time. Further, I made good guesses on the %cigarettes exported and the %share of ITC in India. Finally I got the number of cigarettes consumed per month in India which convinced the panel.
Such questions are very common in analytics and management consulting interview. If you wish to appear for companies of this genre, this article will be very useful. I was fortunate to have got this puzzle. What if I had no clue on the number of ITC factories producing cigarettes? After this interview I tried solving many such puzzles to get a comfort level with such problems. In this article I will walk through some techniques I now use to crack such puzzles.
What does interviewer evaluate using guess estimate case study?  
Very often in the role of Analyst and Consultant, clients expect quick initial scaling or sizing of potential projects. This is the reason that such questions are so common in interviews for recruitment of such roles. The interviewer is looking out for four key traits in this interview.
1. How structured is your approach?
2. How comfortable are you with numbers?
3. Are you able to make quick checks on the efficiency of different methods?
4. Can you do back of the mind calculations and validate the magnitude of numbers?
Framework to solve a guess estimate problem:  
Knowledge of certain techniques used for such guess estimates helps keeping the approach structured in the interview. Let’s address the cigarette estimate problem from the demand side (without using the number of ITC factories) while discussing the key techniques.  Following are the 4 key techniques which will help you in such case interviews :

1. Find the right proxy: This is by far the most important technique. The proxy is a parameter which behaves in a similar manner as the dependent parameter. In the cigarette estimation problem, the population of India is a good proxy for the number of cigarette consumed monthly in India.  If the population of India increases, it can be safely said that cigarette consumption will increase proportionally. Other proxies used is the growth in population, growth in demand of a newly introduced technology, average number of planes parked at major airports etc.
2. Segment till you can find differentiated clusters : Estimating parameters on a segment level is far more accurate than making guesses on the overall population. In the cigarette estimation problem, population below 16 years can safely be ignored for cigarette consumption and female population is expected to have a lower average cigarette consumption than male population. This is how segmentation helps making accurate assumptions.
3. Do smart calculations and number round off : Speed is very critical in such problems and one needs to maintain a balance between accuracy and time consumption. Say you need to fin 2999/3. It is much easier to calculate 3000/3 than 2999/3. In such cases right the answer as 1000 (-) . This indicates the number is slightly lesser than 1000 and can be compensated in further calculations.
4. Validate number magnitude : It is always a good idea to keep on validating intermediate numbers using your experience and sense checks.
Some ground rules to be followed while doing a guess estimate:  
Following are some factors one should keep in mind while solving a guess estimate problem :
1. Analyze all possible uses of the subject. For example, while estimating the number of tennis balls in India, one should consider balls being used in tennis, cricket and all other sports which are potential users of tennis balls.
2. Keep population of your country, state and city on finger tips. As population is the most common proxy for many case studies, such numbers give a good starting point.
3. Have a look on some key parameters for airline management : Many of guess estimate problems are related to airlines. A sense on the number of flights which normally stays in major airports, time lag between flight take off etc. helps.
4. Draw neat diagrams to show the segmentation. This not only helps do calculations quickly but also makes it easier to redo the calculations on the segment level if required.
5. Don’t do round off in the same direction. Such round off magnifies the error term. Putting a sign in front of rounded off number helps.
Step by step approach of solving a guess estimate problem : 
Case 1: Estimate the number of cigarettes consumed monthly in India
Solution: A good proxy in such problem is the population of India i.e. 1.2 billion. Following is an effective way to segment this population:

Following were the key considerations in building the segmentation and the intermediate guesses:
1. The rural population consumes far lesser cigarettes than urban because of the purchasing power difference.
2. Male consume more cigarettes than female in both urban and rural populations.
3. Children below 16 years consume a negligible number of cigarettes.
4. Male to Female ratio in Urban is closer to 1 than that of Rural.
5. Male to Female ratio in younger generations is closer to 1 than that of older. This is because of the increase in awareness level.
6. Bulk of population start smoking after getting into a job and hence the average number cigarettes are higher in older groups.
7. Total number of cigarettes from the supply side also come to around 10 Trillion, which gives a good sense check on the final number.
Case 2: Estimate the number of WhatsApp Android application installed
Solution: A good proxy in this problem is the world population i.e. ~7.2 Billion. Following is a possible approach to this problem :

The actual number of Whatsapp installed on Android phone is slightly more than 100 Million. As can be seen from this example that guess estimates can be fairly accurate if we choose good segments and approximations.
Case 3: Estimate the number of tennis balls bough in India per month
Solution: A good proxy in this problem is the number of cities in India i.e. ~1700. The catch in this problem is to analyze where all can we use tennis balls. Once we have the number of tennis balls used monthly, we can easily find the number of tennis ball bought in a month using the lifetime of tennis balls.
Following is an effective way to segment this population:

Following were the key considerations in building the segmentation and the intermediate guesses:
1. Rural areas have negligible number of tennis courts.
2. Metro cities have the highest number of sectors.
3. For each sectors in metro cities, the number of grounds for both tennis and cricket is higher. This is both because of the bigger area and the higher buying capacity in metros.
4.Number of balls consumed in metros per ground is higher because of the higher engagement in metros.
A challenge for the reader:  
Here is a practical example you can give a shot. Imagine you sitting in an interview and the interviewer asks “Estimate the number of aircrafts in air across the globe at this moment in time.” How will you answer this question ? Write down your approach in the comment box below to get opinion from experts.
End Notes:  
Guess estimates are one of the most common case studies asked in analytics interview. With right tools and techniques, this case study becomes a cake walk.
Did you find the article useful? Share with us any other techniques you incorporate while solving a guess estimate problem.  Do let us know your thoughts about this article in the box below.
If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our facebook page.

Share this:Pocket

	Related
			
						
						
		"
"Extracting right variables for your Regression model","Tavish Srivastava",2013-12-26 04:09:00,5,"
							
										
						Getting the right variables in your model and cleaning them can make or break your model.
The precision of the model depends on the breadth (diversity) and depth (spread of data and correct transformations) of variables. This article will take you through some of the techniques used in the industry to create or transform variables. We will also cover the techniques used in the industry to select the right set of variables out of an exhaustive list created in our next article on the subject.

Types of variables:  
Let’s categorize the possible variables to make the discussion easier and our analysis more structured. Following are the categories of variables we will discuss in this article :
1. Basic variable set
2. Derived variable set
3. Mathematically transformed variable set
4. Bin Variable set
5. Co-variant variable set

Following is a business case we will consider for creating these variable sets :
Predict the total business of a insurance branch in next 3 months.
1. Creating the basic variable set :  
Get this step right and you are half done. There is no set procedure to get the right set of base variables. There are two approaches to get the set of exhaustive base variable set. Following are the approaches :
1. Create hypothesis which can possibly affect the  dependent variable (Here we don’t even care if the data for this variable is even available)
2. Enlist all variables available (Here we don’t even care if this variable can possibly affect the dependent variable)
Here, we try to find all possible variable which can be collected in the analysis without thinking of shortlisting them for the final model. Each of these hypothesis falls in one of the following 3 categories :
a. Demographic variables : These variable defines quantifiable statistics for a data-point.  In the current business case we will include variables like : Location of the branch, Number of Sales managers, Mix of designation in the branch etc.
b. Behavioral variables :  These variables comes from the past performance  of the subject. In the current case we will include variables like business done by branch in last quarter, Ticket size of the business done by branch, performance metrics of the sales managers in the branch.
c. Psychometric variables : For the current business case we will want to include variables like Net Promoter score of the branch, Employee satisfaction score of the branch etc. These types of variables which generally come from surveys are the psychometric variables.
2. Creating the derived variable set :  
 After completing the list of the basic variables we  move on to the derived variables. These variables have a better predictive power and are very stable. These variables are combination of more than one basic variable. Let’s see how can we form derived variables for the case in hand. Following are the possible variables :
a .Revenue generated per resource = Revenue of the branch / Number of sales managers
b. Revenue on investment of the branch = Revenue of the branch / Total Cost of branch
c. Vintage of the branch = Today’s date  - Date of branch opening
d. Ratio of Senior to Junior employee = # Senior employee/ # Junior employee
This list can go on. Try hard to create all possible combination which are expected to influence the dependent variable. For some more ideas on derived variables, read this article.
3. Creating the Mathematically transformed set :  
Till this step we already have all the basic and derived variables. Now is the time to find the best possible transformation for each of these variables. Try to check all possible mathematical transformation such as Sine, Cosine, Logarithmic, Exponential , Square, Square root etc. Once you have all the transformation for each of the variables, you have to choose the transformation that best mimics the dependent variable. This transformation should come from both business sense and statistical method. Following is what I do to choose the best transformation for each variable :
Make a regression model with only 2 steps (step-wise) and the actual dependent variable. Choose the transformation that enter the model in these 2 steps. Check if the transformation found as the best fit makes business sense and use it in the final model. For the current problem in hand Log(Total Revenue) is possibly a better variable than Total Revenue as the marginal addition on prediction will go down as the total revenue increases.
4. Creating the Bin Variable set :  
 Creating bin variables is very essential in regression model. Find the bivariate plot between dependent variable and all other independent variables listed till now. Find the intervals where the relationship breaks in the bivariate. For the case in hand, lets say the performance of the branch is best if “Ratio of Senior to Junior employee” is between 0.9 to 1.1 . In such case we will create a bin variable between 0.9 and 1.1. It will look like as follows :
Bin = 1    for    0.9 <Ratio < 1.1
Bin = 0    otherwise
We will include this bin variable in the regression model.
5. Creating the Co-variant variable set :  
 Here comes the X-factor for our regression model. This step incorporate the best cuts of a CART model and significantly raises the prediction power of the regression model. For more details on this technique, please read this article.
Next steps to complete the regression model :  
 After we are done with the variable collection, following is the order to complete the regression model :
1. Clean the data on each of the dependent and independent variables.
2. Select the best predictive variable for the dependent variable
3. Create the regression model
4. Check the assumptions of the regression model using diagnostic plots (for more details, read this article)
5. Check the predictive power of the model
6. Check the stability of the model
7. Create the implementation tool

End notes :  
The more exhaustive our starting hypothesis the better is the predictive power of the model.Did you find the article useful? Share with us any other variable sets you incorporate in your model.  Do let us know your thoughts about this article in the box below.
If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our facebook page.
Share this:Pocket

	Related
			
						
						
		"
"How do banks identify the next best product need of its customer?","Tavish Srivastava",2013-12-08 22:49:00,1,"
							
										
						
Recently, I got a message of a new unknown transaction done on my credit card. I raised a dispute against the transaction. Within, 10 minutes my dispute was accepted and the transaction got cancelled.  The same day, I saw the product recommended to me as Chip Card and the tagline read “Make your transactions safer”. I ended up taking up this service.
How did the bank manage to convert a customer from potential attrition to a new X-sell lead? Definitely, streamlining of the process helped the banks take quick calls. But more importantly big data has helped banks to recognize each customer and identify his needs. After reading this article you will be forced to think on what grounds did the bank shortlist you for a particular offering every single time you become a X-sell lead.Was this your bank balance, your transactions with grocery vendor, your newly purchased car loan or ,merely, your last complain?
Introduction  : 
I am sure you might have noticed a tab in your online bank account which says “Products for You” and “Deals for you”.How are the banks almost always bang on.

If you are about to say bank use propensity of a customer to buy the new product to make X-sell offers, you know just a fraction of the whole story. These calculations have modified a lot in last 5-7 years. Today, banks perform an infinite number of calculations on a single customer to come up with the next best product for him/her. This primarily includes 4 main factors :
1. How risky is the customer if he takes up the proposed product?
2. What is the propensity of the customer to take up this product?
3. How profitable is the customer expected to be if he takes up the new proposed product?
4. What is the utility of the value propositions of the new product to the customer?
The first three points focus on the profitability for the bank and the last point focuses on the use of this product for the customer. In a well baked strategy both profitability and customer Centricity need to be balanced. None of the factors mentioned can be analyzed alone to get to the final strategy. All the dimensions need to be considered before making an offering. We will discuss this analysis done by banks in more detail in the following sections.
Metrics considered for targeting : 
Lets take an example to make this discussion more relevant. Lets say, Bank X offers three types of credit cards i.e. Miles Card, Rewards Card and Cash Back Card.  Bank X wants to build a strategy to X-sell a second card to its existing Customer base.
Using Propensity of a customer to buy a product for targeting :
Propensity prediction is based on past trends of product take up by various segments of customers.Following is an illustrative table of the probabilities of Miles card holder taking up other two products :

The advantage of using propensity of a customer to take up a product for targeting is that the overall responders are expected to be higher. However, this approach does not consider the quality of the responder. For instance, based on propensity we target household “1032021<U+2033> (say A) and not “2310231<U+2033> (say B).  But if household B generates 10 times the revenue of A, if responded, then B was actually a better target. Hence, we see the need of incorporating the lifetime value or revenue generated by each customer along with the propensity factor.
Using lifetime value of household along with the propensity :
With the expected revenue generated by each household, it is possible to tabulate the expected value generated by finding a product of propensity to buy and revenue generated. Following is an illustrative table of expected values when targeted with a Rewards Card or Cash-back Card.

Clearly, household  “1032021<U+2033> (say A) has a lower expected value for Rewards Card compared to  “2310231<U+2033> (say B).  If the targeting were made only based on propensity values, we might have targeted household with lower expected value. This matrix gives a more complete picture compared to only propensity, but we still have not covered some other dimensions. For instance, we are still not sure on the utility of cards for each customer. Say, household A prefers a Rewards card and household B prefers a Cash Back card. If we still target customer B with Rewards card, it is merely because of the additional profitability and not customer utility. If it was a monopoly market, it wouldn’t have mattered. But as we know, its a highly competitive market and other bank can recognize this need of the customer and offer need based card. If this happens, the primary bank looses household B because it targeted the household with a product which was more profitable for the bank but not as per the need of the customer.
Using Customer preference/Utility for targeting:
If a customer is targeted only based on expected value, this can lead to a higher revenue for the bank but will not help in attaining customer delight. Customer delight might have a low immediate financial impacts but on a longer run, it makes a big difference in a fierce market of banks fighting with each other for wallet share. To incorporate the angle of customer centricity, we add a metric of utilization index for each card for each customer.
Lets take an example, say the major difference between Miles card and Rewards card is the cash limit. For miles card , the cash limit is $10k and for the rewards card is $20k. Any cash spent over limit is chargeable.  If a customer spends more than $10k on a Miles card regularly, he is giving a penalty everytime he oversteps this line. Rewards card, hence, becomes a good value proposition for this customer. In case, this customer has a high expected value as well, this customer is indeed the best target for Rewards card.But targeting customers just based on their preference might not be profitable for the bank.
As we saw each of the dimension had its pros and cons. Let’s make this more interesting and analyze customer in more than one dimension. This is what Banks do for targeting. There are multiple dimensions on which a customer is analyzed before targeting. No single dimension gives optimal targeting strategy.
Targeting based on multiple dimension :
Case 1 : Only Expected Value is considered :
By targeting only high expected value customers, we will end up targeting only customers falling in the last row of the following table.

Such strategy leads to target households (H on expected value and L on utility) who are least interested in this kind of offers and might as well result in household attrition.
Case 2 : Expected Value and Customer Preference considered together :
Let’s try to include Utility as well in targeting.Following targeting is an example of how cross grid helps bank in targeting decision making.

As you can see, instead of households with low preference and high expected value this targeting includes household with high product preference and medium expected value. This might lead to immediate financial negative impact but will lead to higher customer satisfaction and , thereby, higher customer lifetime value.
Adding more dimension leads to a precise targeting. Other than expected value, risk of a customer defaulting is as well considered as an additional dimension.  There are strict risk cut-offs for such targeting . Additionally, Legal authorities restricts banks from discriminating targeting some social class as well. Every targeting strategy needs to be unbiased to these restricted classes.
End Notes:
The examples taken in this article are illustrative. Also, the card offered to me recently, mentioned in the start of this article, was a trigger based X-sell campaign. We will cover the difference between trigger based campaigns and periodic batch campaigns in some other article as it is not the focus of this article.
Share with us any interesting sales pitch your bank made to you. Do you think there are any other dimensions which should be included for targeting of X-sell campaign? Did you find the article useful?  Do let us know your thoughts about this article in the box below.
If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our facebook page.
Share this:Pocket

	Related
			
						
						
		"
"Diagnosing residual plots in linear regression model","Tavish Srivastava",2013-12-01 20:17:00,1,"
							
										
						My first analytics project involved predicting business from each sales agent and coming up with a targeted intervention for each agent.
I built my first linear regression model after devoting a good amount of time on data cleaning and variable preparation. Now was the time to access the predictive power of the model. I got a MAPE of 5%, Gini coefficient of 82% and a high R-square. Gini and MAPE are metrics to gauge the predictive power of linear regression model. Such Gini coefficient and MAPE for an insurance industry sales prediction are considered to be way better than average. To validate the overall prediction we found the aggregate business in an out of time sample. I was shocked to see that the total expected business was not even 80% of the actual business. With such high lift and concordant ratio, I failed to understand what was going wrong. I decided to read more on statistical details of the model. With a better understanding of the model, I started analyzing the model on different dimensions. After a close examination of residual plots, I found that one of the predictor variables had a square relationship with the output variable.

Since then, I validate all the assumptions of the model even before reading the predictive power of the model. This article will take you through all the assumptions in a linear regression and how to validate assumptions and diagnose relationship using residual plots.
Assumptions of Linear Regression Model :
There are number of assumptions of a linear regression model. In modeling, we normally check for five of the assumptions. These are as follows :
1. Relationship between the outcomes and the predictors is linear.
2. Error term  has mean almost equal to zero for each value of outcome.
3. Error term  has constant variance.
4. Errors are uncorrelated.
5. Errors are normally distributed or we have an adequate sample size to rely on large sample theory.
The point to be noted here is that none of these assumptions can be validated by R-square chart, F-statistics or any other model accuracy plots. On the other hand, if any of the assumptions are violated, chances are high that accuracy plot can give misleading results.
 
How to use residual for diagnostics :
Residual analysis is usually done graphically. Following are the two category of graphs we normally look at:
1. Quantile plots : This type of is to assess whether the distribution of the residual is normal or not. The graph is between the actual distribution of residual quantiles and a perfectly normal distribution residuals. If the graph is perfectly overlaying on the diagonal, the residual is normally distributed. Following is an illustrative graph of approximate normally distributed residual.
Let’s try to visualize a quantile plot of a biased residual distribution.

In the graph above, we see the assumption of the residual normal distribution being clearly violated.
2. Scatter plots:  This type of graph is used to assess model assumptions, such as constant variance and linearity, and to identify potential outliers. Following is a scatter plot of perfect residual distribution

Let’s try to visualize a scatter plot of residual distribution which has unequal variance.

In the graph above, we see the assumption of the residual normal distribution being clearly violated.
Example :
For simplicity, I have taken an example of single variable regression model to analyze residual curves. Similar kind of approach is followed for multi-variable as well.
Say, the actual relation of the predictor and the output variable is as follows:

Ignorant of the type of relationship, we start the analysis with the following equation.

Can we diagnose this misfit using residual curves?
After making a comprehensive model, we check all the diagnostic curves. Following is the Q-Q plot for the residual of the final linear equation.

Q-Q plot looks slightly deviated from the baseline, but on both the sides of the baseline. This indicated residuals are distributed approximately in a normal fashion.
Following is the scatter plot of the residual :

Clearly, we see the mean of residual not restricting its value at zero. We also see a parabolic trend of the residual mean. This indicates the predictor variable is also present in squared form. Now, let’s modify the initial equation to  the following equation :

Following is the new scatter plot for the residual of the new equation :

We now clearly see a random distribution and a approximate zero residual mean.
End Notes:
Every linear regression model should be validated on all the residual plots . Such regression plots directionaly guides us to the right form of equations to start with. You might also be interested in the previous article on regression ( http://www.analyticsvidhya.com/blog/2013/10/trick-enhance-power-regression-model-2/ )
Do you think this provides a solution to any problem you face? Are there any other techniques you use to detect the right form of relationship between predictor and output variables ? Do let us know your thoughts in the comments below.
If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our facebook page.
Share this:Pocket

	Related
			
						
						
		"
"Some tricky SAS interview questions","Tavish Srivastava",2013-11-24 21:18:00,1,"
							
										
						While working extensively on SAS-EG , I lost touch of coding in Base SAS. I had to brush up my base SAS before appearing for my first lateral interview. SAS is highly capable of data triangulation, and what distinguishes SAS from other such languages is its simplicity to code. There are some very tricky SAS questions and handling them might become overwhelming for some candidates. I strongly feel a need of a common thread which has all the tricky SAS questions asked in interviews. This article will give a kick start to such a thread. This article will cover 4 of such questions with relevant examples. This article is the first part of tricky SAS questions series. (Next article : http://www.analyticsvidhya.com/blog/2014/04/tricky-base-sas-interview-questions-part-ii/  ) Please note that the content of these articles is based on the information I gathered from various SAS sources.
1. Merging data in SAS :
Merging datasets is the most important step for an analyst. Merging data can be done through both DATA step and PROC SQL. Usually people ignore the difference in the method used by SAS in the two different steps. This is because generally there is no difference in the output created by the two routines. Lets look at the following example :

Problem Statement : In this example, we have 2 datasets. First table gives the product holding for a particular household. Second table gives the gender of each customer in these households. What you need to find out is that if the product is Male biased or neutral. The Male biased product is a product bought by males more than females. You can assume that the product bought by a household belongs to each customer of that household.
Thought process: The first step of this problem is to merge the two tables. We need a Cartesian product of the two tables in this case. After getting the merged dataset, all you need to do is summarize the merged dataset and find the bias.
Code 1

Proc sort data = PROD out =A1; by household;run;
Proc sort data = GENDER out =A2; by household;run;
Data MERGED;
    merge A1(in=a) A2(in=b);
    by household;
    if a AND b;
run;
Code 2 :

PROC SQL;
     Create table work.merged as
           select t1.household,  t1.type,t2.gender
           from prod as t1, gender as t2
           where t1.household = t2.household;
quit; 
Will both the codes give the same result?
The answer is NO. As you might have noticed, the two tables have many-to-many mapping. For getting a cartesian product, we can only use PROC SQL. Apart from many-to-many tables, all the results of merging using the two steps will be exactly same.
Why do we use DATA – MERGE step at all?
DATA-MERGE step is much faster compared to PROC SQL. For big data sets except one having many-to-many mapping, always use DATA- MERGE.
2. Transpose data-sets :
When working on transactions data, we frequently transpose datasets to analyze data. There are two kinds of transposition. First, transposing from wide structure to narrow structure. Consider the following example :

Following are the two methods to do this kind of transposition :
a. DATA STEP :

data transposed;set base;
        array Qtr{3} Q:;
        do i = 1 to 3;Period = cat('Qtr',i);Amount = Qtr{i} ;output;end;
        drop Q1:Q3;
        if Amount ne .;
run; 
b. PROC TRANSPOSE :

proc transpose data = base out = transposed 
                (rename=(Col1=Amount) where=(Amount ne .)) name=Period;
by cust; run; 
In this kind of transposition, both the methods are equally good. PROC TRANSPOSE however takes lesser time because it uses indexing to transpose.
Second, narrow to wide structure. Consider an opposite of the last example.

For this kind of transposition, data step becomes very long and time consuming. Following is a much shorter way to do the same task,

Proc transpose data=transposed out=base (drop=_name_) prefix Q;
       by cust;
       id period;
       var amount;
run; 
3. Passing values from one routine to other:
Imagine a scenario, we want to compare the total marks scored by two classes. Finally the output should be simply the name of the class with the higher score. The score of the two datasets is stored in two separate tables.
There are two methods of doing this question. First, append the two tables and sum the total marks for each or the classes. But imagine if the number of students were too large, we will just multiply the operation time by appending the two tables. Hence, we need a method to pass the value from one table to another. Try the following code:


DATA _null_;set class_1;
       total + marks;
       call symputx ('class1_tot',total);
run;
DATA _null_;set class_2;
       total + marks;
       call symputx ('class2_tot',total);
run;
DATA results;
       if &class1_tot > &class2_tot then better_class = 1;
       else if &class1_tot > &class2_tot then better_class = 2;
       else better_class = 0;
run; 
Funtion symputx creates a macro variable which can be passed between various routines and thus gives us an opportunity to link data-sets.
4. Using where and if : 
“Where” and “if” are both used for sub-setting. Most of the times where and if can be used interchangeably in data step for sub-setting. But, when sub-setting is done on a newly created variable, only if statement can be used. For instance, consider the following two programs,

Code 1 :                                                                                  Code 2 :
data a;set b;                            data a;set b;
      z= x+y;                                  z= x+y;
      if z < 10;                               where z < 10;
run;                                     run; 
Code 2 will give an error in this case, because where cannot be used for sub-setting data based on a newly created variable.
End Notes : 
These codes come directly from my cheat chit. What is especial about these 4 codes, that in aggregate they give me a quick glance to almost all the statement and options used in SAS. If you were able to solve all the questions covered in this article, we think you are up for the next level. You can read the second part of this article here ( http://www.analyticsvidhya.com/blog/2014/04/tricky-base-sas-interview-questions-part-ii/ ) . The second part of the article will have tougher and lengthier questions as compared to those covered in this article.
Have you faced any other SAS problem in analytics interview? Are you facing any specific problem with SAS codes?  Do you think this provides a solution to any problem you face? Do you think there are other methods to solve the problems discussed in a more optimized way? Do let us know your thoughts in the comments below.

You can read part II of this article here
If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our facebook page.
Share this:Pocket

	Related
			
						
						
		"
"Challenges using Clustering with uniform population","Tavish Srivastava",2013-11-17 20:27:00,1,"
							
										
						I was starring at the computer screen for the final clustering result. Finally, I opened the output file and found the first cluster with more than 90% of the data-points. 6 other clusters composed the 10% of the remaining data-points. I packed up my bag and thought over the possible reasons of this dramatic failure of the technique on the way home.
This article will illustrate how to tackle such problems in a systematic manner by exploring the possible reasons of such failure. As this is the second part of the concept (please read basics of clustering here), we assume the reader is comfortable with the two clustering techniques widely used in the industry and their mechanisms.
Definition of a good cluster analysis :
There are two basic requirements of a good cluster analysis :
1. Data-points within same cluster share similar profile : Statistical method to judge this criterion is simply checking the standard deviation for each input variable in each cluster. A perfect separation in case of cluster analysis is rarely achieved. Hence,even one standard deviation distance between two cluster means is considered to be a good separation.
2. Well spread proportion of data-points among clusters : There are no industry standards for this requirement. But a minimum of 5% and maximum of 35% of the total population can be assumed as a safe range for each cluster.
A hard nut to crack:
This illustrative example is very similar to the problem I faced recently while doing a cluster analysis. Following is what I did to build clusters :
1. Identifying all possible variables which can be used for clustering technique
2. Outlier treatment and missing value fixation
3. Identifying most significant variables to be considered for final clustering analysis using VARCLUS, one of the technique to find independent variables
4. Finding final clusters using FASTCLUS, fastest technique to do a k-means clustering
Following is a graphical representation of the clusters I get:

Cluster 1   :            90%
Cluster 2     :            5%
Cluster 3     :            5%
This does not qualify as a good cluster analysis.
What could have gone wrong?
Whenever stuck with bad results for any modelling technique, the best way to solve it is not trying different solution but to explore what could have gone wrong. Let’s try to do the same in this case.  Following are some of the hypothesis around what could have gone wrong :
1. Outliers still present : Try making outlier definition stricter. This has been a savior for me in most of the cases. Try capping and flouring instead of removing the outlier.

2. Presence of overshadowing variable : This kind of variable generally become visible in the FASTCLUS output, in the significance table. If the significance index of a particular variable is exceptionally high, try replacing it with the next best variable. This next best variable can be found at the VARCLUS step.
What if the problem still remains? What does this indicate?
It indicates that the population is too homogeneous to be segmented. This problem is the hardest nut to crack. Try incorporating more data points to start with. If the problem still remains we need to do clustering at a finer level. We will now discuss this method in detail.
Possible Clustering Technique and their advantages
1. Hierarchical Clustering : The advantage of this technique is that it becomes very handy to club different data-points. In other words, the output of the model is a tree and we can choose any combination from the tree to build clusters for different number of clusters. The disadvantage of technique is that it can handle only few data-points and takes exponential time for the high number of observations. If this were not a constraint for the technique, no cluster analysis would have failed.
2. k-means Clustering : The advantage of this technique is that it can handle huge number of observations and takes very less time compared to all other available techniques. The disadvantage of the technique is that changing the process in between to club data-points to the second best cluster is not possible. Hence, the method is much more rigid when compared to  Hierarchical Clustering.
The two techniques have opposite pros and cons and hence can be used together to compliment each other.
What if we combine the two methods?
Following is the technique I finally used to get reasonable and actionable clusters :
1. Use outlier removal and overshadowing variable removal technique
2. Use k-means method to get many granular cluster : Build number of granular clusters. For n number of final cluster required, I put k=3 to 4 times n. Following is an illustrative figure to demonstrate this step :

We get 10 smaller clusters with size ranging from 2-25% of the same population.
3. Use Hierarchical Clustering to club these granular clusters : Plug the mean values of the granular clusters as individual data-points to hierarchical clustering. This will give a line diagram as output. Choose the combinations which will not lead a cluster to go above 35% and make sure all the clusters come above 5%. Following is an illustrative figure of this step :

The output of this step is a set of clusters complying to both the constraints of a good clustering analysis.
End Notes :
As the method demonstrated above uses finer differences between observation, which were not differentiated by simply using k-means, the technique needs to be verified for the separation test.  In such cases, when strong separation is not possible between observations, vastly different actionable should not be taken for each cluster.
What do think of this technique? Do you think this provides a solution to any problem you face? Are there any other techniques you use to improve separation of your clustering models ? Do let us know your thoughts in the comments below.
If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our facebook page.
 
 
Share this:Pocket

	Related
			
						
						
		"
"How to perform clustering analysis","Tavish Srivastava",2013-11-12 14:43:00,3,"
							
										
						Clustering is one of the toughest modelling techniques. 
It takes not only sound technical knowledge, but also good understanding of business. We have split this topic into two articles because of the complexity of the topic. As the technique is very subjective in nature, getting the basics right is very critical.
This article will take you through the basics of clustering. The next article will get into finer details of the technique and identify certain scenarios where the technique fails. The article will also introduce to a simple method to counter such scenarios.
What is clustering analysis?
Clustering analysis is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense or another) to each other than to those in other groups (clusters). Following figure is an example of finding clusters of US population based on their income and debt :

It is one of the subjective modelling technique widely used in the industry. One of the examples of common Clustering usage is segmenting customer portfolio based on demographics, transaction behavior or other behavioral attributes.
Why do we need clustering ?
Analytics industry is dominated by objective modelling like decision tree and regression. If decision tree is capable of doing segmentation, do we even need such an open ended technique? The answer to this question is in one of the advantages of using clustering technique. Clustering generates natural clusters and is not dependent on any driving objective function. Hence such a cluster can be used to analyze the portfolio on different target attributes. For instance, say a decision tree is built on customer profitability in next 3 months. This segmentation cannot be used for making retention strategy for each segment. If segmentation were developed through clustering, both retention and profitability strategy can be built on these segments.
Hence, clustering is a technique generally used to do initial profiling of the portfolio. After having a good understanding of the portfolio, an objective modelling technique is used to build specific strategy.
Industry standard techniques for clustering :
There are a number of algorithm for generating clusters in statistics. But we will discuss in detail only two such techniques which are widely used in the industry. These techniques are as follows :
1. Hierarchical Clustering : This technique operate on the simplest principle, which is data-point closer to base point will behave more similar compared to a data-point which is far from base point. For instance, a , b ,c, d, e,f are 6 students, and we wish to group them into clusters.

Hierarchical Clustering will sequentially group these students and we can stop the process at any number of clusters we want. Following is an illustrative chain of clustering :
Hence, if we want 3 clusters, a , bc and def are the required clusters. So far so simple. The technique uses the very basic of clustering and is, therefore, a very stable technique.
The only problem with the technique is that it is able to only handle small number of data-points and is very time consuming. This is because it tries to calculate the distance between all possible combination and then takes one decision to combine two groups/individual data-point.
2. k-means Clustering : This technique is more frequently used in analytics industry as it is able to handle large number of data points. FASTCLUS is an algorithm used by SAS to generate k-means cluster. Lets try to analyze how it works.

As can be seen from the figure above, we start with a definite number for the number of required cluster (in this case k=2). The algorithm takes 2 random seeds and maps all other data points to these two seeds. The algorithm re-iterates till the overall penalty term is minimized.
When we compare the two techniques, we find that the Hierarchical Clustering starts with individual data-points and sequentially club them to find the final cluster whereas  k-means Clustering  starts from some initial cluster and then tries to reassign data-points to k clusters to minimize the total penalty term. Hence for large number of data-points,   k-means uses far lesser iterations then Hierarchical Clustering.
Steps to perform cluster analysis:
Having discussed what is clustering and its types, lets apply these concepts on a business case. Following is a simple case we will try to solve :
US bank X wants to understand the profile of its customer base to build targeted campaigns.
Step 1 – Hypothesis building : This is the most crucial step of the whole exercise. Try to identify all possible variables that can help segment the portfolio regardless of its availability. Lets try to come up with a list for this example.
a. Customer balance with bank X
b. Number of transaction done in last 1/3/6/12 months
c. Balance change in last 1/3/6/12 months
d. Demographics of the customer
e. Customer total balance with all US banks
The list is just for illustrative purpose. In real scenario this list will be much longer.
Step 2 – Initial shortlist of variable : Once we have all possible variable, start selecting variable as per the data availability. Lets say, for the current example we have only data for Customer balance with bank X and Customer total balance with all US banks (total balance)
Step 3 – Visualize the data : It is very important to know the population spread across the selected variable before starting any analysis. For the current scenario, the exercise becomes simpler as the number of selected variables is only 2. Following is a scatter plot between total balance and Bank X balance (origin taken as mean of both the variables):

This visualization helps me to identify clusters which I can expect after the final analysis. Here, we can see there are four clear clusters in four quadrants. We can expect the same result in the final solution.
Step 4 – Data cleaning : Cluster analysis is very sensitive to outliers. It is very important to clean data on all variables taken into consideration. There are two industry standard ways to do this exercise :
1. Remove the outliers : (Not recommended in case the total data-points are low in number) We remove the data-points beyond mean +/- 3*standard deviation.
2. Capping and flouring of variables : (Recommended approach) We cap and flour all data-points at 1 and 99 percentile.
Lets use the second approach for this case.
Step 4 – Variable clustering :  This step is performed to cluster variables capturing similar attributes in data. And choosing only one variable from each variable cluster will not drop the sepration drastically compared to considering all variables. Remember, the idea is to take minimum number of variables to justify the seperation to make the analysis easier and less time consuming. You can simply use Proc VARCLUS to generate these clusters.

Step 5 – Clustering : We can use any of the two technique discussed in the article depending on the number of observation. k-means is used for a bigger samples. Run a proc fastclus with k=4 (which is apparent from the visualization).
 As we can see, the algorithm found 4 clusters which were already apparent in the visualization. In most business cases the number of variables will be much larger and such visualization won’t be possible and hence
Step 6 – Convergence of clusters : A good cluster analysis has all clusters with population between 5-30% of the overall base. Say, my total number of customer for bank X is 10000. The minimum and maximum size of any cluster should be 500 and 3000. If any of the cluster is beyond the limit than repeat the procedure with additional number of variables. We will discuss in detail about other convergence criterion in the next article.
Step 7 – Profiling of the clusters : After validating the convergence of cluster analysis, we need to identify behavior of each cluster. Lets say we map age and income to each of the four clusters and get following results :

Now is the time to build story around each cluster. Lets take any two cluster and analyze.
Cluster 1 : (High Potential Low balance customer) These customers do have high balance in aggregate but low balance with bank X. Hence, they are high potential customer with low current balance. Also the average salary is on a higher side which validates our hypothesis of customer being high potential.
Cluster 3 : (High Potential high balance customers) Even though the salary and total balance in aggregate is on a lower side, we see a lower average age. This indicates that the customer has a high potential to increase their balance with bank X.
Final notes :
As we saw, using clusters we can understand the portfolio in a better way. We can also build targeted strategy using the profiles of each cluster. In the Part 2 of this article we will discuss following :
1. When is cluster analysis said to be conclusive?
2. Different scenarios in which each of the two techniques dominate?
3. When do both techniques fail?
4. Step by step solution in a scenario when both the techniques fail.
When do you use unsupervised modelling techniques? Do you use any other method for clustering often? What challenges do you face while building clusters? Do let us know your thoughts in comments below.
If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our facebook page.
 
Share this:Pocket

	Related
			
						
						
		"
"Application of Envelopment Analysis to business problems","Tavish Srivastava",2013-11-06 03:45:00,4,"
							
										
						Recently, I was working on a business problem, which required me to find out inefficient branches of a bank X in North America and find root cause of their inefficiencies. 
I had solved several root cause analysis problems in past but finding a quantitative parameter for efficiency was new to me. This was a complex task because efficiency was derived from multiple target variables such as branch revenue vs. capacity, customer satisfaction index, policy persistency etc. Can we assign some weights to each of the target variable and sum them to get efficiency? But how will we get such weights? Is there a scientific way to get these weight?
I did a small research and found a method using simplex programming to obtain efficiency in problems involving multiple input and output parameters. This technique is commonly known as Data Envelopment Analysis. Even though its a popular technique in OR, it is not very common in analytics industry. This article will give you a brief layout of the formulation and explain its utility by a business case.

Lets start with a simple example: 
We have two processes A and B. Each process manufacture some jobs/week. And both jobs have different labor cost. As we know efficiency is a ratio of output and input, two processes will be compared based on their efficiency.Following are some illustrative figures:

Comparing the two efficiency, it can easily be concluded that Process A is more efficient than process B. That was easy! Lets make it slightly more complicated.  Labor Cost might not be the only cost involved. The above analysis assumes that all other costs are similar for the two processes. Lets introduce an additional cost i.e. office rent. Now office rent is proportional to office premise area. Following are some illustrative figures:

The efficiency figures are now swapped. Process B seems to be more efficient when only office Area costs are considered. Now in this case, we know the office rent/square feet. Hence we can calculate the total cost, i.e. sum of Office rent/week and labor cost/week. And determine total efficiency of the two processes using total costs.
What made the determination easy for this case? We already knew the weights to be applied on each of the input variable i.e. Office Area and Labor cost. The weight for Office area was rent/square feet and that of labor cost was one.
Lets complicate the puzzle further: 
The output in this case was simply throughput. In a real life scenario, the inputs and outputs are non comparable parameters. Lets take example of a bank branches. The objective is to compare the branch efficiency and find the most efficient branch. Following are some of the identified input and output parameter of these branches :

First step in any efficiency problem is to identify independent inputs and outputs. Make sure that the inputs and outputs are independent variables. In the above scenario, Employee salary and Branch Rent is in terms of dollars, Competition index (Degree of competition in locality) is an index and Management time share is a percentage term. Clearly Input variables are non additive. Similarly, output parameters are non-additive as well. We will derive an effectiveness of each branch using all these variables.
Know the Maths behind: 
The DEA technique is a kind of simple linear programming. It assumes certain aspects and knowing these aspects is essential before applying the technique. The formulation will make the assumptions clear. Following are the abbreviations used in this formulation :
1. Formulation is done for p processes
2. in(i,k) is the i th input parameter for the k th process
3. out(j,k) is the j th output parameter for the k th process
4. win(i) is the weight of i th input parameter
5. wout(j) is the weight of jth output parameter
Solving the above equations will give us efficiency of each business unit (branch in this case). The solution will also give the relative importance of each input and output parameter. The assumptions in this formulation are :
1. The Input and Output of each business unit are linear functions.
2. Each of input and output variables are independent of each other
3. Input and Output variables are exhaustive
  Lets find efficiency graphically: 
Suppose following are the input and output variables of 6 Processes :

We define the two independent efficiency based on the two output parameters,

Lets plot the two efficiency graphically.

As the name suggests, DEA defines an envelope of 100% efficiency. ABC is the envelope and any point inside the envelope is an inefficient unit.The graphical representation was easy for a two dimensional problem. But for the bank branch problem discussed in the last section, we will have to draw a 9 ( 4-1 *  4-1) dimensional envelope and hence very difficult to visualize.
Advantages of DEA technique: 
Following are the advantages of the technique :
1. No need to explicitly specify a mathematical form for the production function
<U+0097>2. It is proven to be useful in uncovering relationships that remain hidden for other methodologies
<U+0097>3. It is capable of handling multiple inputs and outputs
<U+0097>4. It is capable of being used with any input-output measurement
<U+0097>5. The sources of inefficiency can be analyzed and quantified for every evaluated unit

Final notes: 
DEA is a very useful technique when it comes to comparing efficiency and finding best practices. DEA defines an efficiency envelope and all units inside the envelope are inefficient. Solving the linear programming equation not only gives us efficiency of different units but also establishes relative weights of each input and output variable.Using DEA, I was able to score efficiency of each branch and find the relation between different parameter. Using these metrics, I was able to do a root cause analysis for branch inefficiency and establish branch best practices.


What do think of this technique? Do you think this provides solution to any problem you face? Are there any other techniques you use to find efficiency of different business units? Are you able to tackle the linearity assumption with this technique? Do let us know your thoughts in comments below.
If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our facebook page.

Share this:Pocket

	Related
			
						
						
		"
"Festive season special: Building models on seasonal data","Tavish Srivastava",2013-10-27 21:48:00,1,"
							
										
						Has your model ever failed on out of time validation because of seasonality?
If yes, then you need to know that one of the reason this happens is seasonality in performance or seasonality in cohort. This article will tell how to identify them and then take you through the industry standard techniques to take into account seasonality while building model.
Why does seasonality exist?
Most industries have seasonal business trend.  If there is seasonality in business, it implies that propensity of customer to buy a product is biased towards certain period in the year. This bias can originate because of numerous reasons. One of the most common reason for this bias is common market environment for all customers. For example, Indian financial year ends on 31st March. Because there are tax rebates on insurance products, people tend to buy more insurance product just before the end of financial year to claim these rebates. Hence, March is a high time for insurance industry in India. An analysis of past 10 years insurance business data show that 25-30% of business of insurance industry in India come in the month of March. Similarly, there is a surge in sales of consumer goods in UK and US leading up to Christmas.
Figure below shows a plot of monthly sales trend of a seasonal industry. It can noted that sales peaks and trough  happen in same month year on year. In other words, the trend of sales remain same year on year.

What is the impact if a model is built without considering seasonality in business?
Seasonality has negative impact over both predictive and descriptive model, if not treated explicitly. Lets take the case of a descriptive model and see what is the impact of seasonality over the model. Following is simple decision tree, where we have segmented the customer portfolio into 3 segment based on their attrition rate in next 1 month. Lets say that the month in which we are observing attrition is January (Portfolio attrition rate 30%).

Now say, we implement the model to predict the probability of attrition for the month of July. Following are the possible errors induced by seasonality :
1. Rank ordering among segments might change from 1-2-3.
2. Overall attrition rate might change from 30% and thereby changing individual attrition rate.
Both the errors result in loss of effectiveness of the model on implementing it on different months.
Predictive models are even more seriously impacted by seasonality, because the second error leads to a big deviation in the predictive power of the model. To make and accurate predictive or descriptive model on seasonal data, we need at least 12 months of data for training the model.
Type of Seasonality
There are two types of seasonality which need to be addressed in any model :
1. Seasonality in Performance : This is simpler seasonality to address in any model. The example used in the beginning of the article (Insurance industry business) is a good example to illustrate this type of seasonality. Say, we want to predict the performance (business sourced) of a sales agent in next 3 months. In this case, in which business is seasonal, performance seasonality needs to be addressed to make a stable predictive model.
2. Seasonality in Cohort : This is tougher seasonality to be addressed in a model. Seasonality in cohort is driven by the difference in characteristics of the base population in different months.Whenever cohort is seasonal, performance is seasonal as well. Say, we want to predict the performance (business sourced) by a sales agent in his first 12 months. Now, we know that March business sourcing is much easier than any other month. Hence, a sales agent on-boarding in Jan, Feb and Mar will have a higher average first 3 month performance compared to any other month. A good start is highly correlated to an overall higher 12 month performance. Hence, we have a seasonality in the cohort and agents on-boarding in Jan, Feb and Mar should be treated differently.
Industry Standard techniques to address seasonality
There are three methods followed industry wide to address the two types of seasonality mentioned in last section:
1. Long interval target function : Seasonality in performance can be addressed by taking 12 month long performance window. But this method fails if the seasonality exists in the cohort.

2. Use of same training and scoring target month : This technique addresses both the seasonality issues. Say, we want to predict March attrition. We will train the model on last year march and then use the same model to predict March attrition this year. This technique is robust but fails if there was any characteristic difference between the training and scoring month. Say, the company changed the definition of attrition after March last year. In this case the technique will not take into account the recent trends of attrition and give false prediction for this March.

3. Mix of cohort : This technique is used mainly in risk modelling. It addresses both the seasonality issues. We take a mix of samples from all different types of cohort and use it as the training population. This is the most robust technique to address seasonality both in performance and cohort.  This method does take into account recent trends as well while making the prediction and hence better than last technique in cases where there is some difference in characteristic between the target and the training month. But in cases where the target and training month is exactly same, last technique will give better prediction as mixing cohort will offset the target variable.


Final Notes
Out-of-time validation helps us identify if the model’s performance is being altered by seasonality. Techniques like bootstrapping and Jack-knife can only check the stability of the model and is incapable to check the effect of seasonality over the model.
Do you think this provides solution to any problem you face?  How do you address the problem of seasonality in your modelling ? Are there any other techniques you use to improve performance of your models (prediction or stability)? Do let us know your thoughts in comments below.
If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our facebook page.
Share this:Pocket

	Related
			
						
						
		"
"Trick to enhance power of Regression model","Tavish Srivastava",2013-10-21 23:27:00,2,"
							
										
						We, as analysts, specialize in optimization of already optimized processes. As the optimization gets finer, opportunity to make the process better gets thinner.  One of the predictive modeling technique used frequently use is regression (Linear or Logistic). Another equally competing technique (typically considered as a challenger) is Decision tree.
What if we could combine the benefits of both the techniques to create powerful predictive models?

The trick mentioned in this article does exactly that and can help you improve model lifts by as high as 120%.
 Overview of both the modeling techniques
Before getting into details of this trick, let me touch up briefly on pros and cons of the two mentioned techniques. If you want to read basics of predictive modeling, click here.

Regression assumes continuous variable as is and generates a prediction through fitting curves for each combination input variables.
Decision tree (CART/CHAID), on the other hand, converts these continuous variables into buckets and thereby segments the overall population.
Converting variables into buckets might make the analysis simpler, but it makes the model lose some predictive power because of its indifference for data points lying in the same bucket.
 A simple case study to understand pros and cons of the two techniques
If decision tree losses such an important trait, how come it has a predictive power similar to that of a regression model? It is because it captures the covariance term effectively, which makes a decision tree stronger. Say, we want to find probability of a person to buy a BMW.
Decision tree:
A decision tree simply segments the population in as discrete buckets as possible. This is how a typical decision tree would look like:

Even though the tree does not distinguish between Age 37 yrs. and 90 yrs. or salary $150k and $1M, the covariance between Age and Salary is making the decision tree’s prediction powerful.
Regression model:
On the other hand, logistic regression makes use of Logit function (shape below) to create prediction. A typical equation would look like this:

where a, b and c are constants
Shape of Logit function:                                  

 Industry standard techniques to address shortcomings of regression modeling:
There are two basic techniques to capture covariance and discontinuity of the target variable:

Bin variable with discontinuous relation: This is a technique used in almost all the models. If you are not familiar with this technique, it is nothing but flagging variables in the interval where a strong input variable shows a discontinuous relationship with the output variable.For example, 10% people break at least0 one traffic signal in US everyday. Only 3% of  households with salary between  $70k and $100k break traffic signal. Whereas, 11% of the rest of the population break traffic signal everyday, and this is almost uniformly distributed. In this case, to predict the propensity to break the signal, a good variable can be the salary bin $70k to $100k.
Introduce covariance variables: This is a technique used rarely. The reason being such variables are very difficult to comprehend and difficult to explain business.
Each of these techniques capture co-variance and discontinuous variable well.
However, consider the following scenario where these approach have a high propensity to fail:

Say in the population discussed in last section, people with salary between $100 K and $200 K and age above 35 years form a segment with exceptionally high BMW take up rate (30%). If we use the two discussed techniques, will the model capture this exceptionally high take-up segment? Will regression still be a better model compared to decision tree?

The answer is NO and NO, the regression will not be able to effectively capture this segment. Why does bin technique not capture this? The reason is that binning is done on a one-dimensional variable and in overall population salary bracket $100k to $200k might not even be different from rest. As you can see in the figure above, the response rate of the income bucket $100k to $200k is not differentiated when analyzed on overall population. But this bucket becomes very different when an initial cut of age >=35yrs is added.
Why does covariance term fail as well? The overall covariance between age and salary might not be significant for the overall population. Hence to have higher predictive power, the model needs an input that the trends of a particular segment are significantly different from rest of the population. Therefore,when the two problems i.e. discontinuity and covariance, exist simultaneously, regression model fails to capture the hidden segment.  Decision tree, on the other hand works very well in these scenarios.
 Have you guessed the trick?
Having worked on many of such problems, I find the following solution very handy. Both regression and decision tree have pros. Why not combine the pros of the two methods? I have used this technique in a number of models. And I was pleasantly surprised by the additional predictive power I got every time. There are two ways to combine the two methods:
Introduce a new Covariant Variable: A faster and an effective way to use the tool. Simply add this as one of the input variable for the logistic model.This Covariant term (a bi-variant bin) can be defined as:
Make two alternative models: A time taking but more effective method in case the exceptional bin has reasonable size. In this method we build two regression models separately for the identified bin (Age > 35yrs. and $200k > Salary > $100k ) and the rest of population. And add the two function by following logic.
Here, g(x) is the equation for the identified bin and f(x) is the equation for rest of the population. Z is same as defined in the last block.
I am sure you are wondering “What makes the lift go even higher than the lifts you saw in the two models?” If you are not I will really like to know your opinions on the reason.
How does it work? How do this trick create such impactful results?
Now let’s try to think about what did we just do? Decision tree was coming out to be a better model because of a hidden pocket, which was two-dimensional bin. Even with the limitation of not using the continuous behavior of interval variable decision tree became very efficient to reduce false positive in a particular segment. By introducing the flag of this segment in logistic regression we have given the regression the additional dimension decision tree was able to capture. Hence by additionally using the continuous behavior of interval variables such as age, salary the new logistic regression becomes stronger than the decision tree.
Constraints of this trick:
There are two major constraints of using this technique,
Multi-collinearity: For models, where the VIF factor becomes unacceptable, the number of variables used to create the new input function should be reduced.
High Covariance: When overall covariance between two terms is high, this technique simply fails. This is because we will have to create too many buckets and , therefore, too many variables to be introduced in the regression model. This will introduce very high collinearity to the regression.
In general, I follow a thumb rule of not making more than 6 leaves in the parent tree. This, first of all, captures the most important co-variant buckets and does not introduce the two mentioned problems. Also, make sure the final bucket makes sense with business and is not merely a noise.
 Final notes:
This trick helped me make my lifts rise by as high as 120% of the original lift. The best part of this trick is that it gives you a good starting point for your regression where you start with a couple of already proved significant variables.
What do think of this technique? Do you think this provides solution to any problem you face? Are there any other techniques you use to improve performance of your models (prediction or stability)? Do let us know your thoughts in comments below.
If you like what you just read & want to continue your analytics learning, subscribe to our emails or like our facebook page.
Share this:Pocket

	Related
			
						
						
		"
